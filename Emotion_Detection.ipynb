{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShahkarHassan/SMART-CROWD-ANALYZER/blob/master/Emotion_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9jEfV1BuL-t",
        "colab_type": "text"
      },
      "source": [
        "### Drive Mounter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkX0McPauqiA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c42f9f10-adeb-4b43-aed5-a673f1d63f60"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrNp05O4uzjJ",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprcessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwf7Lj6Vu4m6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fldr=\"/content/drive/My Drive/FYP/emotion/CK+48\"\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CPTeRZFvxnM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d54d3b8-0211-46ca-edf0-f76ac1ac03fe"
      },
      "source": [
        "import os\n",
        "files=os.listdir(fldr)\n",
        "print(files)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['anger', 'contempt', 'disgust', 'fear', 'happy', 'sadness', 'surprise']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLaJ1-2QwERM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Exp=['fear', 'contempt', 'happy', 'anger', 'surprise', 'disgust', 'sadness']"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlQuUFH4wHg9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af19f2c4-c44a-44c8-d563-d7cdc87fb0bf"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "i=0\n",
        "last=[]\n",
        "images=[]\n",
        "labels=[]\n",
        "for fle in files:\n",
        "  idx=Exp.index(fle)\n",
        "  label=idx\n",
        "  \n",
        "  total=fldr+'/'+fle\n",
        "  files_exp= os.listdir(total)\n",
        "\n",
        "  for fle_2 in files_exp:\n",
        "    file_main=total+'/'+fle_2\n",
        "    print(file_main+\"   \"+str(label))\n",
        "    image= cv2.imread(file_main)\n",
        "\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image= cv2.resize(image,(48,48))\n",
        "    images.append(image)\n",
        "    labels.append(label)\n",
        "    i+=1\n",
        "  last.append(i)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S010_004_00000017.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S010_004_00000018.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S010_004_00000019.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S011_004_00000020.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S011_004_00000019.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S011_004_00000021.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S014_003_00000028.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S014_003_00000029.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S022_005_00000030.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S014_003_00000030.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S022_005_00000031.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S022_005_00000032.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S026_003_00000013.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S026_003_00000014.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S026_003_00000015.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S028_001_00000022.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S028_001_00000023.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S028_001_00000024.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S029_001_00000017.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S029_001_00000018.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S029_001_00000019.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S032_003_00000015.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S032_003_00000016.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S032_003_00000017.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S034_003_00000025.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S034_003_00000026.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S034_003_00000027.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S037_003_00000020.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S037_003_00000021.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S037_003_00000022.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S042_004_00000018.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S042_004_00000019.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S042_004_00000020.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S045_005_00000028.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S045_005_00000029.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S045_005_00000030.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S050_004_00000019.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S050_004_00000020.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S050_004_00000021.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S055_004_00000026.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S055_004_00000027.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S055_004_00000028.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S058_005_00000008.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S058_005_00000009.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S058_005_00000010.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S066_005_00000009.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S066_005_00000010.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S066_005_00000011.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S067_004_00000021.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S067_004_00000022.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S067_004_00000023.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S071_004_00000026.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S071_004_00000027.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S071_004_00000028.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S072_005_00000017.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S072_005_00000018.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S072_005_00000019.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S075_008_00000010.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S075_008_00000011.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S075_008_00000012.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S082_005_00000015.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S082_005_00000016.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S082_005_00000017.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S087_007_00000014.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S087_007_00000015.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S087_007_00000016.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S089_003_00000034.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S089_003_00000035.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S089_003_00000036.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S090_007_00000012.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S090_007_00000013.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S090_007_00000014.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S092_003_00000012.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S092_003_00000013.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S092_003_00000014.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S100_005_00000021.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S100_005_00000022.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S100_005_00000023.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S109_003_00000015.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S109_003_00000016.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S109_003_00000017.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S111_006_00000008.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S111_006_00000009.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S111_006_00000010.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S112_005_00000015.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S112_005_00000016.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S112_005_00000017.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S113_008_00000021.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S113_008_00000022.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S113_008_00000023.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S117_006_00000008.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S117_006_00000009.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S117_006_00000010.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S119_008_00000016.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S119_008_00000017.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S119_008_00000018.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S126_008_00000027.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S126_008_00000028.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S126_008_00000029.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S127_010_00000016.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S127_010_00000017.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S127_010_00000018.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S129_006_00000008.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S129_006_00000009.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S129_006_00000010.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S130_007_00000018.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S130_007_00000019.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S130_007_00000020.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S133_003_00000045.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S133_003_00000046.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S133_003_00000047.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S134_003_00000009.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S134_003_00000010.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S134_003_00000011.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S136_005_00000008.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S136_005_00000009.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S136_005_00000010.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S501_001_00000065.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S501_001_00000066.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S501_001_00000067.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S502_001_00000014.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S502_001_00000015.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S502_001_00000016.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S503_001_00000069.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S503_001_00000070.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S503_001_00000071.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S504_001_00000020.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S504_001_00000021.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S504_001_00000022.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S506_001_00000038.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S506_001_00000039.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S506_001_00000040.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S999_001_00000016.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S999_001_00000017.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/anger/S999_001_00000018.png   3\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S138_008_00000007.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S138_008_00000008.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S138_008_00000009.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S139_002_00000011.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S139_002_00000012.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S139_002_00000013.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S147_002_00000011.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S147_002_00000012.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S147_002_00000013.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S148_002_00000013.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S148_002_00000014.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S148_002_00000015.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S149_002_00000013.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S149_002_00000012.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S149_002_00000011.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S151_002_00000027.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S151_002_00000028.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S151_002_00000029.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S154_002_00000011.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S154_002_00000012.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S154_002_00000013.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S155_002_00000010.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S155_002_00000011.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S155_002_00000012.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S156_002_00000019.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S156_002_00000020.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S156_002_00000021.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S157_002_00000009.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S157_002_00000010.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S157_002_00000011.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S158_002_00000009.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S158_002_00000010.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S158_002_00000011.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S160_006_00000008.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S160_006_00000009.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S160_006_00000010.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S502_002_00000007.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S502_002_00000008.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S502_002_00000009.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S503_002_00000006.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S503_002_00000007.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S503_002_00000008.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S504_002_00000007.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S504_002_00000008.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S504_002_00000009.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S505_002_00000019.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S505_002_00000020.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S505_002_00000021.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S506_002_00000007.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S506_002_00000008.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S506_002_00000009.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S895_002_00000005.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S895_002_00000006.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/contempt/S895_002_00000007.png   1\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S005_001_00000009.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S005_001_00000010.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S005_001_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S011_005_00000018.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S011_005_00000019.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S011_005_00000020.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S022_006_00000015.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S022_006_00000017.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S022_006_00000016.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S032_005_00000014.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S032_005_00000015.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S032_005_00000016.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S035_005_00000017.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S035_005_00000018.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S035_005_00000019.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S044_006_00000017.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S044_006_00000018.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S044_006_00000019.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S045_004_00000013.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S045_004_00000014.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S045_004_00000015.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S046_004_00000015.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S046_004_00000016.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S046_004_00000017.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S051_003_00000016.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S051_003_00000018.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S051_003_00000017.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S052_006_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S052_006_00000012.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S052_006_00000013.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S054_004_00000023.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S054_004_00000022.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S054_004_00000024.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S055_003_00000007.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S055_003_00000009.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S055_003_00000008.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S056_002_00000008.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S056_002_00000010.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S056_002_00000009.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S057_003_00000013.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S057_003_00000014.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S057_003_00000015.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S058_006_00000016.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S058_006_00000017.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S058_006_00000018.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S060_005_00000019.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S060_005_00000021.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S060_005_00000020.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S061_004_00000021.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S061_004_00000020.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S062_005_00000027.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S061_004_00000022.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S062_005_00000028.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S062_005_00000029.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S065_005_00000006.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S065_005_00000007.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S065_005_00000008.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S067_006_00000009.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S067_006_00000010.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S067_006_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S068_005_00000019.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S068_005_00000020.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S068_005_00000021.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S069_003_00000009.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S069_003_00000010.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S069_003_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S070_005_00000014.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S070_005_00000015.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S070_005_00000016.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S071_006_00000012.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S073_006_00000012.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S071_006_00000013.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S071_006_00000014.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S073_006_00000013.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S074_004_00000016.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S074_004_00000017.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S073_006_00000014.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S074_004_00000018.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S075_005_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S075_005_00000012.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S075_005_00000010.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S076_005_00000010.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S076_005_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S076_005_00000012.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S077_006_00000012.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S077_006_00000013.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S077_006_00000014.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S078_007_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S078_007_00000012.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S078_007_00000013.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S079_002_00000010.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S079_002_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S079_002_00000012.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S080_008_00000007.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S080_008_00000008.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S080_008_00000009.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S081_008_00000009.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S081_008_00000010.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S081_008_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S082_007_00000008.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S082_007_00000009.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S082_007_00000010.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S085_004_00000015.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S085_004_00000016.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S085_004_00000017.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S087_004_00000010.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S087_004_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S087_004_00000012.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S088_004_00000018.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S088_004_00000020.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S088_004_00000019.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S090_006_00000010.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S090_006_00000009.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S095_006_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S090_006_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S095_006_00000013.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S095_006_00000012.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S096_003_00000010.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S096_003_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S096_003_00000012.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S097_004_00000029.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S097_004_00000028.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S097_004_00000030.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S098_003_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S098_003_00000013.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S098_003_00000012.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S099_007_00000010.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S099_007_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S099_007_00000012.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S102_009_00000014.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S102_009_00000013.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S102_009_00000015.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S105_008_00000008.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S105_008_00000009.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S105_008_00000010.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S106_004_00000006.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S106_004_00000007.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S106_004_00000008.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S107_005_00000009.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S107_005_00000010.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S107_005_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S108_006_00000018.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S108_006_00000019.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S108_006_00000020.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S109_005_00000012.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S109_005_00000013.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S109_005_00000014.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S111_007_00000012.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S111_007_00000013.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S111_007_00000014.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S116_006_00000005.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S116_006_00000006.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S116_006_00000007.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S124_006_00000009.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S124_006_00000010.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S124_006_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S125_008_00000008.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S125_008_00000009.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S125_008_00000010.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S128_004_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S128_004_00000012.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S128_004_00000013.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S129_011_00000017.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S129_011_00000016.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S130_012_00000010.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S130_012_00000009.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S130_012_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S129_011_00000018.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S131_010_00000016.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S131_010_00000017.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S131_010_00000018.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S132_005_00000014.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S132_005_00000015.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S132_005_00000016.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S134_008_00000011.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S134_008_00000012.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/disgust/S134_008_00000013.png   5\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S011_003_00000012.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S011_003_00000013.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S011_003_00000014.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S032_004_00000012.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S032_004_00000013.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S046_003_00000014.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S032_004_00000014.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S046_003_00000015.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S046_003_00000016.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S050_001_00000015.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S050_001_00000016.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S050_001_00000017.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S054_002_00000013.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S054_002_00000014.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S054_002_00000015.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S055_006_00000007.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S055_006_00000006.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S055_006_00000008.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S059_002_00000015.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S059_002_00000016.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S059_002_00000017.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S062_001_00000015.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S062_001_00000016.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S062_001_00000017.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S065_002_00000020.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S065_002_00000021.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S065_002_00000022.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S068_004_00000008.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S068_004_00000009.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S068_004_00000010.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S074_001_00000018.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S074_001_00000019.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S074_001_00000020.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S084_002_00000021.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S084_002_00000022.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S084_002_00000023.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S091_001_00000013.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S091_001_00000014.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S091_001_00000015.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S102_003_00000014.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S102_003_00000015.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S102_003_00000016.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S117_003_00000012.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S117_003_00000013.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S117_003_00000014.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S119_003_00000022.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S119_003_00000023.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S119_003_00000024.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S124_003_00000009.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S124_003_00000010.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S124_003_00000011.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S125_006_00000020.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S125_006_00000021.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S125_006_00000022.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S132_003_00000021.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S132_003_00000022.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S132_003_00000023.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S138_001_00000010.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S138_001_00000011.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S138_001_00000012.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S501_004_00000054.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S501_004_00000055.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S501_004_00000056.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S502_004_00000050.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S502_004_00000051.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S502_004_00000052.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S504_004_00000013.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S504_004_00000014.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S504_004_00000015.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S506_004_00000036.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S506_004_00000037.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S506_004_00000038.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S999_003_00000053.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S999_003_00000054.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/fear/S999_003_00000055.png   0\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S010_006_00000013.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S010_006_00000014.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S010_006_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S011_006_00000011.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S011_006_00000012.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S011_006_00000013.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S014_005_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S014_005_00000016.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S014_005_00000017.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S026_006_00000011.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S026_006_00000012.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S026_006_00000013.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S032_006_00000014.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S032_006_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S034_005_00000008.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S032_006_00000016.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S034_005_00000009.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S034_005_00000010.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S035_006_00000016.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S035_006_00000017.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S035_006_00000018.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S037_006_00000019.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S037_006_00000020.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S037_006_00000021.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S042_006_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S042_006_00000016.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S042_006_00000017.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S044_003_00000012.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S044_003_00000013.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S044_003_00000014.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S050_006_00000021.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S050_006_00000022.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S050_006_00000023.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S052_004_00000031.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S052_004_00000032.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S052_004_00000033.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S053_004_00000022.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S053_004_00000023.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S053_004_00000024.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S055_005_00000043.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S055_005_00000044.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S055_005_00000045.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S056_004_00000018.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S056_004_00000019.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S056_004_00000020.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S057_006_00000031.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S057_006_00000032.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S057_006_00000033.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S060_002_00000024.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S060_002_00000025.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S060_002_00000026.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S061_002_00000013.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S061_002_00000014.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S061_002_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S062_004_00000022.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S062_004_00000023.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S062_004_00000024.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S063_002_00000021.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S063_002_00000022.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S063_002_00000023.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S064_003_00000023.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S064_003_00000024.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S064_003_00000025.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S065_004_00000026.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S065_004_00000027.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S065_004_00000028.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S066_003_00000010.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S066_003_00000011.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S066_003_00000012.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S067_005_00000020.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S067_005_00000021.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S067_005_00000022.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S068_002_00000013.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S068_002_00000014.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S068_002_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S069_004_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S069_004_00000017.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S069_004_00000016.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S070_003_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S070_003_00000016.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S070_003_00000017.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S071_005_00000019.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S071_005_00000020.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S071_005_00000021.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S072_006_00000020.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S072_006_00000021.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S072_006_00000022.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S074_005_00000042.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S074_005_00000041.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S074_005_00000043.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S075_006_00000023.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S075_006_00000024.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S075_006_00000025.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S076_006_00000017.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S076_006_00000018.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S076_006_00000019.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S078_004_00000025.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S078_004_00000026.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S078_004_00000027.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S079_004_00000024.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S079_004_00000026.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S079_004_00000025.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S083_003_00000017.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S083_003_00000018.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S083_003_00000019.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S085_002_00000013.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S085_002_00000012.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S085_002_00000014.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S086_002_00000013.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S086_002_00000014.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S086_002_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S087_005_00000010.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S087_005_00000011.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S087_005_00000012.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S089_002_00000019.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S089_002_00000020.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S089_002_00000021.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S091_003_00000019.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S091_003_00000020.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S091_003_00000021.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S092_004_00000022.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S092_004_00000023.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S092_004_00000024.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S093_004_00000014.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S093_004_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S093_004_00000016.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S094_004_00000010.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S094_004_00000011.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S094_004_00000012.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S095_007_00000019.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S095_007_00000020.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S095_007_00000021.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S096_004_00000009.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S096_004_00000010.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S096_004_00000011.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S097_006_00000017.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S097_006_00000018.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S097_006_00000019.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S098_004_00000013.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S098_004_00000014.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S098_004_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S099_004_00000013.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S099_004_00000014.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S099_004_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S100_006_00000014.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S100_006_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S100_006_00000016.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S106_006_00000009.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S106_006_00000010.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S106_006_00000011.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S108_008_00000011.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S108_008_00000012.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S108_008_00000013.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S109_006_00000013.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S109_006_00000014.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S109_006_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S114_006_00000021.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S114_006_00000022.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S114_006_00000023.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S115_008_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S115_008_00000016.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S115_008_00000017.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S116_007_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S116_007_00000016.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S116_007_00000017.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S124_007_00000022.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S124_007_00000023.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S124_007_00000024.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S125_005_00000011.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S125_005_00000012.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S125_005_00000013.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S127_004_00000014.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S127_004_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S127_004_00000016.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S128_011_00000014.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S128_011_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S128_011_00000016.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S129_012_00000009.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S129_012_00000010.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S129_012_00000011.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S130_013_00000013.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S130_013_00000014.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S130_013_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S131_006_00000020.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S131_006_00000021.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S131_006_00000022.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S132_006_00000021.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S132_006_00000022.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S132_006_00000023.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S133_010_00000012.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S133_010_00000013.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S133_010_00000014.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S134_004_00000013.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S134_004_00000014.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S134_004_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S135_012_00000018.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S135_012_00000019.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S135_012_00000020.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S136_006_00000018.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S136_006_00000019.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S136_006_00000020.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S137_011_00000018.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S137_011_00000019.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S137_011_00000020.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S138_005_00000014.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S138_005_00000015.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/happy/S138_005_00000016.png   2\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S011_002_00000020.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S011_002_00000021.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S011_002_00000022.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S014_002_00000014.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S014_002_00000015.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S014_002_00000016.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S026_002_00000014.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S026_002_00000015.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S026_002_00000016.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S042_002_00000014.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S042_002_00000015.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S042_002_00000016.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S046_001_00000023.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S046_001_00000024.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S046_001_00000025.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S064_004_00000012.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S064_004_00000013.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S064_004_00000014.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S066_004_00000008.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S066_004_00000009.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S066_004_00000010.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S071_002_00000018.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S071_002_00000019.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S071_002_00000020.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S080_005_00000011.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S080_005_00000012.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S080_005_00000013.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S081_002_00000022.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S081_002_00000023.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S081_002_00000024.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S093_001_00000018.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S093_001_00000019.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S093_001_00000020.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S095_010_00000012.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S095_010_00000013.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S095_010_00000014.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S106_002_00000014.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S106_002_00000015.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S106_002_00000016.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S108_005_00000020.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S108_005_00000021.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S108_005_00000022.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S113_003_00000013.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S113_003_00000014.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S113_003_00000015.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S115_004_00000015.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S115_004_00000016.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S115_004_00000017.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S125_001_00000012.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S125_001_00000013.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S125_001_00000014.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S130_009_00000017.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S130_009_00000018.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S130_009_00000019.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S131_003_00000022.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S131_003_00000023.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S131_003_00000024.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S132_002_00000016.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S132_002_00000017.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S132_002_00000018.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S136_003_00000012.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S136_003_00000014.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S136_003_00000013.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S137_005_00000025.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S137_005_00000026.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S137_005_00000027.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S138_007_00000009.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S138_007_00000010.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S138_007_00000011.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S501_006_00000039.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S501_006_00000040.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S501_006_00000041.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S503_006_00000018.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S503_006_00000019.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S503_006_00000020.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S504_006_00000016.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S504_006_00000017.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S504_006_00000018.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S505_006_00000017.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S505_006_00000018.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S505_006_00000019.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S506_006_00000040.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S506_006_00000041.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/sadness/S506_006_00000042.png   6\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S010_002_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S010_002_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S010_002_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S011_001_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S011_001_00000015.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S011_001_00000016.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S014_001_00000027.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S014_001_00000028.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S014_001_00000029.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S022_001_00000028.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S022_001_00000029.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S022_001_00000030.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S026_001_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S026_001_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S026_001_00000015.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S032_001_00000020.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S032_001_00000021.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S032_001_00000022.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S034_001_00000027.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S034_001_00000028.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S034_001_00000029.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S035_001_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S035_001_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S035_001_00000015.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S037_001_00000018.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S037_001_00000019.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S037_001_00000020.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S042_001_00000017.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S042_001_00000018.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S042_001_00000019.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S044_001_00000022.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S044_001_00000023.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S044_001_00000024.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S046_002_00000004.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S046_002_00000005.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S046_002_00000006.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S050_002_00000016.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S050_002_00000017.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S050_002_00000018.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S051_002_00000017.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S051_002_00000018.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S051_002_00000019.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S052_001_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S052_001_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S052_001_00000015.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S053_001_00000021.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S053_001_00000022.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S053_001_00000023.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S054_003_00000005.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S054_003_00000006.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S054_003_00000007.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S055_001_00000010.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S055_001_00000011.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S055_001_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S056_003_00000008.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S056_003_00000009.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S056_003_00000010.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S057_001_00000017.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S057_001_00000018.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S057_001_00000019.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S058_001_00000018.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S058_001_00000019.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S058_001_00000020.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S059_001_00000016.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S059_001_00000017.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S059_001_00000018.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S060_003_00000016.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S060_003_00000017.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S060_003_00000018.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S061_001_00000010.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S061_001_00000011.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S061_001_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S062_002_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S062_002_00000015.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S063_001_00000011.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S062_002_00000016.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S063_001_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S064_001_00000010.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S064_001_00000011.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S063_001_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S064_001_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S065_003_00000020.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S065_003_00000021.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S065_003_00000022.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S066_002_00000020.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S066_002_00000021.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S066_002_00000022.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S067_002_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S067_002_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S067_002_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S068_003_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S068_003_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S068_003_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S069_002_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S069_002_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S069_002_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S070_002_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S070_002_00000015.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S070_002_00000016.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S071_001_00000011.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S071_001_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S071_001_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S073_001_00000011.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S073_001_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S073_001_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S074_002_00000015.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S074_002_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S074_002_00000016.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S075_002_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S075_002_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S075_002_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S076_001_00000015.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S076_001_00000016.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S076_001_00000017.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S077_001_00000026.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S077_001_00000027.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S077_001_00000028.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S078_001_00000031.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S078_001_00000032.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S078_001_00000033.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S079_001_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S079_001_00000010.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S079_001_00000011.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S080_001_00000017.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S081_001_00000017.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S080_001_00000016.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S080_001_00000018.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S081_001_00000019.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S081_001_00000018.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S082_001_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S082_001_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S082_001_00000015.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S084_001_00000008.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S084_001_00000009.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S084_001_00000010.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S085_003_00000011.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S085_003_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S085_003_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S086_001_00000017.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S086_001_00000018.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S086_001_00000019.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S087_001_00000009.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S087_001_00000010.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S087_001_00000011.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S088_001_00000015.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S088_001_00000016.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S088_001_00000017.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S089_001_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S089_001_00000015.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S089_001_00000016.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S090_002_00000009.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S090_002_00000010.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S090_002_00000011.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S092_001_00000015.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S092_001_00000016.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S092_001_00000017.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S094_001_00000008.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S094_001_00000009.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S095_001_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S094_001_00000010.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S095_001_00000015.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S095_001_00000016.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S096_001_00000005.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S096_001_00000006.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S096_001_00000007.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S097_001_00000019.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S097_001_00000020.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S097_001_00000021.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S099_001_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S099_001_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S099_001_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S100_002_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S100_002_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S100_002_00000015.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S101_002_00000017.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S101_002_00000018.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S101_002_00000019.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S102_002_00000016.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S102_002_00000017.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S102_002_00000018.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S107_001_00000008.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S107_001_00000009.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S107_001_00000010.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S110_001_00000011.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S110_001_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S110_001_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S111_001_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S111_001_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S111_001_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S113_001_00000010.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S113_001_00000011.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S113_001_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S114_001_00000016.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S114_001_00000017.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S114_001_00000018.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S115_001_00000006.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S115_001_00000007.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S115_001_00000008.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S116_001_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S116_001_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S116_001_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S117_001_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S117_001_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S117_001_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S119_001_00000009.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S119_001_00000010.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S119_001_00000011.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S122_001_00000010.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S122_001_00000011.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S122_001_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S124_001_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S124_001_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S124_001_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S125_007_00000007.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S125_007_00000008.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S125_007_00000009.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S126_004_00000010.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S126_004_00000011.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S126_004_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S127_001_00000015.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S127_001_00000016.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S127_001_00000017.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S129_002_00000009.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S129_002_00000010.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S129_002_00000011.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S130_001_00000016.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S130_001_00000017.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S130_001_00000018.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S131_001_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S131_001_00000015.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S131_001_00000016.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S132_008_00000008.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S132_008_00000009.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S132_008_00000010.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S133_009_00000004.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S133_009_00000005.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S133_009_00000006.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S135_001_00000037.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S135_001_00000038.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S135_001_00000039.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S136_001_00000017.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S136_001_00000018.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S136_001_00000019.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S137_001_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S137_001_00000013.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S137_001_00000014.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S138_004_00000011.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S138_004_00000012.png   4\n",
            "/content/drive/My Drive/FYP/emotion/CK+48/surprise/S138_004_00000013.png   4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnhH5X6fDusR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBjyfD5245gM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRdHutrn48Q8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "038b249b-ac19-4b34-c4e2-34fc36bb3f6d"
      },
      "source": [
        "last"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-cfde3e342ac3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'last' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm8OGs725qC0",
        "colab_type": "text"
      },
      "source": [
        "#### Fear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQeMquTS5CLY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "79f6f682-6e77-4994-fe15-8ac66537fb57"
      },
      "source": [
        "cv2_imshow(images[24])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cf584b97d646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'cv2_imshow' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B88tJpz65aIl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "93dfa1cf-e03e-4557-a8d0-ceafd86ee655"
      },
      "source": [
        "cv2_imshow(images[40])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-e0e0e2ec9dac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkzhBnKj5tu5",
        "colab_type": "text"
      },
      "source": [
        "#### Contempt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-k3qsf-5eEO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "5fc4edd8-4849-4f0b-ebbe-1ade9c537a8f"
      },
      "source": [
        "cv2_imshow(images[139])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7FA21C5B1780>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWnfH_p_5zGs",
        "colab_type": "text"
      },
      "source": [
        "#### Happy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4od79JNf5wXz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "77e27305-0ed6-49ce-a88a-17c700f46d41"
      },
      "source": [
        "cv2_imshow(images[349])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7FA21C5B1390>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH0H70hf58OZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "c0440cc9-acf1-4ab0-f230-450c48c10933"
      },
      "source": [
        "cv2_imshow(images[300])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7FA2E92C6630>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPDUU8eI7CRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AazMQza7-KX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "3676d107-d986-43a1-f663-75ea4853ed4e"
      },
      "source": [
        "import numpy as np\n",
        "images_f=np.array(images)\n",
        "labels_f=np.array(labels)\n",
        "\n",
        "images_f_2=images_f/255\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-20315eaf1b10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimages_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlabels_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimages_f_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages_f\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3aEjV3oFGKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "2329c30a-4ec1-4ada-b887-036fa703c40d"
      },
      "source": [
        "cv2_imshow(images[300])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7FA2ADFA79B0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urhM93muFJE8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14213bb5-d45f-47d1-d346-e522fe06f78c"
      },
      "source": [
        "images_f_2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(981, 48, 48, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XDADyGeFNEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_of_classes=7\n",
        "labels_encoded=tf.keras.utils.to_categorical(labels_f,num_classes=num_of_classes)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BOhWJ44FvH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test= train_test_split(images_f_2, labels_encoded,test_size=0.25)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwP_RGvUGt0h",
        "colab_type": "text"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YESdhmIpGKAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten,BatchNormalization\n",
        "from tensorflow.keras.layers import Dense, MaxPooling2D,Conv2D\n",
        "from tensorflow.keras.layers import Input,Activation,Add\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def Convolution(input_tensor,filters):\n",
        "    \n",
        "    x = Conv2D(filters=filters,kernel_size=(3, 3),padding = 'same',strides=(1, 1),kernel_regularizer=l2(0.001))(input_tensor)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x= Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "def model(input_shape):\n",
        "  inputs = Input((input_shape))\n",
        "  \n",
        "  conv_1= Convolution(inputs,32)\n",
        "  maxp_1 = MaxPooling2D(pool_size = (2,2)) (conv_1)\n",
        "  conv_2 = Convolution(maxp_1,64)\n",
        "  maxp_2 = MaxPooling2D(pool_size = (2, 2)) (conv_2)\n",
        "  conv_3 = Convolution(maxp_2,128)\n",
        "  maxp_3 = MaxPooling2D(pool_size = (2, 2)) (conv_3)\n",
        "  conv_4 = Convolution(maxp_3,256)\n",
        "  maxp_4 = MaxPooling2D(pool_size = (2, 2)) (conv_4)\n",
        "  flatten= Flatten() (maxp_4)\n",
        "  dense_1= Dense(128,activation='relu')(flatten)\n",
        "  drop_1=Dropout(0.2)(dense_1)\n",
        "  output= Dense(7,activation=\"sigmoid\")(drop_1)\n",
        "\n",
        "  model = Model(inputs=[inputs], outputs=[output])\n",
        "\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\",\n",
        "\tmetrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6_MmJqTGb4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Model=model(input_shape = (48,48,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP8Y6mNwGl-k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "29ac7d6a-70c5-4e8b-c5a8-8fdba03d78b3"
      },
      "source": [
        "Model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 48, 48, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 48, 48, 32)        896       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 48, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 48, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 6, 6, 256)         295168    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               295040    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 684,359\n",
            "Trainable params: 684,359\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em_s679QGyOs",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPf8o5TRGpZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltft2Z6bG2eS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fle_s='Emotion_detection.h5'\n",
        "checkpointer = ModelCheckpoint(fle_s, monitor='loss',verbose=1,save_best_only=True,save_weights_only=False, mode='auto',save_freq='epoch')\n",
        "callback_list=[checkpointer]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf9ZgmksG96G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6eb83fbb-815f-424f-984f-74600920f6de"
      },
      "source": [
        "History=Model.fit(X_train,Y_train,batch_size=32,validation_data=(X_test,Y_test),epochs=1000,callbacks=[callback_list])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 2.0967 - accuracy: 0.2095\n",
            "Epoch 00001: loss improved from inf to 2.09673, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 1s 29ms/step - loss: 2.0967 - accuracy: 0.2095 - val_loss: 1.9929 - val_accuracy: 0.3699\n",
            "Epoch 2/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 1.9795 - accuracy: 0.2361\n",
            "Epoch 00002: loss improved from 2.09673 to 1.95856, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.9586 - accuracy: 0.2490 - val_loss: 1.8577 - val_accuracy: 0.2886\n",
            "Epoch 3/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 1.9301 - accuracy: 0.2599\n",
            "Epoch 00003: loss improved from 1.95856 to 1.91335, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.9133 - accuracy: 0.2680 - val_loss: 1.8104 - val_accuracy: 0.2886\n",
            "Epoch 4/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 1.8435 - accuracy: 0.2747\n",
            "Epoch 00004: loss improved from 1.91335 to 1.84741, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.8474 - accuracy: 0.2558 - val_loss: 1.8485 - val_accuracy: 0.2886\n",
            "Epoch 5/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 1.7852 - accuracy: 0.2747\n",
            "Epoch 00005: loss improved from 1.84741 to 1.78558, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.7856 - accuracy: 0.2803 - val_loss: 1.7326 - val_accuracy: 0.4756\n",
            "Epoch 6/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 1.5910 - accuracy: 0.3958\n",
            "Epoch 00006: loss improved from 1.78558 to 1.57159, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.5716 - accuracy: 0.4095 - val_loss: 1.5252 - val_accuracy: 0.4837\n",
            "Epoch 7/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 1.3969 - accuracy: 0.5214\n",
            "Epoch 00007: loss improved from 1.57159 to 1.39413, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.3941 - accuracy: 0.5034 - val_loss: 1.2458 - val_accuracy: 0.5447\n",
            "Epoch 8/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 1.2221 - accuracy: 0.5345\n",
            "Epoch 00008: loss improved from 1.39413 to 1.19231, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.1923 - accuracy: 0.5551 - val_loss: 0.9587 - val_accuracy: 0.6504\n",
            "Epoch 9/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 1.0569 - accuracy: 0.6337\n",
            "Epoch 00009: loss improved from 1.19231 to 1.02759, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 1.0276 - accuracy: 0.6449 - val_loss: 0.8341 - val_accuracy: 0.7154\n",
            "Epoch 10/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.8266 - accuracy: 0.7270\n",
            "Epoch 00010: loss improved from 1.02759 to 0.83907, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.8391 - accuracy: 0.7265 - val_loss: 0.7555 - val_accuracy: 0.8130\n",
            "Epoch 11/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.7370 - accuracy: 0.7569\n",
            "Epoch 00011: loss improved from 0.83907 to 0.74784, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.7478 - accuracy: 0.7592 - val_loss: 0.6062 - val_accuracy: 0.8415\n",
            "Epoch 12/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.6949 - accuracy: 0.7845\n",
            "Epoch 00012: loss improved from 0.74784 to 0.67237, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.6724 - accuracy: 0.8041 - val_loss: 0.5415 - val_accuracy: 0.8780\n",
            "Epoch 13/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.5520 - accuracy: 0.8355\n",
            "Epoch 00013: loss improved from 0.67237 to 0.55363, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.5536 - accuracy: 0.8381 - val_loss: 0.5028 - val_accuracy: 0.8943\n",
            "Epoch 14/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.5160 - accuracy: 0.8553\n",
            "Epoch 00014: loss improved from 0.55363 to 0.51001, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.5100 - accuracy: 0.8639 - val_loss: 0.5023 - val_accuracy: 0.8902\n",
            "Epoch 15/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.4349 - accuracy: 0.8715\n",
            "Epoch 00015: loss improved from 0.51001 to 0.43136, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 1s 26ms/step - loss: 0.4314 - accuracy: 0.8762 - val_loss: 0.4018 - val_accuracy: 0.9065\n",
            "Epoch 16/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.3559 - accuracy: 0.9211\n",
            "Epoch 00016: loss improved from 0.43136 to 0.35432, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.3543 - accuracy: 0.9197 - val_loss: 0.3203 - val_accuracy: 0.9187\n",
            "Epoch 17/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.3885 - accuracy: 0.9149\n",
            "Epoch 00017: loss did not improve from 0.35432\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.3822 - accuracy: 0.9170 - val_loss: 0.3628 - val_accuracy: 0.9187\n",
            "Epoch 18/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.3862 - accuracy: 0.9079\n",
            "Epoch 00018: loss did not improve from 0.35432\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.3665 - accuracy: 0.9170 - val_loss: 0.2654 - val_accuracy: 0.9593\n",
            "Epoch 19/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.3379 - accuracy: 0.9276\n",
            "Epoch 00019: loss improved from 0.35432 to 0.34087, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.3409 - accuracy: 0.9306 - val_loss: 0.3287 - val_accuracy: 0.9472\n",
            "Epoch 20/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2983 - accuracy: 0.9490\n",
            "Epoch 00020: loss improved from 0.34087 to 0.29910, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.2991 - accuracy: 0.9510 - val_loss: 0.2799 - val_accuracy: 0.9593\n",
            "Epoch 21/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.3230 - accuracy: 0.9326\n",
            "Epoch 00021: loss did not improve from 0.29910\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.3156 - accuracy: 0.9374 - val_loss: 0.2621 - val_accuracy: 0.9593\n",
            "Epoch 22/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.2025 - accuracy: 0.9792\n",
            "Epoch 00022: loss improved from 0.29910 to 0.22872, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.2287 - accuracy: 0.9660 - val_loss: 0.2101 - val_accuracy: 0.9715\n",
            "Epoch 23/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2447 - accuracy: 0.9622\n",
            "Epoch 00023: loss did not improve from 0.22872\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.2374 - accuracy: 0.9646 - val_loss: 0.2051 - val_accuracy: 0.9797\n",
            "Epoch 24/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2189 - accuracy: 0.9688\n",
            "Epoch 00024: loss improved from 0.22872 to 0.21863, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.2186 - accuracy: 0.9714 - val_loss: 0.2252 - val_accuracy: 0.9837\n",
            "Epoch 25/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2163 - accuracy: 0.9770\n",
            "Epoch 00025: loss improved from 0.21863 to 0.21509, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.2151 - accuracy: 0.9769 - val_loss: 0.1984 - val_accuracy: 0.9756\n",
            "Epoch 26/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.2373 - accuracy: 0.9653\n",
            "Epoch 00026: loss did not improve from 0.21509\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.2404 - accuracy: 0.9619 - val_loss: 0.2620 - val_accuracy: 0.9431\n",
            "Epoch 27/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.2645 - accuracy: 0.9635\n",
            "Epoch 00027: loss did not improve from 0.21509\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.2580 - accuracy: 0.9633 - val_loss: 0.1944 - val_accuracy: 0.9919\n",
            "Epoch 28/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2068 - accuracy: 0.9671\n",
            "Epoch 00028: loss improved from 0.21509 to 0.21016, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.2102 - accuracy: 0.9673 - val_loss: 0.2274 - val_accuracy: 0.9756\n",
            "Epoch 29/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.2000 - accuracy: 0.9757\n",
            "Epoch 00029: loss improved from 0.21016 to 0.20954, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.2095 - accuracy: 0.9728 - val_loss: 0.2078 - val_accuracy: 0.9675\n",
            "Epoch 30/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1953 - accuracy: 0.9852\n",
            "Epoch 00030: loss improved from 0.20954 to 0.18846, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1885 - accuracy: 0.9878 - val_loss: 0.1692 - val_accuracy: 0.9919\n",
            "Epoch 31/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2111 - accuracy: 0.9688\n",
            "Epoch 00031: loss did not improve from 0.18846\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.2138 - accuracy: 0.9687 - val_loss: 0.2271 - val_accuracy: 0.9634\n",
            "Epoch 32/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1943 - accuracy: 0.9868\n",
            "Epoch 00032: loss improved from 0.18846 to 0.18781, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1878 - accuracy: 0.9891 - val_loss: 0.1935 - val_accuracy: 0.9797\n",
            "Epoch 33/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1869 - accuracy: 0.9792\n",
            "Epoch 00033: loss improved from 0.18781 to 0.18377, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.1838 - accuracy: 0.9810 - val_loss: 0.1693 - val_accuracy: 0.9919\n",
            "Epoch 34/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1558 - accuracy: 0.9934\n",
            "Epoch 00034: loss improved from 0.18377 to 0.16536, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.1654 - accuracy: 0.9878 - val_loss: 0.1543 - val_accuracy: 1.0000\n",
            "Epoch 35/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1782 - accuracy: 0.9836\n",
            "Epoch 00035: loss did not improve from 0.16536\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1831 - accuracy: 0.9810 - val_loss: 0.1763 - val_accuracy: 0.9797\n",
            "Epoch 36/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2653 - accuracy: 0.9539\n",
            "Epoch 00036: loss did not improve from 0.16536\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.2676 - accuracy: 0.9537 - val_loss: 0.2142 - val_accuracy: 0.9837\n",
            "Epoch 37/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1978 - accuracy: 0.9704\n",
            "Epoch 00037: loss did not improve from 0.16536\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.2071 - accuracy: 0.9673 - val_loss: 0.2961 - val_accuracy: 0.9390\n",
            "Epoch 38/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2104 - accuracy: 0.9737\n",
            "Epoch 00038: loss did not improve from 0.16536\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.2015 - accuracy: 0.9782 - val_loss: 0.1895 - val_accuracy: 0.9756\n",
            "Epoch 39/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1803 - accuracy: 0.9819\n",
            "Epoch 00039: loss did not improve from 0.16536\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1838 - accuracy: 0.9823 - val_loss: 0.1812 - val_accuracy: 0.9919\n",
            "Epoch 40/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1569 - accuracy: 0.9918\n",
            "Epoch 00040: loss improved from 0.16536 to 0.15442, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.1544 - accuracy: 0.9932 - val_loss: 0.1732 - val_accuracy: 0.9797\n",
            "Epoch 41/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1585 - accuracy: 0.9913\n",
            "Epoch 00041: loss did not improve from 0.15442\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1547 - accuracy: 0.9918 - val_loss: 0.1677 - val_accuracy: 0.9797\n",
            "Epoch 42/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1543 - accuracy: 0.9885\n",
            "Epoch 00042: loss improved from 0.15442 to 0.15296, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.1530 - accuracy: 0.9891 - val_loss: 0.1741 - val_accuracy: 0.9797\n",
            "Epoch 43/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1376 - accuracy: 0.9948\n",
            "Epoch 00043: loss improved from 0.15296 to 0.14664, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.1466 - accuracy: 0.9918 - val_loss: 0.1483 - val_accuracy: 0.9837\n",
            "Epoch 44/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1466 - accuracy: 0.9901\n",
            "Epoch 00044: loss improved from 0.14664 to 0.14643, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.1464 - accuracy: 0.9905 - val_loss: 0.1561 - val_accuracy: 0.9837\n",
            "Epoch 45/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1487 - accuracy: 0.9901\n",
            "Epoch 00045: loss improved from 0.14643 to 0.14524, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.1452 - accuracy: 0.9918 - val_loss: 0.1611 - val_accuracy: 0.9837\n",
            "Epoch 46/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1276 - accuracy: 0.9983\n",
            "Epoch 00046: loss improved from 0.14524 to 0.12886, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.1289 - accuracy: 0.9959 - val_loss: 0.1560 - val_accuracy: 0.9797\n",
            "Epoch 47/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1232 - accuracy: 0.9984\n",
            "Epoch 00047: loss improved from 0.12886 to 0.12788, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.1279 - accuracy: 0.9959 - val_loss: 0.1341 - val_accuracy: 0.9878\n",
            "Epoch 48/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1340 - accuracy: 0.9918\n",
            "Epoch 00048: loss did not improve from 0.12788\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1308 - accuracy: 0.9932 - val_loss: 0.1387 - val_accuracy: 0.9959\n",
            "Epoch 49/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1344 - accuracy: 0.9951\n",
            "Epoch 00049: loss did not improve from 0.12788\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1333 - accuracy: 0.9946 - val_loss: 0.1351 - val_accuracy: 0.9878\n",
            "Epoch 50/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1205 - accuracy: 0.9967\n",
            "Epoch 00050: loss improved from 0.12788 to 0.12009, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 1s 24ms/step - loss: 0.1201 - accuracy: 0.9959 - val_loss: 0.1328 - val_accuracy: 0.9878\n",
            "Epoch 51/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1412 - accuracy: 0.9868\n",
            "Epoch 00051: loss did not improve from 0.12009\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1379 - accuracy: 0.9891 - val_loss: 0.1329 - val_accuracy: 0.9919\n",
            "Epoch 52/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1222 - accuracy: 0.9934\n",
            "Epoch 00052: loss did not improve from 0.12009\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1232 - accuracy: 0.9932 - val_loss: 0.1791 - val_accuracy: 0.9675\n",
            "Epoch 53/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1574 - accuracy: 0.9786\n",
            "Epoch 00053: loss did not improve from 0.12009\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1526 - accuracy: 0.9810 - val_loss: 0.1396 - val_accuracy: 0.9919\n",
            "Epoch 54/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1349 - accuracy: 0.9934\n",
            "Epoch 00054: loss did not improve from 0.12009\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1371 - accuracy: 0.9918 - val_loss: 0.1265 - val_accuracy: 0.9959\n",
            "Epoch 55/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1459 - accuracy: 0.9819\n",
            "Epoch 00055: loss did not improve from 0.12009\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1478 - accuracy: 0.9823 - val_loss: 0.1840 - val_accuracy: 0.9715\n",
            "Epoch 56/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1506 - accuracy: 0.9852\n",
            "Epoch 00056: loss did not improve from 0.12009\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1529 - accuracy: 0.9823 - val_loss: 0.1417 - val_accuracy: 0.9878\n",
            "Epoch 57/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1286 - accuracy: 0.9918\n",
            "Epoch 00057: loss did not improve from 0.12009\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1270 - accuracy: 0.9918 - val_loss: 0.1267 - val_accuracy: 0.9919\n",
            "Epoch 58/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1252 - accuracy: 0.9918\n",
            "Epoch 00058: loss did not improve from 0.12009\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1230 - accuracy: 0.9932 - val_loss: 0.1437 - val_accuracy: 0.9837\n",
            "Epoch 59/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1136 - accuracy: 0.9984\n",
            "Epoch 00059: loss improved from 0.12009 to 0.11242, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.1124 - accuracy: 0.9986 - val_loss: 0.1517 - val_accuracy: 0.9837\n",
            "Epoch 60/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1182 - accuracy: 0.9934\n",
            "Epoch 00060: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1221 - accuracy: 0.9932 - val_loss: 0.1349 - val_accuracy: 0.9756\n",
            "Epoch 61/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1198 - accuracy: 0.9918\n",
            "Epoch 00061: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1270 - accuracy: 0.9878 - val_loss: 0.1278 - val_accuracy: 0.9919\n",
            "Epoch 62/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1136 - accuracy: 0.9951\n",
            "Epoch 00062: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1140 - accuracy: 0.9946 - val_loss: 0.1446 - val_accuracy: 0.9837\n",
            "Epoch 63/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1236 - accuracy: 0.9951\n",
            "Epoch 00063: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1242 - accuracy: 0.9932 - val_loss: 0.1226 - val_accuracy: 0.9919\n",
            "Epoch 64/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1183 - accuracy: 0.9918\n",
            "Epoch 00064: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1211 - accuracy: 0.9918 - val_loss: 0.1301 - val_accuracy: 0.9837\n",
            "Epoch 65/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1136 - accuracy: 0.9901\n",
            "Epoch 00065: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1147 - accuracy: 0.9891 - val_loss: 0.1305 - val_accuracy: 0.9878\n",
            "Epoch 66/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1174 - accuracy: 0.9934\n",
            "Epoch 00066: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1164 - accuracy: 0.9946 - val_loss: 0.1210 - val_accuracy: 0.9919\n",
            "Epoch 67/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1371 - accuracy: 0.9901\n",
            "Epoch 00067: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1393 - accuracy: 0.9905 - val_loss: 0.1353 - val_accuracy: 0.9919\n",
            "Epoch 68/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1279 - accuracy: 0.9913\n",
            "Epoch 00068: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1267 - accuracy: 0.9918 - val_loss: 0.1368 - val_accuracy: 0.9878\n",
            "Epoch 69/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1382 - accuracy: 0.9885\n",
            "Epoch 00069: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1389 - accuracy: 0.9878 - val_loss: 0.1370 - val_accuracy: 0.9837\n",
            "Epoch 70/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1294 - accuracy: 0.9901\n",
            "Epoch 00070: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1289 - accuracy: 0.9891 - val_loss: 0.1538 - val_accuracy: 0.9756\n",
            "Epoch 71/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1819 - accuracy: 0.9688\n",
            "Epoch 00071: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1738 - accuracy: 0.9741 - val_loss: 0.1958 - val_accuracy: 0.9715\n",
            "Epoch 72/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1390 - accuracy: 0.9868\n",
            "Epoch 00072: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1433 - accuracy: 0.9823 - val_loss: 0.1247 - val_accuracy: 0.9959\n",
            "Epoch 73/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1438 - accuracy: 0.9868\n",
            "Epoch 00073: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1450 - accuracy: 0.9864 - val_loss: 0.1329 - val_accuracy: 0.9878\n",
            "Epoch 74/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1375 - accuracy: 0.9901\n",
            "Epoch 00074: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1373 - accuracy: 0.9891 - val_loss: 0.1364 - val_accuracy: 0.9878\n",
            "Epoch 75/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1155 - accuracy: 0.9967\n",
            "Epoch 00075: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1251 - accuracy: 0.9918 - val_loss: 0.1532 - val_accuracy: 0.9715\n",
            "Epoch 76/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1203 - accuracy: 0.9901\n",
            "Epoch 00076: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1245 - accuracy: 0.9891 - val_loss: 0.1271 - val_accuracy: 0.9959\n",
            "Epoch 77/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1363 - accuracy: 0.9852\n",
            "Epoch 00077: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1318 - accuracy: 0.9878 - val_loss: 0.1998 - val_accuracy: 0.9675\n",
            "Epoch 78/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1135 - accuracy: 0.9951\n",
            "Epoch 00078: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1184 - accuracy: 0.9932 - val_loss: 0.1132 - val_accuracy: 0.9919\n",
            "Epoch 79/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1395 - accuracy: 0.9861\n",
            "Epoch 00079: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1342 - accuracy: 0.9891 - val_loss: 0.1486 - val_accuracy: 0.9756\n",
            "Epoch 80/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1159 - accuracy: 0.9967\n",
            "Epoch 00080: loss did not improve from 0.11242\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1140 - accuracy: 0.9973 - val_loss: 0.1265 - val_accuracy: 0.9919\n",
            "Epoch 81/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1037 - accuracy: 0.9984\n",
            "Epoch 00081: loss improved from 0.11242 to 0.10443, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.1044 - accuracy: 0.9973 - val_loss: 0.1180 - val_accuracy: 0.9959\n",
            "Epoch 82/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1041 - accuracy: 0.9967\n",
            "Epoch 00082: loss did not improve from 0.10443\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1047 - accuracy: 0.9973 - val_loss: 0.1137 - val_accuracy: 0.9919\n",
            "Epoch 83/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0986 - accuracy: 0.9984\n",
            "Epoch 00083: loss improved from 0.10443 to 0.09892, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0989 - accuracy: 0.9986 - val_loss: 0.1278 - val_accuracy: 0.9756\n",
            "Epoch 84/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1151 - accuracy: 0.9918\n",
            "Epoch 00084: loss did not improve from 0.09892\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1167 - accuracy: 0.9918 - val_loss: 0.1382 - val_accuracy: 0.9756\n",
            "Epoch 85/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1013 - accuracy: 0.9983\n",
            "Epoch 00085: loss did not improve from 0.09892\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1014 - accuracy: 0.9973 - val_loss: 0.1278 - val_accuracy: 0.9878\n",
            "Epoch 86/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1047 - accuracy: 0.9918\n",
            "Epoch 00086: loss did not improve from 0.09892\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1053 - accuracy: 0.9918 - val_loss: 0.1129 - val_accuracy: 0.9919\n",
            "Epoch 87/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1150 - accuracy: 0.9885\n",
            "Epoch 00087: loss did not improve from 0.09892\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1123 - accuracy: 0.9905 - val_loss: 0.1077 - val_accuracy: 0.9919\n",
            "Epoch 88/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0960 - accuracy: 0.9984\n",
            "Epoch 00088: loss improved from 0.09892 to 0.09573, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0957 - accuracy: 0.9986 - val_loss: 0.0945 - val_accuracy: 1.0000\n",
            "Epoch 89/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0922 - accuracy: 0.9984\n",
            "Epoch 00089: loss improved from 0.09573 to 0.09192, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0919 - accuracy: 0.9986 - val_loss: 0.0918 - val_accuracy: 1.0000\n",
            "Epoch 90/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0912 - accuracy: 0.9983\n",
            "Epoch 00090: loss improved from 0.09192 to 0.09105, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0911 - accuracy: 0.9986 - val_loss: 0.0862 - val_accuracy: 1.0000\n",
            "Epoch 91/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0888 - accuracy: 0.9967\n",
            "Epoch 00091: loss improved from 0.09105 to 0.08850, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0885 - accuracy: 0.9959 - val_loss: 0.0937 - val_accuracy: 0.9959\n",
            "Epoch 92/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0871 - accuracy: 0.9967\n",
            "Epoch 00092: loss improved from 0.08850 to 0.08707, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.0871 - accuracy: 0.9959 - val_loss: 0.0880 - val_accuracy: 0.9959\n",
            "Epoch 93/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1252 - accuracy: 0.9836\n",
            "Epoch 00093: loss did not improve from 0.08707\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1283 - accuracy: 0.9823 - val_loss: 0.1517 - val_accuracy: 0.9756\n",
            "Epoch 94/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1121 - accuracy: 0.9934\n",
            "Epoch 00094: loss did not improve from 0.08707\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1124 - accuracy: 0.9918 - val_loss: 0.1009 - val_accuracy: 0.9959\n",
            "Epoch 95/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1119 - accuracy: 0.9948\n",
            "Epoch 00095: loss did not improve from 0.08707\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1423 - accuracy: 0.9796 - val_loss: 0.1170 - val_accuracy: 0.9959\n",
            "Epoch 96/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1321 - accuracy: 0.9885\n",
            "Epoch 00096: loss did not improve from 0.08707\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1303 - accuracy: 0.9891 - val_loss: 0.1311 - val_accuracy: 0.9878\n",
            "Epoch 97/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1260 - accuracy: 0.9852\n",
            "Epoch 00097: loss did not improve from 0.08707\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1284 - accuracy: 0.9837 - val_loss: 0.1568 - val_accuracy: 0.9837\n",
            "Epoch 98/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1114 - accuracy: 0.9948\n",
            "Epoch 00098: loss did not improve from 0.08707\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1100 - accuracy: 0.9932 - val_loss: 0.1225 - val_accuracy: 0.9878\n",
            "Epoch 99/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0964 - accuracy: 1.0000\n",
            "Epoch 00099: loss did not improve from 0.08707\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0955 - accuracy: 1.0000 - val_loss: 0.1022 - val_accuracy: 0.9919\n",
            "Epoch 100/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0968 - accuracy: 0.9967\n",
            "Epoch 00100: loss did not improve from 0.08707\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0959 - accuracy: 0.9973 - val_loss: 0.1193 - val_accuracy: 0.9837\n",
            "Epoch 101/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1252 - accuracy: 0.9868\n",
            "Epoch 00101: loss did not improve from 0.08707\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1291 - accuracy: 0.9878 - val_loss: 0.1281 - val_accuracy: 0.9837\n",
            "Epoch 102/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1147 - accuracy: 0.9901\n",
            "Epoch 00102: loss did not improve from 0.08707\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1157 - accuracy: 0.9891 - val_loss: 0.1438 - val_accuracy: 0.9797\n",
            "Epoch 103/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1119 - accuracy: 0.9905\n",
            "Epoch 00103: loss did not improve from 0.08707\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1119 - accuracy: 0.9905 - val_loss: 0.1188 - val_accuracy: 0.9878\n",
            "Epoch 104/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1299 - accuracy: 0.9819\n",
            "Epoch 00104: loss did not improve from 0.08707\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1272 - accuracy: 0.9837 - val_loss: 0.1327 - val_accuracy: 0.9837\n",
            "Epoch 105/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1029 - accuracy: 0.9951\n",
            "Epoch 00105: loss did not improve from 0.08707\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1029 - accuracy: 0.9932 - val_loss: 0.0967 - val_accuracy: 0.9959\n",
            "Epoch 106/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0916 - accuracy: 1.0000\n",
            "Epoch 00106: loss did not improve from 0.08707\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0915 - accuracy: 1.0000 - val_loss: 0.1015 - val_accuracy: 0.9919\n",
            "Epoch 107/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0901 - accuracy: 0.9967\n",
            "Epoch 00107: loss did not improve from 0.08707\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0902 - accuracy: 0.9973 - val_loss: 0.1082 - val_accuracy: 0.9837\n",
            "Epoch 108/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0870 - accuracy: 0.9983\n",
            "Epoch 00108: loss improved from 0.08707 to 0.08688, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0869 - accuracy: 0.9986 - val_loss: 0.1063 - val_accuracy: 0.9878\n",
            "Epoch 109/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0836 - accuracy: 0.9984\n",
            "Epoch 00109: loss improved from 0.08688 to 0.08409, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0841 - accuracy: 0.9973 - val_loss: 0.0942 - val_accuracy: 0.9919\n",
            "Epoch 110/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0830 - accuracy: 0.9984\n",
            "Epoch 00110: loss improved from 0.08409 to 0.08378, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0838 - accuracy: 0.9973 - val_loss: 0.0973 - val_accuracy: 0.9919\n",
            "Epoch 111/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0868 - accuracy: 0.9967\n",
            "Epoch 00111: loss did not improve from 0.08378\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0859 - accuracy: 0.9973 - val_loss: 0.1131 - val_accuracy: 0.9837\n",
            "Epoch 112/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0973 - accuracy: 0.9918\n",
            "Epoch 00112: loss did not improve from 0.08378\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0986 - accuracy: 0.9918 - val_loss: 0.1041 - val_accuracy: 0.9919\n",
            "Epoch 113/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0905 - accuracy: 0.9967\n",
            "Epoch 00113: loss did not improve from 0.08378\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0886 - accuracy: 0.9973 - val_loss: 0.0847 - val_accuracy: 0.9959\n",
            "Epoch 114/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0854 - accuracy: 0.9934\n",
            "Epoch 00114: loss did not improve from 0.08378\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0882 - accuracy: 0.9932 - val_loss: 0.0891 - val_accuracy: 0.9959\n",
            "Epoch 115/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0842 - accuracy: 0.9951\n",
            "Epoch 00115: loss improved from 0.08378 to 0.08299, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0830 - accuracy: 0.9959 - val_loss: 0.1107 - val_accuracy: 0.9837\n",
            "Epoch 116/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0820 - accuracy: 0.9967\n",
            "Epoch 00116: loss improved from 0.08299 to 0.08142, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0814 - accuracy: 0.9973 - val_loss: 0.0873 - val_accuracy: 0.9959\n",
            "Epoch 117/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0821 - accuracy: 0.9951\n",
            "Epoch 00117: loss did not improve from 0.08142\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0824 - accuracy: 0.9946 - val_loss: 0.1047 - val_accuracy: 0.9837\n",
            "Epoch 118/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0834 - accuracy: 0.9967\n",
            "Epoch 00118: loss did not improve from 0.08142\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0819 - accuracy: 0.9973 - val_loss: 0.0938 - val_accuracy: 0.9919\n",
            "Epoch 119/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0767 - accuracy: 0.9984\n",
            "Epoch 00119: loss improved from 0.08142 to 0.07566, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0757 - accuracy: 0.9986 - val_loss: 0.1015 - val_accuracy: 0.9878\n",
            "Epoch 120/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0734 - accuracy: 1.0000\n",
            "Epoch 00120: loss improved from 0.07566 to 0.07303, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0730 - accuracy: 1.0000 - val_loss: 0.0841 - val_accuracy: 0.9959\n",
            "Epoch 121/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0726 - accuracy: 0.9984\n",
            "Epoch 00121: loss improved from 0.07303 to 0.07216, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0722 - accuracy: 0.9986 - val_loss: 0.0774 - val_accuracy: 1.0000\n",
            "Epoch 122/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0718 - accuracy: 1.0000\n",
            "Epoch 00122: loss did not improve from 0.07216\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0722 - accuracy: 1.0000 - val_loss: 0.0814 - val_accuracy: 0.9919\n",
            "Epoch 123/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0751 - accuracy: 0.9967\n",
            "Epoch 00123: loss did not improve from 0.07216\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0733 - accuracy: 0.9973 - val_loss: 0.0783 - val_accuracy: 0.9959\n",
            "Epoch 124/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0716 - accuracy: 0.9984\n",
            "Epoch 00124: loss improved from 0.07216 to 0.07059, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0706 - accuracy: 0.9986 - val_loss: 0.1063 - val_accuracy: 0.9837\n",
            "Epoch 125/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0748 - accuracy: 0.9967\n",
            "Epoch 00125: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0744 - accuracy: 0.9959 - val_loss: 0.0850 - val_accuracy: 0.9919\n",
            "Epoch 126/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0782 - accuracy: 0.9934\n",
            "Epoch 00126: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0802 - accuracy: 0.9918 - val_loss: 0.0804 - val_accuracy: 0.9959\n",
            "Epoch 127/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9878\n",
            "Epoch 00127: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1005 - accuracy: 0.9878 - val_loss: 0.0993 - val_accuracy: 0.9878\n",
            "Epoch 128/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0784 - accuracy: 0.9967\n",
            "Epoch 00128: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0774 - accuracy: 0.9973 - val_loss: 0.1139 - val_accuracy: 0.9837\n",
            "Epoch 129/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1040 - accuracy: 0.9901\n",
            "Epoch 00129: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1206 - accuracy: 0.9837 - val_loss: 0.1389 - val_accuracy: 0.9837\n",
            "Epoch 130/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1310 - accuracy: 0.9836\n",
            "Epoch 00130: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1306 - accuracy: 0.9837 - val_loss: 0.1058 - val_accuracy: 0.9919\n",
            "Epoch 131/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1301 - accuracy: 0.9786\n",
            "Epoch 00131: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1446 - accuracy: 0.9755 - val_loss: 0.2015 - val_accuracy: 0.9797\n",
            "Epoch 132/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1423 - accuracy: 0.9809\n",
            "Epoch 00132: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1396 - accuracy: 0.9810 - val_loss: 0.1311 - val_accuracy: 0.9837\n",
            "Epoch 133/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1027 - accuracy: 0.9934\n",
            "Epoch 00133: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1013 - accuracy: 0.9946 - val_loss: 0.1227 - val_accuracy: 0.9837\n",
            "Epoch 134/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0947 - accuracy: 0.9951\n",
            "Epoch 00134: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1017 - accuracy: 0.9905 - val_loss: 0.1145 - val_accuracy: 0.9878\n",
            "Epoch 135/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1201 - accuracy: 0.9868\n",
            "Epoch 00135: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1179 - accuracy: 0.9878 - val_loss: 0.1045 - val_accuracy: 0.9959\n",
            "Epoch 136/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0936 - accuracy: 0.9965\n",
            "Epoch 00136: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0916 - accuracy: 0.9973 - val_loss: 0.1143 - val_accuracy: 0.9878\n",
            "Epoch 137/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0953 - accuracy: 0.9983\n",
            "Epoch 00137: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0925 - accuracy: 0.9986 - val_loss: 0.0908 - val_accuracy: 1.0000\n",
            "Epoch 138/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0845 - accuracy: 0.9984\n",
            "Epoch 00138: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0876 - accuracy: 0.9973 - val_loss: 0.1007 - val_accuracy: 0.9919\n",
            "Epoch 139/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0997 - accuracy: 0.9934\n",
            "Epoch 00139: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1052 - accuracy: 0.9905 - val_loss: 0.1213 - val_accuracy: 0.9837\n",
            "Epoch 140/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1049 - accuracy: 0.9901\n",
            "Epoch 00140: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1142 - accuracy: 0.9864 - val_loss: 0.1299 - val_accuracy: 0.9878\n",
            "Epoch 141/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1080 - accuracy: 0.9918\n",
            "Epoch 00141: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1067 - accuracy: 0.9918 - val_loss: 0.0878 - val_accuracy: 1.0000\n",
            "Epoch 142/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0855 - accuracy: 0.9984\n",
            "Epoch 00142: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0853 - accuracy: 0.9986 - val_loss: 0.1079 - val_accuracy: 0.9878\n",
            "Epoch 143/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0883 - accuracy: 0.9984\n",
            "Epoch 00143: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0919 - accuracy: 0.9959 - val_loss: 0.0880 - val_accuracy: 1.0000\n",
            "Epoch 144/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1271 - accuracy: 0.9861\n",
            "Epoch 00144: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1213 - accuracy: 0.9891 - val_loss: 0.1299 - val_accuracy: 0.9837\n",
            "Epoch 145/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0997 - accuracy: 0.9948\n",
            "Epoch 00145: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0966 - accuracy: 0.9959 - val_loss: 0.1003 - val_accuracy: 0.9959\n",
            "Epoch 146/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0969 - accuracy: 0.9918\n",
            "Epoch 00146: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0994 - accuracy: 0.9905 - val_loss: 0.0972 - val_accuracy: 0.9959\n",
            "Epoch 147/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0951 - accuracy: 0.9951\n",
            "Epoch 00147: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0939 - accuracy: 0.9959 - val_loss: 0.1152 - val_accuracy: 0.9878\n",
            "Epoch 148/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0924 - accuracy: 0.9946\n",
            "Epoch 00148: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0924 - accuracy: 0.9946 - val_loss: 0.0921 - val_accuracy: 0.9959\n",
            "Epoch 149/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0885 - accuracy: 0.9951\n",
            "Epoch 00149: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0904 - accuracy: 0.9946 - val_loss: 0.0802 - val_accuracy: 1.0000\n",
            "Epoch 150/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0879 - accuracy: 0.9967\n",
            "Epoch 00150: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0869 - accuracy: 0.9973 - val_loss: 0.0915 - val_accuracy: 0.9919\n",
            "Epoch 151/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0791 - accuracy: 1.0000\n",
            "Epoch 00151: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0784 - accuracy: 1.0000 - val_loss: 0.0790 - val_accuracy: 1.0000\n",
            "Epoch 152/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0809 - accuracy: 0.9948\n",
            "Epoch 00152: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0787 - accuracy: 0.9959 - val_loss: 0.0760 - val_accuracy: 0.9959\n",
            "Epoch 153/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0719 - accuracy: 1.0000\n",
            "Epoch 00153: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0718 - accuracy: 1.0000 - val_loss: 0.0771 - val_accuracy: 1.0000\n",
            "Epoch 154/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0755 - accuracy: 0.9967\n",
            "Epoch 00154: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0770 - accuracy: 0.9959 - val_loss: 0.0917 - val_accuracy: 0.9959\n",
            "Epoch 155/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0773 - accuracy: 0.9984\n",
            "Epoch 00155: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0767 - accuracy: 0.9986 - val_loss: 0.0921 - val_accuracy: 0.9878\n",
            "Epoch 156/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0704 - accuracy: 1.0000\n",
            "Epoch 00156: loss improved from 0.07059 to 0.07059, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0706 - accuracy: 1.0000 - val_loss: 0.0837 - val_accuracy: 0.9919\n",
            "Epoch 157/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0749 - accuracy: 0.9967\n",
            "Epoch 00157: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0849 - accuracy: 0.9946 - val_loss: 0.0732 - val_accuracy: 1.0000\n",
            "Epoch 158/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1290 - accuracy: 0.9836\n",
            "Epoch 00158: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1214 - accuracy: 0.9864 - val_loss: 0.0948 - val_accuracy: 0.9959\n",
            "Epoch 159/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1114 - accuracy: 0.9852\n",
            "Epoch 00159: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1130 - accuracy: 0.9850 - val_loss: 0.1175 - val_accuracy: 0.9878\n",
            "Epoch 160/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0877 - accuracy: 0.9948\n",
            "Epoch 00160: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0865 - accuracy: 0.9946 - val_loss: 0.0811 - val_accuracy: 1.0000\n",
            "Epoch 161/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0808 - accuracy: 0.9951\n",
            "Epoch 00161: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0821 - accuracy: 0.9946 - val_loss: 0.0766 - val_accuracy: 1.0000\n",
            "Epoch 162/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0804 - accuracy: 0.9967\n",
            "Epoch 00162: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0810 - accuracy: 0.9973 - val_loss: 0.0867 - val_accuracy: 0.9959\n",
            "Epoch 163/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0761 - accuracy: 1.0000\n",
            "Epoch 00163: loss did not improve from 0.07059\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0749 - accuracy: 1.0000 - val_loss: 0.0787 - val_accuracy: 0.9959\n",
            "Epoch 164/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0695 - accuracy: 1.0000\n",
            "Epoch 00164: loss improved from 0.07059 to 0.06992, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.0699 - accuracy: 1.0000 - val_loss: 0.0712 - val_accuracy: 1.0000\n",
            "Epoch 165/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0688 - accuracy: 1.0000\n",
            "Epoch 00165: loss improved from 0.06992 to 0.06857, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0686 - accuracy: 1.0000 - val_loss: 0.0704 - val_accuracy: 1.0000\n",
            "Epoch 166/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0658 - accuracy: 1.0000\n",
            "Epoch 00166: loss improved from 0.06857 to 0.06672, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0667 - accuracy: 1.0000 - val_loss: 0.0700 - val_accuracy: 0.9959\n",
            "Epoch 167/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0690 - accuracy: 0.9965\n",
            "Epoch 00167: loss did not improve from 0.06672\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0692 - accuracy: 0.9973 - val_loss: 0.0986 - val_accuracy: 0.9919\n",
            "Epoch 168/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0737 - accuracy: 0.9967\n",
            "Epoch 00168: loss did not improve from 0.06672\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0756 - accuracy: 0.9959 - val_loss: 0.0810 - val_accuracy: 0.9919\n",
            "Epoch 169/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0785 - accuracy: 0.9934\n",
            "Epoch 00169: loss did not improve from 0.06672\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0774 - accuracy: 0.9946 - val_loss: 0.0858 - val_accuracy: 0.9919\n",
            "Epoch 170/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0720 - accuracy: 0.9934\n",
            "Epoch 00170: loss did not improve from 0.06672\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0726 - accuracy: 0.9946 - val_loss: 0.0800 - val_accuracy: 0.9959\n",
            "Epoch 171/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0713 - accuracy: 0.9965\n",
            "Epoch 00171: loss did not improve from 0.06672\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0710 - accuracy: 0.9973 - val_loss: 0.0820 - val_accuracy: 0.9919\n",
            "Epoch 172/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0694 - accuracy: 0.9984\n",
            "Epoch 00172: loss did not improve from 0.06672\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0708 - accuracy: 0.9986 - val_loss: 0.0795 - val_accuracy: 0.9919\n",
            "Epoch 173/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0690 - accuracy: 0.9984\n",
            "Epoch 00173: loss did not improve from 0.06672\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0686 - accuracy: 0.9986 - val_loss: 0.0726 - val_accuracy: 0.9959\n",
            "Epoch 174/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0652 - accuracy: 1.0000\n",
            "Epoch 00174: loss improved from 0.06672 to 0.06445, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0645 - accuracy: 1.0000 - val_loss: 0.0833 - val_accuracy: 0.9919\n",
            "Epoch 175/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0843 - accuracy: 0.9901\n",
            "Epoch 00175: loss did not improve from 0.06445\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0843 - accuracy: 0.9905 - val_loss: 0.0884 - val_accuracy: 0.9878\n",
            "Epoch 176/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0886 - accuracy: 0.9901\n",
            "Epoch 00176: loss did not improve from 0.06445\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0906 - accuracy: 0.9878 - val_loss: 0.1494 - val_accuracy: 0.9715\n",
            "Epoch 177/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0850 - accuracy: 0.9934\n",
            "Epoch 00177: loss did not improve from 0.06445\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0850 - accuracy: 0.9932 - val_loss: 0.1043 - val_accuracy: 0.9837\n",
            "Epoch 178/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0835 - accuracy: 0.9901\n",
            "Epoch 00178: loss did not improve from 0.06445\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0808 - accuracy: 0.9918 - val_loss: 0.0795 - val_accuracy: 0.9959\n",
            "Epoch 179/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9973\n",
            "Epoch 00179: loss did not improve from 0.06445\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0690 - accuracy: 0.9973 - val_loss: 0.0774 - val_accuracy: 0.9959\n",
            "Epoch 180/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0707 - accuracy: 0.9984\n",
            "Epoch 00180: loss did not improve from 0.06445\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0725 - accuracy: 0.9973 - val_loss: 0.0694 - val_accuracy: 1.0000\n",
            "Epoch 181/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0764 - accuracy: 0.9967\n",
            "Epoch 00181: loss did not improve from 0.06445\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0753 - accuracy: 0.9959 - val_loss: 0.0774 - val_accuracy: 0.9919\n",
            "Epoch 182/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0673 - accuracy: 1.0000\n",
            "Epoch 00182: loss did not improve from 0.06445\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0676 - accuracy: 1.0000 - val_loss: 0.0690 - val_accuracy: 1.0000\n",
            "Epoch 183/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0637 - accuracy: 0.9984\n",
            "Epoch 00183: loss improved from 0.06445 to 0.06289, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0629 - accuracy: 0.9986 - val_loss: 0.0702 - val_accuracy: 0.9959\n",
            "Epoch 184/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0772 - accuracy: 0.9934\n",
            "Epoch 00184: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0770 - accuracy: 0.9932 - val_loss: 0.0860 - val_accuracy: 0.9837\n",
            "Epoch 185/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0749 - accuracy: 0.9934\n",
            "Epoch 00185: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0755 - accuracy: 0.9932 - val_loss: 0.0653 - val_accuracy: 1.0000\n",
            "Epoch 186/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0769 - accuracy: 0.9967\n",
            "Epoch 00186: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0781 - accuracy: 0.9946 - val_loss: 0.0819 - val_accuracy: 0.9919\n",
            "Epoch 187/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1122 - accuracy: 0.9836\n",
            "Epoch 00187: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1219 - accuracy: 0.9810 - val_loss: 0.1329 - val_accuracy: 0.9756\n",
            "Epoch 188/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0930 - accuracy: 0.9913\n",
            "Epoch 00188: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0871 - accuracy: 0.9932 - val_loss: 0.0947 - val_accuracy: 0.9878\n",
            "Epoch 189/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0711 - accuracy: 1.0000\n",
            "Epoch 00189: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0708 - accuracy: 1.0000 - val_loss: 0.0814 - val_accuracy: 0.9919\n",
            "Epoch 190/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0921 - accuracy: 0.9918\n",
            "Epoch 00190: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0889 - accuracy: 0.9932 - val_loss: 0.1069 - val_accuracy: 0.9837\n",
            "Epoch 191/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0881 - accuracy: 0.9896\n",
            "Epoch 00191: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0881 - accuracy: 0.9891 - val_loss: 0.0916 - val_accuracy: 0.9959\n",
            "Epoch 192/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0825 - accuracy: 0.9901\n",
            "Epoch 00192: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0830 - accuracy: 0.9905 - val_loss: 0.0726 - val_accuracy: 0.9959\n",
            "Epoch 193/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0797 - accuracy: 0.9967\n",
            "Epoch 00193: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0774 - accuracy: 0.9973 - val_loss: 0.0777 - val_accuracy: 0.9959\n",
            "Epoch 194/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0793 - accuracy: 0.9951\n",
            "Epoch 00194: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0780 - accuracy: 0.9959 - val_loss: 0.0768 - val_accuracy: 0.9959\n",
            "Epoch 195/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0728 - accuracy: 0.9967\n",
            "Epoch 00195: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0730 - accuracy: 0.9973 - val_loss: 0.0708 - val_accuracy: 1.0000\n",
            "Epoch 196/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0688 - accuracy: 0.9984\n",
            "Epoch 00196: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0679 - accuracy: 0.9986 - val_loss: 0.0662 - val_accuracy: 1.0000\n",
            "Epoch 197/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0800 - accuracy: 0.9931\n",
            "Epoch 00197: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0807 - accuracy: 0.9932 - val_loss: 0.0756 - val_accuracy: 0.9959\n",
            "Epoch 198/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0855 - accuracy: 0.9934\n",
            "Epoch 00198: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0864 - accuracy: 0.9932 - val_loss: 0.0747 - val_accuracy: 0.9959\n",
            "Epoch 199/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9946\n",
            "Epoch 00199: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0762 - accuracy: 0.9946 - val_loss: 0.0904 - val_accuracy: 0.9878\n",
            "Epoch 200/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0881 - accuracy: 0.9901\n",
            "Epoch 00200: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0893 - accuracy: 0.9905 - val_loss: 0.1095 - val_accuracy: 0.9878\n",
            "Epoch 201/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0910 - accuracy: 0.9918\n",
            "Epoch 00201: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0882 - accuracy: 0.9932 - val_loss: 0.0762 - val_accuracy: 0.9959\n",
            "Epoch 202/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0735 - accuracy: 0.9967\n",
            "Epoch 00202: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0731 - accuracy: 0.9973 - val_loss: 0.0676 - val_accuracy: 1.0000\n",
            "Epoch 203/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0795 - accuracy: 0.9951\n",
            "Epoch 00203: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0794 - accuracy: 0.9959 - val_loss: 0.0810 - val_accuracy: 0.9919\n",
            "Epoch 204/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0706 - accuracy: 0.9967\n",
            "Epoch 00204: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0693 - accuracy: 0.9973 - val_loss: 0.0801 - val_accuracy: 0.9959\n",
            "Epoch 205/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0659 - accuracy: 1.0000\n",
            "Epoch 00205: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0653 - accuracy: 1.0000 - val_loss: 0.0778 - val_accuracy: 0.9959\n",
            "Epoch 206/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0657 - accuracy: 1.0000\n",
            "Epoch 00206: loss did not improve from 0.06289\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0656 - accuracy: 1.0000 - val_loss: 0.0643 - val_accuracy: 1.0000\n",
            "Epoch 207/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0618 - accuracy: 1.0000\n",
            "Epoch 00207: loss improved from 0.06289 to 0.06168, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0617 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9959\n",
            "Epoch 208/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0625 - accuracy: 0.9984\n",
            "Epoch 00208: loss did not improve from 0.06168\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0625 - accuracy: 0.9986 - val_loss: 0.0660 - val_accuracy: 0.9959\n",
            "Epoch 209/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0587 - accuracy: 1.0000\n",
            "Epoch 00209: loss did not improve from 0.06168\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0624 - accuracy: 0.9986 - val_loss: 0.0603 - val_accuracy: 1.0000\n",
            "Epoch 210/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0672 - accuracy: 0.9967\n",
            "Epoch 00210: loss did not improve from 0.06168\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0702 - accuracy: 0.9959 - val_loss: 0.1150 - val_accuracy: 0.9756\n",
            "Epoch 211/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0785 - accuracy: 0.9918\n",
            "Epoch 00211: loss did not improve from 0.06168\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0767 - accuracy: 0.9932 - val_loss: 0.0760 - val_accuracy: 0.9959\n",
            "Epoch 212/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0813 - accuracy: 0.9951\n",
            "Epoch 00212: loss did not improve from 0.06168\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0801 - accuracy: 0.9946 - val_loss: 0.0658 - val_accuracy: 1.0000\n",
            "Epoch 213/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0630 - accuracy: 1.0000\n",
            "Epoch 00213: loss did not improve from 0.06168\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0634 - accuracy: 0.9986 - val_loss: 0.0757 - val_accuracy: 0.9959\n",
            "Epoch 214/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0601 - accuracy: 1.0000\n",
            "Epoch 00214: loss improved from 0.06168 to 0.05946, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0595 - accuracy: 1.0000 - val_loss: 0.0624 - val_accuracy: 1.0000\n",
            "Epoch 215/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0563 - accuracy: 1.0000\n",
            "Epoch 00215: loss improved from 0.05946 to 0.05850, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0585 - accuracy: 0.9986 - val_loss: 0.0602 - val_accuracy: 1.0000\n",
            "Epoch 216/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0584 - accuracy: 0.9984\n",
            "Epoch 00216: loss improved from 0.05850 to 0.05849, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0585 - accuracy: 0.9986 - val_loss: 0.0605 - val_accuracy: 0.9959\n",
            "Epoch 217/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0592 - accuracy: 0.9984\n",
            "Epoch 00217: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0624 - accuracy: 0.9959 - val_loss: 0.0583 - val_accuracy: 1.0000\n",
            "Epoch 218/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9932\n",
            "Epoch 00218: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0751 - accuracy: 0.9932 - val_loss: 0.0706 - val_accuracy: 1.0000\n",
            "Epoch 219/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0844 - accuracy: 0.9918\n",
            "Epoch 00219: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0831 - accuracy: 0.9918 - val_loss: 0.0881 - val_accuracy: 0.9919\n",
            "Epoch 220/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0658 - accuracy: 0.9984\n",
            "Epoch 00220: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0643 - accuracy: 0.9986 - val_loss: 0.0656 - val_accuracy: 1.0000\n",
            "Epoch 221/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0643 - accuracy: 0.9967\n",
            "Epoch 00221: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0632 - accuracy: 0.9973 - val_loss: 0.0657 - val_accuracy: 1.0000\n",
            "Epoch 222/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0575 - accuracy: 1.0000\n",
            "Epoch 00222: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0616 - accuracy: 0.9986 - val_loss: 0.0620 - val_accuracy: 0.9959\n",
            "Epoch 223/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0843 - accuracy: 0.9901\n",
            "Epoch 00223: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0826 - accuracy: 0.9905 - val_loss: 0.0947 - val_accuracy: 0.9797\n",
            "Epoch 224/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0707 - accuracy: 0.9984\n",
            "Epoch 00224: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0794 - accuracy: 0.9959 - val_loss: 0.0991 - val_accuracy: 0.9837\n",
            "Epoch 225/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1042 - accuracy: 0.9852\n",
            "Epoch 00225: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1078 - accuracy: 0.9850 - val_loss: 0.0826 - val_accuracy: 1.0000\n",
            "Epoch 226/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.9837\n",
            "Epoch 00226: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0962 - accuracy: 0.9837 - val_loss: 0.0804 - val_accuracy: 0.9959\n",
            "Epoch 227/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0721 - accuracy: 0.9984\n",
            "Epoch 00227: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0717 - accuracy: 0.9986 - val_loss: 0.0800 - val_accuracy: 0.9959\n",
            "Epoch 228/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0713 - accuracy: 0.9948\n",
            "Epoch 00228: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0709 - accuracy: 0.9959 - val_loss: 0.0683 - val_accuracy: 1.0000\n",
            "Epoch 229/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0721 - accuracy: 0.9984\n",
            "Epoch 00229: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0708 - accuracy: 0.9986 - val_loss: 0.0700 - val_accuracy: 1.0000\n",
            "Epoch 230/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0648 - accuracy: 0.9983\n",
            "Epoch 00230: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0691 - accuracy: 0.9973 - val_loss: 0.0683 - val_accuracy: 0.9959\n",
            "Epoch 231/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0728 - accuracy: 0.9934\n",
            "Epoch 00231: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0761 - accuracy: 0.9918 - val_loss: 0.0806 - val_accuracy: 0.9959\n",
            "Epoch 232/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0837 - accuracy: 0.9951\n",
            "Epoch 00232: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0818 - accuracy: 0.9946 - val_loss: 0.0692 - val_accuracy: 1.0000\n",
            "Epoch 233/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0640 - accuracy: 1.0000\n",
            "Epoch 00233: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0648 - accuracy: 1.0000 - val_loss: 0.0646 - val_accuracy: 0.9959\n",
            "Epoch 234/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0644 - accuracy: 0.9983\n",
            "Epoch 00234: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0645 - accuracy: 0.9986 - val_loss: 0.0712 - val_accuracy: 0.9959\n",
            "Epoch 235/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0624 - accuracy: 1.0000\n",
            "Epoch 00235: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0669 - accuracy: 0.9986 - val_loss: 0.0803 - val_accuracy: 0.9919\n",
            "Epoch 236/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0648 - accuracy: 0.9984\n",
            "Epoch 00236: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0641 - accuracy: 0.9986 - val_loss: 0.0652 - val_accuracy: 1.0000\n",
            "Epoch 237/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0603 - accuracy: 0.9984\n",
            "Epoch 00237: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0598 - accuracy: 0.9986 - val_loss: 0.0690 - val_accuracy: 0.9959\n",
            "Epoch 238/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0631 - accuracy: 0.9984\n",
            "Epoch 00238: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0624 - accuracy: 0.9986 - val_loss: 0.1224 - val_accuracy: 0.9756\n",
            "Epoch 239/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0613 - accuracy: 0.9984\n",
            "Epoch 00239: loss did not improve from 0.05849\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0603 - accuracy: 0.9986 - val_loss: 0.0666 - val_accuracy: 1.0000\n",
            "Epoch 240/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0566 - accuracy: 1.0000\n",
            "Epoch 00240: loss improved from 0.05849 to 0.05642, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0564 - accuracy: 1.0000 - val_loss: 0.0620 - val_accuracy: 0.9959\n",
            "Epoch 241/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0555 - accuracy: 1.0000\n",
            "Epoch 00241: loss improved from 0.05642 to 0.05556, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0556 - accuracy: 1.0000 - val_loss: 0.0583 - val_accuracy: 0.9959\n",
            "Epoch 242/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0547 - accuracy: 1.0000\n",
            "Epoch 00242: loss improved from 0.05556 to 0.05445, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0544 - accuracy: 1.0000 - val_loss: 0.0550 - val_accuracy: 1.0000\n",
            "Epoch 243/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0510 - accuracy: 1.0000\n",
            "Epoch 00243: loss improved from 0.05445 to 0.05113, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0511 - accuracy: 1.0000 - val_loss: 0.0578 - val_accuracy: 0.9959\n",
            "Epoch 244/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0574 - accuracy: 0.9951\n",
            "Epoch 00244: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0570 - accuracy: 0.9959 - val_loss: 0.0650 - val_accuracy: 0.9959\n",
            "Epoch 245/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0543 - accuracy: 1.0000\n",
            "Epoch 00245: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0542 - accuracy: 1.0000 - val_loss: 0.0836 - val_accuracy: 0.9837\n",
            "Epoch 246/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0512 - accuracy: 1.0000\n",
            "Epoch 00246: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0515 - accuracy: 1.0000 - val_loss: 0.0583 - val_accuracy: 0.9919\n",
            "Epoch 247/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0778 - accuracy: 0.9918\n",
            "Epoch 00247: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0774 - accuracy: 0.9918 - val_loss: 0.1065 - val_accuracy: 0.9797\n",
            "Epoch 248/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0758 - accuracy: 0.9951\n",
            "Epoch 00248: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0725 - accuracy: 0.9959 - val_loss: 0.0647 - val_accuracy: 0.9959\n",
            "Epoch 249/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0718 - accuracy: 0.9918\n",
            "Epoch 00249: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0694 - accuracy: 0.9932 - val_loss: 0.0630 - val_accuracy: 0.9959\n",
            "Epoch 250/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0713 - accuracy: 0.9934\n",
            "Epoch 00250: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0693 - accuracy: 0.9946 - val_loss: 0.0725 - val_accuracy: 0.9959\n",
            "Epoch 251/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0665 - accuracy: 0.9967\n",
            "Epoch 00251: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0649 - accuracy: 0.9973 - val_loss: 0.0564 - val_accuracy: 1.0000\n",
            "Epoch 252/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0616 - accuracy: 0.9967\n",
            "Epoch 00252: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0603 - accuracy: 0.9973 - val_loss: 0.0651 - val_accuracy: 0.9959\n",
            "Epoch 253/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0822 - accuracy: 0.9931\n",
            "Epoch 00253: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0890 - accuracy: 0.9918 - val_loss: 0.0692 - val_accuracy: 0.9959\n",
            "Epoch 254/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0799 - accuracy: 0.9951\n",
            "Epoch 00254: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0772 - accuracy: 0.9959 - val_loss: 0.1035 - val_accuracy: 0.9797\n",
            "Epoch 255/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0842 - accuracy: 0.9885\n",
            "Epoch 00255: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0844 - accuracy: 0.9878 - val_loss: 0.1079 - val_accuracy: 0.9837\n",
            "Epoch 256/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1447 - accuracy: 0.9753\n",
            "Epoch 00256: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1577 - accuracy: 0.9714 - val_loss: 0.1380 - val_accuracy: 0.9593\n",
            "Epoch 257/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1151 - accuracy: 0.9803\n",
            "Epoch 00257: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1119 - accuracy: 0.9823 - val_loss: 0.1009 - val_accuracy: 0.9878\n",
            "Epoch 258/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0987 - accuracy: 0.9878\n",
            "Epoch 00258: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0942 - accuracy: 0.9891 - val_loss: 0.0986 - val_accuracy: 0.9919\n",
            "Epoch 259/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0746 - accuracy: 0.9967\n",
            "Epoch 00259: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0741 - accuracy: 0.9973 - val_loss: 0.0885 - val_accuracy: 0.9837\n",
            "Epoch 260/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0763 - accuracy: 0.9951\n",
            "Epoch 00260: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0760 - accuracy: 0.9946 - val_loss: 0.0769 - val_accuracy: 0.9959\n",
            "Epoch 261/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0687 - accuracy: 0.9984\n",
            "Epoch 00261: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0699 - accuracy: 0.9973 - val_loss: 0.0677 - val_accuracy: 1.0000\n",
            "Epoch 262/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0749 - accuracy: 0.9951\n",
            "Epoch 00262: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0730 - accuracy: 0.9959 - val_loss: 0.0759 - val_accuracy: 0.9919\n",
            "Epoch 263/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0668 - accuracy: 0.9984\n",
            "Epoch 00263: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0660 - accuracy: 0.9986 - val_loss: 0.0970 - val_accuracy: 0.9919\n",
            "Epoch 264/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0693 - accuracy: 0.9951\n",
            "Epoch 00264: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0716 - accuracy: 0.9946 - val_loss: 0.0900 - val_accuracy: 0.9919\n",
            "Epoch 265/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1017 - accuracy: 0.9913\n",
            "Epoch 00265: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1017 - accuracy: 0.9891 - val_loss: 0.0876 - val_accuracy: 0.9959\n",
            "Epoch 266/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0874 - accuracy: 0.9934\n",
            "Epoch 00266: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0841 - accuracy: 0.9946 - val_loss: 0.1005 - val_accuracy: 0.9878\n",
            "Epoch 267/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0806 - accuracy: 0.9951\n",
            "Epoch 00267: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0779 - accuracy: 0.9959 - val_loss: 0.0763 - val_accuracy: 0.9959\n",
            "Epoch 268/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9973\n",
            "Epoch 00268: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0732 - accuracy: 0.9973 - val_loss: 0.0685 - val_accuracy: 1.0000\n",
            "Epoch 269/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0805 - accuracy: 0.9984\n",
            "Epoch 00269: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0786 - accuracy: 0.9986 - val_loss: 0.0659 - val_accuracy: 1.0000\n",
            "Epoch 270/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0933 - accuracy: 0.9868\n",
            "Epoch 00270: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0923 - accuracy: 0.9878 - val_loss: 0.0784 - val_accuracy: 1.0000\n",
            "Epoch 271/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0741 - accuracy: 1.0000\n",
            "Epoch 00271: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0724 - accuracy: 1.0000 - val_loss: 0.0683 - val_accuracy: 1.0000\n",
            "Epoch 272/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0640 - accuracy: 1.0000\n",
            "Epoch 00272: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0642 - accuracy: 1.0000 - val_loss: 0.0642 - val_accuracy: 1.0000\n",
            "Epoch 273/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0629 - accuracy: 1.0000\n",
            "Epoch 00273: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0625 - accuracy: 1.0000 - val_loss: 0.0632 - val_accuracy: 1.0000\n",
            "Epoch 274/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0640 - accuracy: 1.0000\n",
            "Epoch 00274: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0634 - accuracy: 1.0000 - val_loss: 0.0627 - val_accuracy: 1.0000\n",
            "Epoch 275/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0604 - accuracy: 1.0000\n",
            "Epoch 00275: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0600 - accuracy: 1.0000 - val_loss: 0.0618 - val_accuracy: 1.0000\n",
            "Epoch 276/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0677 - accuracy: 0.9965\n",
            "Epoch 00276: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0752 - accuracy: 0.9946 - val_loss: 0.1212 - val_accuracy: 0.9634\n",
            "Epoch 277/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0715 - accuracy: 0.9967\n",
            "Epoch 00277: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0704 - accuracy: 0.9959 - val_loss: 0.0741 - val_accuracy: 0.9959\n",
            "Epoch 278/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0678 - accuracy: 0.9984\n",
            "Epoch 00278: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0668 - accuracy: 0.9986 - val_loss: 0.0668 - val_accuracy: 1.0000\n",
            "Epoch 279/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0694 - accuracy: 0.9984\n",
            "Epoch 00279: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0684 - accuracy: 0.9986 - val_loss: 0.0612 - val_accuracy: 1.0000\n",
            "Epoch 280/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0691 - accuracy: 0.9951\n",
            "Epoch 00280: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0748 - accuracy: 0.9946 - val_loss: 0.0818 - val_accuracy: 0.9919\n",
            "Epoch 281/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0640 - accuracy: 0.9984\n",
            "Epoch 00281: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0637 - accuracy: 0.9986 - val_loss: 0.0875 - val_accuracy: 0.9878\n",
            "Epoch 282/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0650 - accuracy: 0.9984\n",
            "Epoch 00282: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0638 - accuracy: 0.9986 - val_loss: 0.0624 - val_accuracy: 0.9959\n",
            "Epoch 283/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0581 - accuracy: 1.0000\n",
            "Epoch 00283: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0587 - accuracy: 1.0000 - val_loss: 0.0681 - val_accuracy: 0.9959\n",
            "Epoch 284/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0600 - accuracy: 0.9984\n",
            "Epoch 00284: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0596 - accuracy: 0.9986 - val_loss: 0.1566 - val_accuracy: 0.9634\n",
            "Epoch 285/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0672 - accuracy: 0.9967\n",
            "Epoch 00285: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0661 - accuracy: 0.9973 - val_loss: 0.0637 - val_accuracy: 1.0000\n",
            "Epoch 286/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0605 - accuracy: 0.9967\n",
            "Epoch 00286: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0618 - accuracy: 0.9959 - val_loss: 0.0679 - val_accuracy: 0.9959\n",
            "Epoch 287/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0605 - accuracy: 0.9984\n",
            "Epoch 00287: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0594 - accuracy: 0.9986 - val_loss: 0.0571 - val_accuracy: 1.0000\n",
            "Epoch 288/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0575 - accuracy: 1.0000\n",
            "Epoch 00288: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0571 - accuracy: 1.0000 - val_loss: 0.0621 - val_accuracy: 0.9959\n",
            "Epoch 289/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0542 - accuracy: 1.0000\n",
            "Epoch 00289: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0550 - accuracy: 1.0000 - val_loss: 0.0625 - val_accuracy: 0.9959\n",
            "Epoch 290/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0528 - accuracy: 1.0000\n",
            "Epoch 00290: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0526 - accuracy: 1.0000 - val_loss: 0.0564 - val_accuracy: 1.0000\n",
            "Epoch 291/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0531 - accuracy: 1.0000\n",
            "Epoch 00291: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0530 - accuracy: 1.0000 - val_loss: 0.0531 - val_accuracy: 1.0000\n",
            "Epoch 292/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0518 - accuracy: 0.9984\n",
            "Epoch 00292: loss did not improve from 0.05113\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0528 - accuracy: 0.9986 - val_loss: 0.0553 - val_accuracy: 1.0000\n",
            "Epoch 293/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0500 - accuracy: 1.0000\n",
            "Epoch 00293: loss improved from 0.05113 to 0.04995, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0499 - accuracy: 1.0000 - val_loss: 0.0531 - val_accuracy: 0.9959\n",
            "Epoch 294/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0497 - accuracy: 1.0000\n",
            "Epoch 00294: loss improved from 0.04995 to 0.04933, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0493 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 1.0000\n",
            "Epoch 295/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0490 - accuracy: 0.9984\n",
            "Epoch 00295: loss improved from 0.04933 to 0.04863, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0486 - accuracy: 0.9986 - val_loss: 0.0500 - val_accuracy: 1.0000\n",
            "Epoch 296/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0581 - accuracy: 0.9967\n",
            "Epoch 00296: loss did not improve from 0.04863\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0565 - accuracy: 0.9973 - val_loss: 0.0543 - val_accuracy: 0.9959\n",
            "Epoch 297/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0493 - accuracy: 1.0000\n",
            "Epoch 00297: loss did not improve from 0.04863\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0488 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 1.0000\n",
            "Epoch 298/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0479 - accuracy: 1.0000\n",
            "Epoch 00298: loss improved from 0.04863 to 0.04776, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0478 - accuracy: 1.0000 - val_loss: 0.0500 - val_accuracy: 1.0000\n",
            "Epoch 299/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0483 - accuracy: 0.9983\n",
            "Epoch 00299: loss improved from 0.04776 to 0.04760, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0476 - accuracy: 0.9986 - val_loss: 0.0545 - val_accuracy: 0.9959\n",
            "Epoch 300/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0551 - accuracy: 0.9984\n",
            "Epoch 00300: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0569 - accuracy: 0.9973 - val_loss: 0.1179 - val_accuracy: 0.9593\n",
            "Epoch 301/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0728 - accuracy: 0.9901\n",
            "Epoch 00301: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0763 - accuracy: 0.9878 - val_loss: 0.0645 - val_accuracy: 0.9878\n",
            "Epoch 302/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0789 - accuracy: 0.9918\n",
            "Epoch 00302: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0748 - accuracy: 0.9932 - val_loss: 0.0771 - val_accuracy: 0.9878\n",
            "Epoch 303/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0577 - accuracy: 0.9984\n",
            "Epoch 00303: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0589 - accuracy: 0.9973 - val_loss: 0.0591 - val_accuracy: 1.0000\n",
            "Epoch 304/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9946\n",
            "Epoch 00304: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0595 - accuracy: 0.9946 - val_loss: 0.0827 - val_accuracy: 0.9878\n",
            "Epoch 305/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0545 - accuracy: 0.9984\n",
            "Epoch 00305: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0547 - accuracy: 0.9986 - val_loss: 0.0525 - val_accuracy: 1.0000\n",
            "Epoch 306/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0529 - accuracy: 0.9984\n",
            "Epoch 00306: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0522 - accuracy: 0.9986 - val_loss: 0.0492 - val_accuracy: 1.0000\n",
            "Epoch 307/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0549 - accuracy: 0.9967\n",
            "Epoch 00307: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0540 - accuracy: 0.9973 - val_loss: 0.0590 - val_accuracy: 1.0000\n",
            "Epoch 308/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0675 - accuracy: 0.9918\n",
            "Epoch 00308: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0651 - accuracy: 0.9932 - val_loss: 0.0541 - val_accuracy: 1.0000\n",
            "Epoch 309/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0653 - accuracy: 0.9948\n",
            "Epoch 00309: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0639 - accuracy: 0.9959 - val_loss: 0.1143 - val_accuracy: 0.9675\n",
            "Epoch 310/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0737 - accuracy: 0.9885\n",
            "Epoch 00310: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0723 - accuracy: 0.9905 - val_loss: 0.0789 - val_accuracy: 0.9919\n",
            "Epoch 311/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0585 - accuracy: 0.9984\n",
            "Epoch 00311: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0583 - accuracy: 0.9973 - val_loss: 0.0730 - val_accuracy: 0.9837\n",
            "Epoch 312/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0720 - accuracy: 0.9885\n",
            "Epoch 00312: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0724 - accuracy: 0.9891 - val_loss: 0.0637 - val_accuracy: 0.9959\n",
            "Epoch 313/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0692 - accuracy: 0.9934\n",
            "Epoch 00313: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0672 - accuracy: 0.9946 - val_loss: 0.0760 - val_accuracy: 0.9919\n",
            "Epoch 314/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0592 - accuracy: 0.9984\n",
            "Epoch 00314: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0581 - accuracy: 0.9986 - val_loss: 0.0578 - val_accuracy: 1.0000\n",
            "Epoch 315/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0564 - accuracy: 1.0000\n",
            "Epoch 00315: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0559 - accuracy: 1.0000 - val_loss: 0.0599 - val_accuracy: 0.9959\n",
            "Epoch 316/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0611 - accuracy: 0.9984\n",
            "Epoch 00316: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0608 - accuracy: 0.9986 - val_loss: 0.0655 - val_accuracy: 1.0000\n",
            "Epoch 317/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0557 - accuracy: 0.9984\n",
            "Epoch 00317: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0559 - accuracy: 0.9986 - val_loss: 0.0538 - val_accuracy: 1.0000\n",
            "Epoch 318/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0533 - accuracy: 1.0000\n",
            "Epoch 00318: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0525 - accuracy: 1.0000 - val_loss: 0.0543 - val_accuracy: 1.0000\n",
            "Epoch 319/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0497 - accuracy: 1.0000\n",
            "Epoch 00319: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0495 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 1.0000\n",
            "Epoch 320/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0488 - accuracy: 1.0000\n",
            "Epoch 00320: loss did not improve from 0.04760\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0484 - accuracy: 1.0000 - val_loss: 0.0503 - val_accuracy: 1.0000\n",
            "Epoch 321/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0478 - accuracy: 1.0000\n",
            "Epoch 00321: loss improved from 0.04760 to 0.04752, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0475 - accuracy: 1.0000 - val_loss: 0.0484 - val_accuracy: 1.0000\n",
            "Epoch 322/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0473 - accuracy: 1.0000\n",
            "Epoch 00322: loss improved from 0.04752 to 0.04692, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0469 - accuracy: 1.0000 - val_loss: 0.0482 - val_accuracy: 1.0000\n",
            "Epoch 323/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0450 - accuracy: 1.0000\n",
            "Epoch 00323: loss improved from 0.04692 to 0.04532, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0453 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 1.0000\n",
            "Epoch 324/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0452 - accuracy: 1.0000\n",
            "Epoch 00324: loss improved from 0.04532 to 0.04510, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0451 - accuracy: 1.0000 - val_loss: 0.0550 - val_accuracy: 0.9959\n",
            "Epoch 325/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0476 - accuracy: 0.9984\n",
            "Epoch 00325: loss did not improve from 0.04510\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0533 - accuracy: 0.9973 - val_loss: 0.0449 - val_accuracy: 1.0000\n",
            "Epoch 326/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0496 - accuracy: 0.9984\n",
            "Epoch 00326: loss did not improve from 0.04510\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0488 - accuracy: 0.9986 - val_loss: 0.0539 - val_accuracy: 0.9959\n",
            "Epoch 327/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0484 - accuracy: 0.9984\n",
            "Epoch 00327: loss did not improve from 0.04510\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0479 - accuracy: 0.9986 - val_loss: 0.0465 - val_accuracy: 1.0000\n",
            "Epoch 328/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0442 - accuracy: 1.0000\n",
            "Epoch 00328: loss improved from 0.04510 to 0.04453, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0445 - accuracy: 1.0000 - val_loss: 0.0437 - val_accuracy: 1.0000\n",
            "Epoch 329/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0603 - accuracy: 0.9951\n",
            "Epoch 00329: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0622 - accuracy: 0.9946 - val_loss: 0.0556 - val_accuracy: 1.0000\n",
            "Epoch 330/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0575 - accuracy: 0.9967\n",
            "Epoch 00330: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0598 - accuracy: 0.9959 - val_loss: 0.0580 - val_accuracy: 0.9959\n",
            "Epoch 331/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0604 - accuracy: 0.9934\n",
            "Epoch 00331: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0617 - accuracy: 0.9932 - val_loss: 0.0744 - val_accuracy: 0.9878\n",
            "Epoch 332/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0662 - accuracy: 0.9951\n",
            "Epoch 00332: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0640 - accuracy: 0.9959 - val_loss: 0.0571 - val_accuracy: 1.0000\n",
            "Epoch 333/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0687 - accuracy: 0.9901\n",
            "Epoch 00333: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0680 - accuracy: 0.9905 - val_loss: 0.0593 - val_accuracy: 0.9959\n",
            "Epoch 334/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0552 - accuracy: 1.0000\n",
            "Epoch 00334: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0542 - accuracy: 1.0000 - val_loss: 0.0628 - val_accuracy: 0.9959\n",
            "Epoch 335/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0538 - accuracy: 0.9983\n",
            "Epoch 00335: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0526 - accuracy: 0.9986 - val_loss: 0.0560 - val_accuracy: 0.9959\n",
            "Epoch 336/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0609 - accuracy: 0.9951\n",
            "Epoch 00336: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0696 - accuracy: 0.9918 - val_loss: 0.0629 - val_accuracy: 0.9959\n",
            "Epoch 337/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0732 - accuracy: 0.9934\n",
            "Epoch 00337: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0719 - accuracy: 0.9932 - val_loss: 0.0722 - val_accuracy: 0.9878\n",
            "Epoch 338/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0632 - accuracy: 0.9967\n",
            "Epoch 00338: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0627 - accuracy: 0.9973 - val_loss: 0.0645 - val_accuracy: 0.9959\n",
            "Epoch 339/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0552 - accuracy: 0.9983\n",
            "Epoch 00339: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0555 - accuracy: 0.9986 - val_loss: 0.0735 - val_accuracy: 0.9837\n",
            "Epoch 340/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0558 - accuracy: 0.9984\n",
            "Epoch 00340: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0547 - accuracy: 0.9986 - val_loss: 0.0719 - val_accuracy: 0.9919\n",
            "Epoch 341/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0505 - accuracy: 1.0000\n",
            "Epoch 00341: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0501 - accuracy: 1.0000 - val_loss: 0.0569 - val_accuracy: 0.9959\n",
            "Epoch 342/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0483 - accuracy: 1.0000\n",
            "Epoch 00342: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0486 - accuracy: 1.0000 - val_loss: 0.0504 - val_accuracy: 1.0000\n",
            "Epoch 343/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0478 - accuracy: 1.0000\n",
            "Epoch 00343: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0498 - accuracy: 0.9986 - val_loss: 0.0485 - val_accuracy: 1.0000\n",
            "Epoch 344/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0464 - accuracy: 1.0000\n",
            "Epoch 00344: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0472 - accuracy: 1.0000 - val_loss: 0.0483 - val_accuracy: 1.0000\n",
            "Epoch 345/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0567 - accuracy: 0.9967\n",
            "Epoch 00345: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0550 - accuracy: 0.9973 - val_loss: 0.1047 - val_accuracy: 0.9837\n",
            "Epoch 346/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0704 - accuracy: 0.9934\n",
            "Epoch 00346: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0810 - accuracy: 0.9891 - val_loss: 0.1139 - val_accuracy: 0.9837\n",
            "Epoch 347/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0862 - accuracy: 0.9844\n",
            "Epoch 00347: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0863 - accuracy: 0.9850 - val_loss: 0.0804 - val_accuracy: 0.9919\n",
            "Epoch 348/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0721 - accuracy: 0.9913\n",
            "Epoch 00348: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0698 - accuracy: 0.9932 - val_loss: 0.0628 - val_accuracy: 0.9959\n",
            "Epoch 349/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0559 - accuracy: 0.9983\n",
            "Epoch 00349: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0572 - accuracy: 0.9973 - val_loss: 0.0743 - val_accuracy: 0.9959\n",
            "Epoch 350/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0607 - accuracy: 0.9967\n",
            "Epoch 00350: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0594 - accuracy: 0.9973 - val_loss: 0.0629 - val_accuracy: 0.9959\n",
            "Epoch 351/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0621 - accuracy: 0.9951\n",
            "Epoch 00351: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0601 - accuracy: 0.9959 - val_loss: 0.0701 - val_accuracy: 0.9878\n",
            "Epoch 352/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0598 - accuracy: 0.9951\n",
            "Epoch 00352: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0589 - accuracy: 0.9959 - val_loss: 0.0742 - val_accuracy: 0.9919\n",
            "Epoch 353/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0557 - accuracy: 0.9984\n",
            "Epoch 00353: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0552 - accuracy: 0.9986 - val_loss: 0.0591 - val_accuracy: 0.9959\n",
            "Epoch 354/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0507 - accuracy: 1.0000\n",
            "Epoch 00354: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0506 - accuracy: 1.0000 - val_loss: 0.0605 - val_accuracy: 0.9919\n",
            "Epoch 355/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0513 - accuracy: 1.0000\n",
            "Epoch 00355: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0507 - accuracy: 1.0000 - val_loss: 0.0551 - val_accuracy: 1.0000\n",
            "Epoch 356/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0490 - accuracy: 0.9984\n",
            "Epoch 00356: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0486 - accuracy: 0.9986 - val_loss: 0.0543 - val_accuracy: 1.0000\n",
            "Epoch 357/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0467 - accuracy: 1.0000\n",
            "Epoch 00357: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0467 - accuracy: 1.0000 - val_loss: 0.0507 - val_accuracy: 1.0000\n",
            "Epoch 358/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0473 - accuracy: 1.0000\n",
            "Epoch 00358: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0478 - accuracy: 1.0000 - val_loss: 0.0498 - val_accuracy: 0.9959\n",
            "Epoch 359/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0535 - accuracy: 0.9984\n",
            "Epoch 00359: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0564 - accuracy: 0.9959 - val_loss: 0.0685 - val_accuracy: 0.9919\n",
            "Epoch 360/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0637 - accuracy: 0.9918\n",
            "Epoch 00360: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0638 - accuracy: 0.9918 - val_loss: 0.0586 - val_accuracy: 0.9959\n",
            "Epoch 361/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0545 - accuracy: 0.9965\n",
            "Epoch 00361: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0529 - accuracy: 0.9973 - val_loss: 0.0552 - val_accuracy: 0.9959\n",
            "Epoch 362/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0604 - accuracy: 0.9951\n",
            "Epoch 00362: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0626 - accuracy: 0.9946 - val_loss: 0.0541 - val_accuracy: 1.0000\n",
            "Epoch 363/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0515 - accuracy: 1.0000\n",
            "Epoch 00363: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0512 - accuracy: 1.0000 - val_loss: 0.0592 - val_accuracy: 0.9919\n",
            "Epoch 364/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0640 - accuracy: 0.9913\n",
            "Epoch 00364: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0725 - accuracy: 0.9878 - val_loss: 0.0709 - val_accuracy: 0.9919\n",
            "Epoch 365/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0941 - accuracy: 0.9844\n",
            "Epoch 00365: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1010 - accuracy: 0.9796 - val_loss: 0.0705 - val_accuracy: 0.9959\n",
            "Epoch 366/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0682 - accuracy: 0.9951\n",
            "Epoch 00366: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0680 - accuracy: 0.9946 - val_loss: 0.0691 - val_accuracy: 0.9919\n",
            "Epoch 367/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0713 - accuracy: 0.9934\n",
            "Epoch 00367: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0685 - accuracy: 0.9946 - val_loss: 0.0870 - val_accuracy: 0.9837\n",
            "Epoch 368/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0555 - accuracy: 1.0000\n",
            "Epoch 00368: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0547 - accuracy: 1.0000 - val_loss: 0.0572 - val_accuracy: 0.9959\n",
            "Epoch 369/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0552 - accuracy: 0.9984\n",
            "Epoch 00369: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0552 - accuracy: 0.9986 - val_loss: 0.0540 - val_accuracy: 1.0000\n",
            "Epoch 370/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0506 - accuracy: 1.0000\n",
            "Epoch 00370: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0507 - accuracy: 1.0000 - val_loss: 0.0548 - val_accuracy: 0.9959\n",
            "Epoch 371/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0508 - accuracy: 0.9984\n",
            "Epoch 00371: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0504 - accuracy: 0.9986 - val_loss: 0.0515 - val_accuracy: 1.0000\n",
            "Epoch 372/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0480 - accuracy: 1.0000\n",
            "Epoch 00372: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0479 - accuracy: 1.0000 - val_loss: 0.0498 - val_accuracy: 1.0000\n",
            "Epoch 373/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0488 - accuracy: 0.9984\n",
            "Epoch 00373: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0492 - accuracy: 0.9986 - val_loss: 0.0552 - val_accuracy: 0.9959\n",
            "Epoch 374/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0479 - accuracy: 1.0000\n",
            "Epoch 00374: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 0.0528 - val_accuracy: 1.0000\n",
            "Epoch 375/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0497 - accuracy: 0.9983\n",
            "Epoch 00375: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0515 - accuracy: 0.9973 - val_loss: 0.0479 - val_accuracy: 1.0000\n",
            "Epoch 376/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0549 - accuracy: 0.9951\n",
            "Epoch 00376: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0580 - accuracy: 0.9932 - val_loss: 0.0527 - val_accuracy: 0.9959\n",
            "Epoch 377/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0524 - accuracy: 0.9984\n",
            "Epoch 00377: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0518 - accuracy: 0.9986 - val_loss: 0.0526 - val_accuracy: 1.0000\n",
            "Epoch 378/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0480 - accuracy: 0.9984\n",
            "Epoch 00378: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0483 - accuracy: 0.9986 - val_loss: 0.0486 - val_accuracy: 1.0000\n",
            "Epoch 379/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0483 - accuracy: 0.9984\n",
            "Epoch 00379: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0512 - accuracy: 0.9959 - val_loss: 0.0593 - val_accuracy: 0.9959\n",
            "Epoch 380/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0486 - accuracy: 1.0000\n",
            "Epoch 00380: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0483 - accuracy: 1.0000 - val_loss: 0.0474 - val_accuracy: 1.0000\n",
            "Epoch 381/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0509 - accuracy: 0.9983\n",
            "Epoch 00381: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0500 - accuracy: 0.9986 - val_loss: 0.0667 - val_accuracy: 0.9919\n",
            "Epoch 382/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0503 - accuracy: 0.9984\n",
            "Epoch 00382: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0499 - accuracy: 0.9986 - val_loss: 0.0463 - val_accuracy: 1.0000\n",
            "Epoch 383/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0548 - accuracy: 0.9965\n",
            "Epoch 00383: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0542 - accuracy: 0.9973 - val_loss: 0.0499 - val_accuracy: 1.0000\n",
            "Epoch 384/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0463 - accuracy: 0.9984\n",
            "Epoch 00384: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0466 - accuracy: 0.9986 - val_loss: 0.0513 - val_accuracy: 0.9959\n",
            "Epoch 385/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0489 - accuracy: 0.9984\n",
            "Epoch 00385: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0485 - accuracy: 0.9986 - val_loss: 0.0556 - val_accuracy: 1.0000\n",
            "Epoch 386/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0471 - accuracy: 0.9984\n",
            "Epoch 00386: loss did not improve from 0.04453\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0468 - accuracy: 0.9986 - val_loss: 0.0469 - val_accuracy: 1.0000\n",
            "Epoch 387/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0442 - accuracy: 1.0000\n",
            "Epoch 00387: loss improved from 0.04453 to 0.04397, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0440 - accuracy: 1.0000 - val_loss: 0.0490 - val_accuracy: 1.0000\n",
            "Epoch 388/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0438 - accuracy: 1.0000\n",
            "Epoch 00388: loss improved from 0.04397 to 0.04348, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0435 - accuracy: 1.0000 - val_loss: 0.0446 - val_accuracy: 1.0000\n",
            "Epoch 389/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0527 - accuracy: 0.9967\n",
            "Epoch 00389: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0514 - accuracy: 0.9973 - val_loss: 0.0456 - val_accuracy: 1.0000\n",
            "Epoch 390/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0631 - accuracy: 0.9885\n",
            "Epoch 00390: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0618 - accuracy: 0.9905 - val_loss: 0.0608 - val_accuracy: 0.9959\n",
            "Epoch 391/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0723 - accuracy: 0.9918\n",
            "Epoch 00391: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0693 - accuracy: 0.9932 - val_loss: 0.0519 - val_accuracy: 1.0000\n",
            "Epoch 392/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0469 - accuracy: 1.0000\n",
            "Epoch 00392: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0478 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 0.9959\n",
            "Epoch 393/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0453 - accuracy: 1.0000\n",
            "Epoch 00393: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0450 - accuracy: 1.0000 - val_loss: 0.0474 - val_accuracy: 1.0000\n",
            "Epoch 394/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0672 - accuracy: 0.9918\n",
            "Epoch 00394: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0660 - accuracy: 0.9918 - val_loss: 0.1029 - val_accuracy: 0.9756\n",
            "Epoch 395/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0597 - accuracy: 0.9951\n",
            "Epoch 00395: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0582 - accuracy: 0.9959 - val_loss: 0.0684 - val_accuracy: 0.9919\n",
            "Epoch 396/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0574 - accuracy: 0.9965\n",
            "Epoch 00396: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0555 - accuracy: 0.9973 - val_loss: 0.0481 - val_accuracy: 1.0000\n",
            "Epoch 397/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0549 - accuracy: 0.9951\n",
            "Epoch 00397: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0533 - accuracy: 0.9959 - val_loss: 0.0513 - val_accuracy: 1.0000\n",
            "Epoch 398/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0557 - accuracy: 0.9984\n",
            "Epoch 00398: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0545 - accuracy: 0.9986 - val_loss: 0.0493 - val_accuracy: 0.9959\n",
            "Epoch 399/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0661 - accuracy: 0.9934\n",
            "Epoch 00399: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0692 - accuracy: 0.9932 - val_loss: 0.1565 - val_accuracy: 0.9634\n",
            "Epoch 400/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1002 - accuracy: 0.9878\n",
            "Epoch 00400: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0997 - accuracy: 0.9864 - val_loss: 0.1260 - val_accuracy: 0.9756\n",
            "Epoch 401/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0607 - accuracy: 0.9984\n",
            "Epoch 00401: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0595 - accuracy: 0.9986 - val_loss: 0.0605 - val_accuracy: 0.9959\n",
            "Epoch 402/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0884 - accuracy: 0.9819\n",
            "Epoch 00402: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0845 - accuracy: 0.9837 - val_loss: 0.0563 - val_accuracy: 1.0000\n",
            "Epoch 403/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0856 - accuracy: 0.9885\n",
            "Epoch 00403: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0823 - accuracy: 0.9905 - val_loss: 0.0650 - val_accuracy: 1.0000\n",
            "Epoch 404/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0621 - accuracy: 0.9983\n",
            "Epoch 00404: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0606 - accuracy: 0.9986 - val_loss: 0.0561 - val_accuracy: 1.0000\n",
            "Epoch 405/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0543 - accuracy: 1.0000\n",
            "Epoch 00405: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0537 - accuracy: 1.0000 - val_loss: 0.0556 - val_accuracy: 0.9959\n",
            "Epoch 406/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0515 - accuracy: 1.0000\n",
            "Epoch 00406: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0513 - accuracy: 1.0000 - val_loss: 0.0530 - val_accuracy: 1.0000\n",
            "Epoch 407/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0502 - accuracy: 1.0000\n",
            "Epoch 00407: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0500 - accuracy: 1.0000 - val_loss: 0.0543 - val_accuracy: 0.9959\n",
            "Epoch 408/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0497 - accuracy: 1.0000\n",
            "Epoch 00408: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0495 - accuracy: 1.0000 - val_loss: 0.0520 - val_accuracy: 1.0000\n",
            "Epoch 409/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0487 - accuracy: 1.0000\n",
            "Epoch 00409: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0484 - accuracy: 1.0000 - val_loss: 0.0543 - val_accuracy: 0.9959\n",
            "Epoch 410/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0473 - accuracy: 1.0000\n",
            "Epoch 00410: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0474 - accuracy: 1.0000 - val_loss: 0.0561 - val_accuracy: 0.9959\n",
            "Epoch 411/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0463 - accuracy: 1.0000\n",
            "Epoch 00411: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0462 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9959\n",
            "Epoch 412/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0453 - accuracy: 1.0000\n",
            "Epoch 00412: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0453 - accuracy: 1.0000 - val_loss: 0.0513 - val_accuracy: 0.9959\n",
            "Epoch 413/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0447 - accuracy: 1.0000\n",
            "Epoch 00413: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0447 - accuracy: 1.0000 - val_loss: 0.0497 - val_accuracy: 0.9959\n",
            "Epoch 414/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0451 - accuracy: 1.0000\n",
            "Epoch 00414: loss did not improve from 0.04348\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 0.0502 - val_accuracy: 0.9959\n",
            "Epoch 415/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0434 - accuracy: 1.0000\n",
            "Epoch 00415: loss improved from 0.04348 to 0.04321, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0432 - accuracy: 1.0000 - val_loss: 0.0446 - val_accuracy: 1.0000\n",
            "Epoch 416/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0431 - accuracy: 1.0000\n",
            "Epoch 00416: loss improved from 0.04321 to 0.04290, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 0.0427 - val_accuracy: 1.0000\n",
            "Epoch 417/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0418 - accuracy: 1.0000\n",
            "Epoch 00417: loss improved from 0.04290 to 0.04168, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0417 - accuracy: 1.0000 - val_loss: 0.0421 - val_accuracy: 1.0000\n",
            "Epoch 418/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0420 - accuracy: 1.0000\n",
            "Epoch 00418: loss did not improve from 0.04168\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0425 - accuracy: 1.0000 - val_loss: 0.0423 - val_accuracy: 1.0000\n",
            "Epoch 419/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0405 - accuracy: 1.0000\n",
            "Epoch 00419: loss improved from 0.04168 to 0.04039, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0404 - accuracy: 1.0000 - val_loss: 0.0480 - val_accuracy: 0.9959\n",
            "Epoch 420/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0410 - accuracy: 1.0000\n",
            "Epoch 00420: loss did not improve from 0.04039\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0407 - accuracy: 1.0000 - val_loss: 0.0461 - val_accuracy: 0.9959\n",
            "Epoch 421/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0429 - accuracy: 0.9984\n",
            "Epoch 00421: loss did not improve from 0.04039\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0427 - accuracy: 0.9986 - val_loss: 0.0416 - val_accuracy: 1.0000\n",
            "Epoch 422/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0409 - accuracy: 1.0000\n",
            "Epoch 00422: loss did not improve from 0.04039\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0412 - accuracy: 1.0000 - val_loss: 0.0498 - val_accuracy: 0.9959\n",
            "Epoch 423/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0400 - accuracy: 1.0000\n",
            "Epoch 00423: loss improved from 0.04039 to 0.03969, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0397 - accuracy: 1.0000 - val_loss: 0.0398 - val_accuracy: 1.0000\n",
            "Epoch 424/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0435 - accuracy: 0.9967\n",
            "Epoch 00424: loss did not improve from 0.03969\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0427 - accuracy: 0.9973 - val_loss: 0.0518 - val_accuracy: 0.9919\n",
            "Epoch 425/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0397 - accuracy: 1.0000\n",
            "Epoch 00425: loss did not improve from 0.03969\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0398 - accuracy: 1.0000 - val_loss: 0.0395 - val_accuracy: 1.0000\n",
            "Epoch 426/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0399 - accuracy: 0.9984\n",
            "Epoch 00426: loss improved from 0.03969 to 0.03937, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0394 - accuracy: 0.9986 - val_loss: 0.0398 - val_accuracy: 1.0000\n",
            "Epoch 427/1000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.0377 - accuracy: 1.0000\n",
            "Epoch 00427: loss improved from 0.03937 to 0.03781, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0378 - accuracy: 1.0000 - val_loss: 0.0400 - val_accuracy: 1.0000\n",
            "Epoch 428/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0384 - accuracy: 1.0000\n",
            "Epoch 00428: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0391 - accuracy: 1.0000 - val_loss: 0.0386 - val_accuracy: 1.0000\n",
            "Epoch 429/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0419 - accuracy: 0.9984\n",
            "Epoch 00429: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0412 - accuracy: 0.9986 - val_loss: 0.0622 - val_accuracy: 0.9878\n",
            "Epoch 430/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0551 - accuracy: 0.9967\n",
            "Epoch 00430: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0527 - accuracy: 0.9973 - val_loss: 0.0446 - val_accuracy: 1.0000\n",
            "Epoch 431/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0565 - accuracy: 0.9934\n",
            "Epoch 00431: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0554 - accuracy: 0.9946 - val_loss: 0.0494 - val_accuracy: 0.9959\n",
            "Epoch 432/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0504 - accuracy: 0.9951\n",
            "Epoch 00432: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0497 - accuracy: 0.9959 - val_loss: 0.0528 - val_accuracy: 1.0000\n",
            "Epoch 433/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0532 - accuracy: 0.9951\n",
            "Epoch 00433: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0512 - accuracy: 0.9959 - val_loss: 0.0461 - val_accuracy: 1.0000\n",
            "Epoch 434/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0613 - accuracy: 0.9934\n",
            "Epoch 00434: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0587 - accuracy: 0.9946 - val_loss: 0.0744 - val_accuracy: 0.9919\n",
            "Epoch 435/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0507 - accuracy: 0.9967\n",
            "Epoch 00435: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0550 - accuracy: 0.9959 - val_loss: 0.0536 - val_accuracy: 0.9959\n",
            "Epoch 436/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0624 - accuracy: 0.9918\n",
            "Epoch 00436: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0644 - accuracy: 0.9918 - val_loss: 0.1270 - val_accuracy: 0.9715\n",
            "Epoch 437/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0709 - accuracy: 0.9901\n",
            "Epoch 00437: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0671 - accuracy: 0.9918 - val_loss: 0.0778 - val_accuracy: 0.9919\n",
            "Epoch 438/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0743 - accuracy: 0.9918\n",
            "Epoch 00438: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0727 - accuracy: 0.9905 - val_loss: 0.0561 - val_accuracy: 0.9959\n",
            "Epoch 439/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0680 - accuracy: 0.9918\n",
            "Epoch 00439: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0667 - accuracy: 0.9918 - val_loss: 0.0488 - val_accuracy: 1.0000\n",
            "Epoch 440/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0569 - accuracy: 0.9934\n",
            "Epoch 00440: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0603 - accuracy: 0.9918 - val_loss: 0.0709 - val_accuracy: 0.9878\n",
            "Epoch 441/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0681 - accuracy: 0.9951\n",
            "Epoch 00441: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0670 - accuracy: 0.9946 - val_loss: 0.0680 - val_accuracy: 0.9919\n",
            "Epoch 442/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0607 - accuracy: 0.9951\n",
            "Epoch 00442: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0633 - accuracy: 0.9946 - val_loss: 0.0556 - val_accuracy: 1.0000\n",
            "Epoch 443/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0746 - accuracy: 0.9852\n",
            "Epoch 00443: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0747 - accuracy: 0.9864 - val_loss: 0.0721 - val_accuracy: 0.9878\n",
            "Epoch 444/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0576 - accuracy: 0.9967\n",
            "Epoch 00444: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0603 - accuracy: 0.9959 - val_loss: 0.0517 - val_accuracy: 1.0000\n",
            "Epoch 445/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0630 - accuracy: 0.9967\n",
            "Epoch 00445: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0672 - accuracy: 0.9932 - val_loss: 0.0641 - val_accuracy: 0.9959\n",
            "Epoch 446/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0620 - accuracy: 0.9951\n",
            "Epoch 00446: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0614 - accuracy: 0.9946 - val_loss: 0.0560 - val_accuracy: 0.9959\n",
            "Epoch 447/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0522 - accuracy: 1.0000\n",
            "Epoch 00447: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0526 - accuracy: 1.0000 - val_loss: 0.0529 - val_accuracy: 0.9959\n",
            "Epoch 448/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0481 - accuracy: 1.0000\n",
            "Epoch 00448: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0482 - accuracy: 1.0000 - val_loss: 0.0486 - val_accuracy: 1.0000\n",
            "Epoch 449/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0503 - accuracy: 0.9984\n",
            "Epoch 00449: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0502 - accuracy: 0.9986 - val_loss: 0.0565 - val_accuracy: 0.9959\n",
            "Epoch 450/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0469 - accuracy: 1.0000\n",
            "Epoch 00450: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0465 - accuracy: 1.0000 - val_loss: 0.0579 - val_accuracy: 0.9959\n",
            "Epoch 451/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0506 - accuracy: 0.9965\n",
            "Epoch 00451: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0511 - accuracy: 0.9959 - val_loss: 0.0553 - val_accuracy: 0.9959\n",
            "Epoch 452/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0532 - accuracy: 0.9984\n",
            "Epoch 00452: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0518 - accuracy: 0.9986 - val_loss: 0.0457 - val_accuracy: 1.0000\n",
            "Epoch 453/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0487 - accuracy: 1.0000\n",
            "Epoch 00453: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0479 - accuracy: 1.0000 - val_loss: 0.0447 - val_accuracy: 1.0000\n",
            "Epoch 454/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0447 - accuracy: 1.0000\n",
            "Epoch 00454: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0445 - accuracy: 1.0000 - val_loss: 0.0444 - val_accuracy: 1.0000\n",
            "Epoch 455/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0464 - accuracy: 0.9984\n",
            "Epoch 00455: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0458 - accuracy: 0.9986 - val_loss: 0.0438 - val_accuracy: 1.0000\n",
            "Epoch 456/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0442 - accuracy: 1.0000\n",
            "Epoch 00456: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0441 - accuracy: 1.0000 - val_loss: 0.0459 - val_accuracy: 1.0000\n",
            "Epoch 457/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0423 - accuracy: 1.0000\n",
            "Epoch 00457: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0422 - accuracy: 1.0000 - val_loss: 0.0428 - val_accuracy: 1.0000\n",
            "Epoch 458/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0423 - accuracy: 1.0000\n",
            "Epoch 00458: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0420 - accuracy: 1.0000 - val_loss: 0.0417 - val_accuracy: 1.0000\n",
            "Epoch 459/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0406 - accuracy: 1.0000\n",
            "Epoch 00459: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0408 - accuracy: 1.0000 - val_loss: 0.0405 - val_accuracy: 1.0000\n",
            "Epoch 460/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0407 - accuracy: 1.0000\n",
            "Epoch 00460: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0405 - accuracy: 1.0000 - val_loss: 0.0399 - val_accuracy: 1.0000\n",
            "Epoch 461/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0560 - accuracy: 0.9934\n",
            "Epoch 00461: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0537 - accuracy: 0.9946 - val_loss: 0.0454 - val_accuracy: 1.0000\n",
            "Epoch 462/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0670 - accuracy: 0.9885\n",
            "Epoch 00462: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0677 - accuracy: 0.9878 - val_loss: 0.0874 - val_accuracy: 0.9756\n",
            "Epoch 463/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0806 - accuracy: 0.9918\n",
            "Epoch 00463: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0764 - accuracy: 0.9918 - val_loss: 0.1007 - val_accuracy: 0.9797\n",
            "Epoch 464/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0739 - accuracy: 0.9984\n",
            "Epoch 00464: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0711 - accuracy: 0.9986 - val_loss: 0.0610 - val_accuracy: 0.9959\n",
            "Epoch 465/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0466 - accuracy: 1.0000\n",
            "Epoch 00465: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0464 - accuracy: 1.0000 - val_loss: 0.0459 - val_accuracy: 1.0000\n",
            "Epoch 466/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0437 - accuracy: 1.0000\n",
            "Epoch 00466: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0437 - accuracy: 1.0000 - val_loss: 0.0436 - val_accuracy: 1.0000\n",
            "Epoch 467/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0453 - accuracy: 0.9984\n",
            "Epoch 00467: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0452 - accuracy: 0.9986 - val_loss: 0.0436 - val_accuracy: 1.0000\n",
            "Epoch 468/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0472 - accuracy: 0.9984\n",
            "Epoch 00468: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0483 - accuracy: 0.9973 - val_loss: 0.0544 - val_accuracy: 0.9959\n",
            "Epoch 469/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0505 - accuracy: 0.9984\n",
            "Epoch 00469: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0505 - accuracy: 0.9973 - val_loss: 0.0441 - val_accuracy: 1.0000\n",
            "Epoch 470/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0460 - accuracy: 0.9984\n",
            "Epoch 00470: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0453 - accuracy: 0.9986 - val_loss: 0.0437 - val_accuracy: 1.0000\n",
            "Epoch 471/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0444 - accuracy: 0.9983\n",
            "Epoch 00471: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0442 - accuracy: 0.9986 - val_loss: 0.0423 - val_accuracy: 1.0000\n",
            "Epoch 472/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0431 - accuracy: 1.0000\n",
            "Epoch 00472: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0442 - accuracy: 1.0000 - val_loss: 0.0409 - val_accuracy: 1.0000\n",
            "Epoch 473/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0428 - accuracy: 1.0000\n",
            "Epoch 00473: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0425 - accuracy: 1.0000 - val_loss: 0.0423 - val_accuracy: 1.0000\n",
            "Epoch 474/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0405 - accuracy: 1.0000\n",
            "Epoch 00474: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0404 - accuracy: 1.0000 - val_loss: 0.0401 - val_accuracy: 1.0000\n",
            "Epoch 475/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0410 - accuracy: 1.0000\n",
            "Epoch 00475: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.0400 - val_accuracy: 1.0000\n",
            "Epoch 476/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0426 - accuracy: 0.9984\n",
            "Epoch 00476: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0433 - accuracy: 0.9973 - val_loss: 0.0424 - val_accuracy: 1.0000\n",
            "Epoch 477/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0528 - accuracy: 0.9948\n",
            "Epoch 00477: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0547 - accuracy: 0.9946 - val_loss: 0.0705 - val_accuracy: 0.9919\n",
            "Epoch 478/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0525 - accuracy: 0.9951\n",
            "Epoch 00478: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0527 - accuracy: 0.9946 - val_loss: 0.0618 - val_accuracy: 0.9878\n",
            "Epoch 479/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0525 - accuracy: 0.9951\n",
            "Epoch 00479: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0530 - accuracy: 0.9946 - val_loss: 0.0559 - val_accuracy: 0.9919\n",
            "Epoch 480/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0470 - accuracy: 0.9967\n",
            "Epoch 00480: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0461 - accuracy: 0.9973 - val_loss: 0.0440 - val_accuracy: 1.0000\n",
            "Epoch 481/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0413 - accuracy: 1.0000\n",
            "Epoch 00481: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0412 - accuracy: 1.0000 - val_loss: 0.0426 - val_accuracy: 1.0000\n",
            "Epoch 482/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0446 - accuracy: 0.9984\n",
            "Epoch 00482: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0449 - accuracy: 0.9986 - val_loss: 0.0449 - val_accuracy: 1.0000\n",
            "Epoch 483/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0440 - accuracy: 0.9984\n",
            "Epoch 00483: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0438 - accuracy: 0.9986 - val_loss: 0.0421 - val_accuracy: 1.0000\n",
            "Epoch 484/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0409 - accuracy: 1.0000\n",
            "Epoch 00484: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0407 - accuracy: 1.0000 - val_loss: 0.0434 - val_accuracy: 1.0000\n",
            "Epoch 485/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0406 - accuracy: 0.9984\n",
            "Epoch 00485: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0405 - accuracy: 0.9986 - val_loss: 0.0432 - val_accuracy: 0.9959\n",
            "Epoch 486/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0459 - accuracy: 0.9967\n",
            "Epoch 00486: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0455 - accuracy: 0.9973 - val_loss: 0.0384 - val_accuracy: 1.0000\n",
            "Epoch 487/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0429 - accuracy: 0.9967\n",
            "Epoch 00487: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0432 - accuracy: 0.9973 - val_loss: 0.0903 - val_accuracy: 0.9756\n",
            "Epoch 488/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0598 - accuracy: 0.9934\n",
            "Epoch 00488: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0588 - accuracy: 0.9932 - val_loss: 0.0503 - val_accuracy: 0.9959\n",
            "Epoch 489/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0552 - accuracy: 0.9951\n",
            "Epoch 00489: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0561 - accuracy: 0.9932 - val_loss: 0.0494 - val_accuracy: 0.9959\n",
            "Epoch 490/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0465 - accuracy: 0.9984\n",
            "Epoch 00490: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0459 - accuracy: 0.9986 - val_loss: 0.0439 - val_accuracy: 1.0000\n",
            "Epoch 491/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0450 - accuracy: 0.9967\n",
            "Epoch 00491: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0443 - accuracy: 0.9973 - val_loss: 0.0398 - val_accuracy: 1.0000\n",
            "Epoch 492/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0424 - accuracy: 0.9984\n",
            "Epoch 00492: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0422 - accuracy: 0.9986 - val_loss: 0.0392 - val_accuracy: 1.0000\n",
            "Epoch 493/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0527 - accuracy: 0.9967\n",
            "Epoch 00493: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0576 - accuracy: 0.9946 - val_loss: 0.0467 - val_accuracy: 1.0000\n",
            "Epoch 494/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0501 - accuracy: 0.9984\n",
            "Epoch 00494: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0490 - accuracy: 0.9986 - val_loss: 0.0426 - val_accuracy: 1.0000\n",
            "Epoch 495/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0451 - accuracy: 0.9984\n",
            "Epoch 00495: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0479 - accuracy: 0.9959 - val_loss: 0.0527 - val_accuracy: 0.9959\n",
            "Epoch 496/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0508 - accuracy: 0.9967\n",
            "Epoch 00496: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0511 - accuracy: 0.9959 - val_loss: 0.0451 - val_accuracy: 0.9959\n",
            "Epoch 497/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0514 - accuracy: 0.9967\n",
            "Epoch 00497: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0513 - accuracy: 0.9959 - val_loss: 0.0712 - val_accuracy: 0.9919\n",
            "Epoch 498/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0447 - accuracy: 0.9984\n",
            "Epoch 00498: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0445 - accuracy: 0.9986 - val_loss: 0.0417 - val_accuracy: 1.0000\n",
            "Epoch 499/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0429 - accuracy: 1.0000\n",
            "Epoch 00499: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0422 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 1.0000\n",
            "Epoch 500/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0396 - accuracy: 1.0000\n",
            "Epoch 00500: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0410 - accuracy: 0.9986 - val_loss: 0.0390 - val_accuracy: 1.0000\n",
            "Epoch 501/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0402 - accuracy: 1.0000\n",
            "Epoch 00501: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0399 - accuracy: 1.0000 - val_loss: 0.0407 - val_accuracy: 1.0000\n",
            "Epoch 502/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0430 - accuracy: 0.9984\n",
            "Epoch 00502: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0431 - accuracy: 0.9986 - val_loss: 0.0387 - val_accuracy: 1.0000\n",
            "Epoch 503/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0434 - accuracy: 0.9984\n",
            "Epoch 00503: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0433 - accuracy: 0.9986 - val_loss: 0.0849 - val_accuracy: 0.9715\n",
            "Epoch 504/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0564 - accuracy: 0.9951\n",
            "Epoch 00504: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0535 - accuracy: 0.9959 - val_loss: 0.0423 - val_accuracy: 1.0000\n",
            "Epoch 505/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0395 - accuracy: 1.0000\n",
            "Epoch 00505: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0422 - accuracy: 0.9986 - val_loss: 0.0397 - val_accuracy: 1.0000\n",
            "Epoch 506/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0475 - accuracy: 0.9967\n",
            "Epoch 00506: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0467 - accuracy: 0.9973 - val_loss: 0.0460 - val_accuracy: 0.9959\n",
            "Epoch 507/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0528 - accuracy: 0.9951\n",
            "Epoch 00507: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0549 - accuracy: 0.9932 - val_loss: 0.0500 - val_accuracy: 0.9959\n",
            "Epoch 508/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0648 - accuracy: 0.9896\n",
            "Epoch 00508: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0613 - accuracy: 0.9918 - val_loss: 0.0550 - val_accuracy: 1.0000\n",
            "Epoch 509/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0599 - accuracy: 0.9983\n",
            "Epoch 00509: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0568 - accuracy: 0.9986 - val_loss: 0.0806 - val_accuracy: 0.9878\n",
            "Epoch 510/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0422 - accuracy: 1.0000\n",
            "Epoch 00510: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0440 - accuracy: 0.9986 - val_loss: 0.0672 - val_accuracy: 0.9878\n",
            "Epoch 511/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0418 - accuracy: 0.9984\n",
            "Epoch 00511: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0419 - accuracy: 0.9986 - val_loss: 0.0402 - val_accuracy: 1.0000\n",
            "Epoch 512/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0412 - accuracy: 1.0000\n",
            "Epoch 00512: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0410 - accuracy: 1.0000 - val_loss: 0.0403 - val_accuracy: 1.0000\n",
            "Epoch 513/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0396 - accuracy: 1.0000\n",
            "Epoch 00513: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0395 - accuracy: 1.0000 - val_loss: 0.0404 - val_accuracy: 1.0000\n",
            "Epoch 514/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0443 - accuracy: 0.9967\n",
            "Epoch 00514: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0437 - accuracy: 0.9973 - val_loss: 0.0394 - val_accuracy: 1.0000\n",
            "Epoch 515/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0439 - accuracy: 0.9967\n",
            "Epoch 00515: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0432 - accuracy: 0.9973 - val_loss: 0.0426 - val_accuracy: 1.0000\n",
            "Epoch 516/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0390 - accuracy: 1.0000\n",
            "Epoch 00516: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0391 - accuracy: 1.0000 - val_loss: 0.0376 - val_accuracy: 1.0000\n",
            "Epoch 517/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0381 - accuracy: 1.0000\n",
            "Epoch 00517: loss did not improve from 0.03781\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0381 - accuracy: 1.0000 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
            "Epoch 518/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0376 - accuracy: 1.0000\n",
            "Epoch 00518: loss improved from 0.03781 to 0.03741, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 0.0383 - val_accuracy: 1.0000\n",
            "Epoch 519/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0394 - accuracy: 0.9984\n",
            "Epoch 00519: loss did not improve from 0.03741\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0390 - accuracy: 0.9986 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
            "Epoch 520/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0372 - accuracy: 1.0000\n",
            "Epoch 00520: loss did not improve from 0.03741\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0378 - accuracy: 1.0000 - val_loss: 0.0360 - val_accuracy: 1.0000\n",
            "Epoch 521/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0377 - accuracy: 1.0000\n",
            "Epoch 00521: loss did not improve from 0.03741\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0386 - accuracy: 1.0000 - val_loss: 0.0369 - val_accuracy: 1.0000\n",
            "Epoch 522/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0370 - accuracy: 1.0000\n",
            "Epoch 00522: loss improved from 0.03741 to 0.03674, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 1.0000\n",
            "Epoch 523/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0357 - accuracy: 1.0000\n",
            "Epoch 00523: loss improved from 0.03674 to 0.03582, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
            "Epoch 524/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0361 - accuracy: 1.0000\n",
            "Epoch 00524: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 1.0000\n",
            "Epoch 525/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0485 - accuracy: 0.9951\n",
            "Epoch 00525: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0499 - accuracy: 0.9946 - val_loss: 0.0737 - val_accuracy: 0.9878\n",
            "Epoch 526/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0780 - accuracy: 0.9852\n",
            "Epoch 00526: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0733 - accuracy: 0.9878 - val_loss: 0.0654 - val_accuracy: 0.9878\n",
            "Epoch 527/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0607 - accuracy: 0.9931\n",
            "Epoch 00527: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0586 - accuracy: 0.9946 - val_loss: 0.0571 - val_accuracy: 0.9959\n",
            "Epoch 528/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0478 - accuracy: 0.9984\n",
            "Epoch 00528: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0466 - accuracy: 0.9986 - val_loss: 0.0419 - val_accuracy: 1.0000\n",
            "Epoch 529/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0514 - accuracy: 0.9965\n",
            "Epoch 00529: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0503 - accuracy: 0.9959 - val_loss: 0.0475 - val_accuracy: 0.9959\n",
            "Epoch 530/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0420 - accuracy: 1.0000\n",
            "Epoch 00530: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0427 - accuracy: 1.0000 - val_loss: 0.0399 - val_accuracy: 1.0000\n",
            "Epoch 531/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0426 - accuracy: 0.9984\n",
            "Epoch 00531: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0453 - accuracy: 0.9973 - val_loss: 0.0549 - val_accuracy: 0.9959\n",
            "Epoch 532/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0432 - accuracy: 0.9984\n",
            "Epoch 00532: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0459 - accuracy: 0.9973 - val_loss: 0.0441 - val_accuracy: 0.9959\n",
            "Epoch 533/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0401 - accuracy: 1.0000\n",
            "Epoch 00533: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0402 - accuracy: 1.0000 - val_loss: 0.0398 - val_accuracy: 1.0000\n",
            "Epoch 534/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0484 - accuracy: 0.9967\n",
            "Epoch 00534: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0469 - accuracy: 0.9973 - val_loss: 0.0441 - val_accuracy: 1.0000\n",
            "Epoch 535/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0493 - accuracy: 0.9984\n",
            "Epoch 00535: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0534 - accuracy: 0.9973 - val_loss: 0.0456 - val_accuracy: 0.9959\n",
            "Epoch 536/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0437 - accuracy: 0.9984\n",
            "Epoch 00536: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0483 - accuracy: 0.9973 - val_loss: 0.0385 - val_accuracy: 1.0000\n",
            "Epoch 537/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0792 - accuracy: 0.9836\n",
            "Epoch 00537: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0746 - accuracy: 0.9864 - val_loss: 0.0576 - val_accuracy: 0.9919\n",
            "Epoch 538/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0659 - accuracy: 0.9901\n",
            "Epoch 00538: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0683 - accuracy: 0.9891 - val_loss: 0.0562 - val_accuracy: 0.9959\n",
            "Epoch 539/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0559 - accuracy: 0.9918\n",
            "Epoch 00539: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0610 - accuracy: 0.9918 - val_loss: 0.0546 - val_accuracy: 0.9959\n",
            "Epoch 540/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0434 - accuracy: 1.0000\n",
            "Epoch 00540: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0436 - accuracy: 1.0000 - val_loss: 0.0535 - val_accuracy: 0.9959\n",
            "Epoch 541/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0471 - accuracy: 0.9984\n",
            "Epoch 00541: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0475 - accuracy: 0.9973 - val_loss: 0.0464 - val_accuracy: 1.0000\n",
            "Epoch 542/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0488 - accuracy: 0.9984\n",
            "Epoch 00542: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0477 - accuracy: 0.9986 - val_loss: 0.0515 - val_accuracy: 0.9959\n",
            "Epoch 543/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0445 - accuracy: 1.0000\n",
            "Epoch 00543: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0440 - accuracy: 1.0000 - val_loss: 0.0438 - val_accuracy: 1.0000\n",
            "Epoch 544/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0453 - accuracy: 0.9983\n",
            "Epoch 00544: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0448 - accuracy: 0.9986 - val_loss: 0.0534 - val_accuracy: 0.9959\n",
            "Epoch 545/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0409 - accuracy: 1.0000\n",
            "Epoch 00545: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0408 - accuracy: 1.0000 - val_loss: 0.0452 - val_accuracy: 1.0000\n",
            "Epoch 546/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0401 - accuracy: 1.0000\n",
            "Epoch 00546: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0399 - accuracy: 1.0000 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
            "Epoch 547/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 1.0000\n",
            "Epoch 00547: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0391 - accuracy: 1.0000 - val_loss: 0.0386 - val_accuracy: 1.0000\n",
            "Epoch 548/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0383 - accuracy: 1.0000\n",
            "Epoch 00548: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0382 - accuracy: 1.0000 - val_loss: 0.0378 - val_accuracy: 1.0000\n",
            "Epoch 549/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0373 - accuracy: 1.0000\n",
            "Epoch 00549: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
            "Epoch 550/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0371 - accuracy: 1.0000\n",
            "Epoch 00550: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0369 - accuracy: 1.0000 - val_loss: 0.0378 - val_accuracy: 1.0000\n",
            "Epoch 551/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0360 - accuracy: 1.0000\n",
            "Epoch 00551: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
            "Epoch 552/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0381 - accuracy: 0.9984\n",
            "Epoch 00552: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0380 - accuracy: 0.9986 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
            "Epoch 553/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0428 - accuracy: 0.9967\n",
            "Epoch 00553: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0430 - accuracy: 0.9959 - val_loss: 0.1005 - val_accuracy: 0.9756\n",
            "Epoch 554/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0994 - accuracy: 0.9852\n",
            "Epoch 00554: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1108 - accuracy: 0.9782 - val_loss: 0.0803 - val_accuracy: 0.9837\n",
            "Epoch 555/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0580 - accuracy: 0.9948\n",
            "Epoch 00555: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0577 - accuracy: 0.9932 - val_loss: 0.0535 - val_accuracy: 1.0000\n",
            "Epoch 556/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0489 - accuracy: 0.9967\n",
            "Epoch 00556: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0483 - accuracy: 0.9973 - val_loss: 0.0422 - val_accuracy: 1.0000\n",
            "Epoch 557/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0602 - accuracy: 0.9934\n",
            "Epoch 00557: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0579 - accuracy: 0.9946 - val_loss: 0.0626 - val_accuracy: 0.9919\n",
            "Epoch 558/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0504 - accuracy: 0.9934\n",
            "Epoch 00558: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0504 - accuracy: 0.9946 - val_loss: 0.0471 - val_accuracy: 1.0000\n",
            "Epoch 559/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0755 - accuracy: 0.9885\n",
            "Epoch 00559: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0763 - accuracy: 0.9878 - val_loss: 0.1250 - val_accuracy: 0.9675\n",
            "Epoch 560/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0547 - accuracy: 0.9967\n",
            "Epoch 00560: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0527 - accuracy: 0.9973 - val_loss: 0.0428 - val_accuracy: 1.0000\n",
            "Epoch 561/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0450 - accuracy: 1.0000\n",
            "Epoch 00561: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0469 - accuracy: 0.9986 - val_loss: 0.0423 - val_accuracy: 1.0000\n",
            "Epoch 562/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0478 - accuracy: 0.9984\n",
            "Epoch 00562: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0467 - accuracy: 0.9986 - val_loss: 0.0419 - val_accuracy: 1.0000\n",
            "Epoch 563/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0480 - accuracy: 0.9965\n",
            "Epoch 00563: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0579 - accuracy: 0.9959 - val_loss: 0.0448 - val_accuracy: 1.0000\n",
            "Epoch 564/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0487 - accuracy: 0.9983\n",
            "Epoch 00564: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0487 - accuracy: 0.9973 - val_loss: 0.0455 - val_accuracy: 1.0000\n",
            "Epoch 565/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0455 - accuracy: 1.0000\n",
            "Epoch 00565: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0445 - accuracy: 1.0000 - val_loss: 0.0420 - val_accuracy: 1.0000\n",
            "Epoch 566/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0475 - accuracy: 0.9984\n",
            "Epoch 00566: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0473 - accuracy: 0.9986 - val_loss: 0.0568 - val_accuracy: 0.9959\n",
            "Epoch 567/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0443 - accuracy: 1.0000\n",
            "Epoch 00567: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0442 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 1.0000\n",
            "Epoch 568/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0424 - accuracy: 0.9983\n",
            "Epoch 00568: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0418 - accuracy: 0.9986 - val_loss: 0.0417 - val_accuracy: 1.0000\n",
            "Epoch 569/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0403 - accuracy: 1.0000\n",
            "Epoch 00569: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 0.0429 - val_accuracy: 1.0000\n",
            "Epoch 570/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0389 - accuracy: 1.0000\n",
            "Epoch 00570: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0388 - accuracy: 1.0000 - val_loss: 0.0411 - val_accuracy: 1.0000\n",
            "Epoch 571/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0410 - accuracy: 1.0000\n",
            "Epoch 00571: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0404 - accuracy: 1.0000 - val_loss: 0.0408 - val_accuracy: 1.0000\n",
            "Epoch 572/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0387 - accuracy: 1.0000\n",
            "Epoch 00572: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0386 - accuracy: 1.0000 - val_loss: 0.0401 - val_accuracy: 1.0000\n",
            "Epoch 573/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0379 - accuracy: 1.0000\n",
            "Epoch 00573: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0376 - accuracy: 1.0000 - val_loss: 0.0402 - val_accuracy: 1.0000\n",
            "Epoch 574/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0383 - accuracy: 1.0000\n",
            "Epoch 00574: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0387 - accuracy: 1.0000 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
            "Epoch 575/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0367 - accuracy: 1.0000\n",
            "Epoch 00575: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0366 - accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
            "Epoch 576/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0365 - accuracy: 1.0000\n",
            "Epoch 00576: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 1.0000\n",
            "Epoch 577/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0361 - accuracy: 1.0000\n",
            "Epoch 00577: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0362 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 1.0000\n",
            "Epoch 578/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0374 - accuracy: 1.0000\n",
            "Epoch 00578: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0370 - accuracy: 1.0000 - val_loss: 0.0368 - val_accuracy: 1.0000\n",
            "Epoch 579/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0351 - accuracy: 1.0000\n",
            "Epoch 00579: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0371 - accuracy: 0.9986 - val_loss: 0.0350 - val_accuracy: 1.0000\n",
            "Epoch 580/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0382 - accuracy: 1.0000\n",
            "Epoch 00580: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 0.0394 - val_accuracy: 0.9959\n",
            "Epoch 581/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0373 - accuracy: 0.9984\n",
            "Epoch 00581: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0369 - accuracy: 0.9986 - val_loss: 0.0455 - val_accuracy: 0.9959\n",
            "Epoch 582/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0367 - accuracy: 0.9983\n",
            "Epoch 00582: loss did not improve from 0.03582\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0361 - accuracy: 0.9986 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
            "Epoch 583/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0343 - accuracy: 1.0000\n",
            "Epoch 00583: loss improved from 0.03582 to 0.03527, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0353 - accuracy: 0.9986 - val_loss: 0.0388 - val_accuracy: 0.9959\n",
            "Epoch 584/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0534 - accuracy: 0.9951\n",
            "Epoch 00584: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0619 - accuracy: 0.9946 - val_loss: 0.1209 - val_accuracy: 0.9756\n",
            "Epoch 585/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0669 - accuracy: 0.9868\n",
            "Epoch 00585: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0650 - accuracy: 0.9878 - val_loss: 0.1244 - val_accuracy: 0.9756\n",
            "Epoch 586/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0700 - accuracy: 0.9913\n",
            "Epoch 00586: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0728 - accuracy: 0.9918 - val_loss: 0.0479 - val_accuracy: 1.0000\n",
            "Epoch 587/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0450 - accuracy: 1.0000\n",
            "Epoch 00587: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0439 - accuracy: 1.0000 - val_loss: 0.0436 - val_accuracy: 1.0000\n",
            "Epoch 588/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0404 - accuracy: 1.0000\n",
            "Epoch 00588: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0399 - accuracy: 1.0000 - val_loss: 0.0405 - val_accuracy: 1.0000\n",
            "Epoch 589/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0396 - accuracy: 1.0000\n",
            "Epoch 00589: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0398 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 1.0000\n",
            "Epoch 590/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0431 - accuracy: 0.9967\n",
            "Epoch 00590: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0468 - accuracy: 0.9946 - val_loss: 0.0385 - val_accuracy: 1.0000\n",
            "Epoch 591/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0480 - accuracy: 0.9967\n",
            "Epoch 00591: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0467 - accuracy: 0.9973 - val_loss: 0.0507 - val_accuracy: 1.0000\n",
            "Epoch 592/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0589 - accuracy: 0.9934\n",
            "Epoch 00592: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0562 - accuracy: 0.9946 - val_loss: 0.0456 - val_accuracy: 1.0000\n",
            "Epoch 593/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0414 - accuracy: 1.0000\n",
            "Epoch 00593: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.0398 - val_accuracy: 1.0000\n",
            "Epoch 594/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0396 - accuracy: 1.0000\n",
            "Epoch 00594: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0392 - accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
            "Epoch 595/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0401 - accuracy: 0.9984\n",
            "Epoch 00595: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0423 - accuracy: 0.9973 - val_loss: 0.0443 - val_accuracy: 1.0000\n",
            "Epoch 596/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0512 - accuracy: 0.9984\n",
            "Epoch 00596: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0523 - accuracy: 0.9973 - val_loss: 0.0442 - val_accuracy: 0.9959\n",
            "Epoch 597/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0585 - accuracy: 0.9885\n",
            "Epoch 00597: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0560 - accuracy: 0.9905 - val_loss: 0.0537 - val_accuracy: 0.9959\n",
            "Epoch 598/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0495 - accuracy: 0.9948\n",
            "Epoch 00598: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0487 - accuracy: 0.9959 - val_loss: 0.0458 - val_accuracy: 0.9959\n",
            "Epoch 599/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0459 - accuracy: 0.9965\n",
            "Epoch 00599: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0454 - accuracy: 0.9973 - val_loss: 0.0606 - val_accuracy: 0.9919\n",
            "Epoch 600/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0564 - accuracy: 0.9951\n",
            "Epoch 00600: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0561 - accuracy: 0.9946 - val_loss: 0.0427 - val_accuracy: 1.0000\n",
            "Epoch 601/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0503 - accuracy: 0.9951\n",
            "Epoch 00601: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0494 - accuracy: 0.9959 - val_loss: 0.0416 - val_accuracy: 1.0000\n",
            "Epoch 602/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0456 - accuracy: 0.9984\n",
            "Epoch 00602: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0455 - accuracy: 0.9986 - val_loss: 0.0399 - val_accuracy: 1.0000\n",
            "Epoch 603/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0535 - accuracy: 0.9951\n",
            "Epoch 00603: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0518 - accuracy: 0.9959 - val_loss: 0.0519 - val_accuracy: 1.0000\n",
            "Epoch 604/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0448 - accuracy: 0.9984\n",
            "Epoch 00604: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0440 - accuracy: 0.9986 - val_loss: 0.0428 - val_accuracy: 1.0000\n",
            "Epoch 605/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0432 - accuracy: 0.9983\n",
            "Epoch 00605: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0455 - accuracy: 0.9973 - val_loss: 0.0433 - val_accuracy: 1.0000\n",
            "Epoch 606/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0497 - accuracy: 0.9967\n",
            "Epoch 00606: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0478 - accuracy: 0.9973 - val_loss: 0.0490 - val_accuracy: 0.9959\n",
            "Epoch 607/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0554 - accuracy: 0.9934\n",
            "Epoch 00607: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0527 - accuracy: 0.9946 - val_loss: 0.0479 - val_accuracy: 0.9959\n",
            "Epoch 608/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0538 - accuracy: 0.9984\n",
            "Epoch 00608: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0529 - accuracy: 0.9973 - val_loss: 0.0495 - val_accuracy: 1.0000\n",
            "Epoch 609/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0521 - accuracy: 0.9965\n",
            "Epoch 00609: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0499 - accuracy: 0.9973 - val_loss: 0.0451 - val_accuracy: 1.0000\n",
            "Epoch 610/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0499 - accuracy: 0.9934\n",
            "Epoch 00610: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0489 - accuracy: 0.9946 - val_loss: 0.0716 - val_accuracy: 0.9837\n",
            "Epoch 611/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0416 - accuracy: 1.0000\n",
            "Epoch 00611: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0412 - accuracy: 1.0000 - val_loss: 0.0402 - val_accuracy: 1.0000\n",
            "Epoch 612/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0479 - accuracy: 0.9967\n",
            "Epoch 00612: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0462 - accuracy: 0.9973 - val_loss: 0.0401 - val_accuracy: 1.0000\n",
            "Epoch 613/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0388 - accuracy: 1.0000\n",
            "Epoch 00613: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0387 - accuracy: 1.0000 - val_loss: 0.0391 - val_accuracy: 1.0000\n",
            "Epoch 614/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0379 - accuracy: 1.0000\n",
            "Epoch 00614: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0379 - accuracy: 1.0000 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
            "Epoch 615/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0376 - accuracy: 1.0000\n",
            "Epoch 00615: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 0.0368 - val_accuracy: 1.0000\n",
            "Epoch 616/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0373 - accuracy: 1.0000\n",
            "Epoch 00616: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0372 - accuracy: 1.0000 - val_loss: 0.0361 - val_accuracy: 1.0000\n",
            "Epoch 617/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0381 - accuracy: 1.0000\n",
            "Epoch 00617: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0377 - accuracy: 1.0000 - val_loss: 0.0358 - val_accuracy: 1.0000\n",
            "Epoch 618/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0364 - accuracy: 1.0000\n",
            "Epoch 00618: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0388 - accuracy: 0.9986 - val_loss: 0.0362 - val_accuracy: 1.0000\n",
            "Epoch 619/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0362 - accuracy: 1.0000\n",
            "Epoch 00619: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.0350 - val_accuracy: 1.0000\n",
            "Epoch 620/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0364 - accuracy: 1.0000\n",
            "Epoch 00620: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.0344 - val_accuracy: 1.0000\n",
            "Epoch 621/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0426 - accuracy: 0.9967\n",
            "Epoch 00621: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0416 - accuracy: 0.9973 - val_loss: 0.0347 - val_accuracy: 1.0000\n",
            "Epoch 622/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0358 - accuracy: 1.0000\n",
            "Epoch 00622: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0364 - accuracy: 1.0000 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
            "Epoch 623/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0583 - accuracy: 0.9967\n",
            "Epoch 00623: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0626 - accuracy: 0.9946 - val_loss: 0.0474 - val_accuracy: 0.9959\n",
            "Epoch 624/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0522 - accuracy: 0.9918\n",
            "Epoch 00624: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0499 - accuracy: 0.9932 - val_loss: 0.0449 - val_accuracy: 1.0000\n",
            "Epoch 625/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0493 - accuracy: 0.9951\n",
            "Epoch 00625: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0508 - accuracy: 0.9932 - val_loss: 0.0428 - val_accuracy: 1.0000\n",
            "Epoch 626/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0594 - accuracy: 0.9918\n",
            "Epoch 00626: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0579 - accuracy: 0.9918 - val_loss: 0.0699 - val_accuracy: 0.9878\n",
            "Epoch 627/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0510 - accuracy: 0.9967\n",
            "Epoch 00627: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0499 - accuracy: 0.9973 - val_loss: 0.0437 - val_accuracy: 1.0000\n",
            "Epoch 628/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0557 - accuracy: 0.9967\n",
            "Epoch 00628: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0583 - accuracy: 0.9959 - val_loss: 0.0604 - val_accuracy: 0.9878\n",
            "Epoch 629/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0446 - accuracy: 0.9984\n",
            "Epoch 00629: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0438 - accuracy: 0.9986 - val_loss: 0.0498 - val_accuracy: 0.9959\n",
            "Epoch 630/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0497 - accuracy: 0.9951\n",
            "Epoch 00630: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0483 - accuracy: 0.9959 - val_loss: 0.0465 - val_accuracy: 0.9959\n",
            "Epoch 631/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0468 - accuracy: 0.9984\n",
            "Epoch 00631: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0461 - accuracy: 0.9986 - val_loss: 0.0421 - val_accuracy: 1.0000\n",
            "Epoch 632/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0488 - accuracy: 0.9967\n",
            "Epoch 00632: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0487 - accuracy: 0.9973 - val_loss: 0.0469 - val_accuracy: 1.0000\n",
            "Epoch 633/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0525 - accuracy: 0.9967\n",
            "Epoch 00633: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0505 - accuracy: 0.9973 - val_loss: 0.0429 - val_accuracy: 1.0000\n",
            "Epoch 634/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0414 - accuracy: 1.0000\n",
            "Epoch 00634: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0431 - accuracy: 0.9986 - val_loss: 0.0389 - val_accuracy: 1.0000\n",
            "Epoch 635/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0429 - accuracy: 0.9965\n",
            "Epoch 00635: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0421 - accuracy: 0.9973 - val_loss: 0.0616 - val_accuracy: 0.9919\n",
            "Epoch 636/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0405 - accuracy: 1.0000\n",
            "Epoch 00636: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0415 - accuracy: 0.9986 - val_loss: 0.0376 - val_accuracy: 1.0000\n",
            "Epoch 637/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0394 - accuracy: 0.9984\n",
            "Epoch 00637: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0424 - accuracy: 0.9973 - val_loss: 0.0395 - val_accuracy: 1.0000\n",
            "Epoch 638/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0457 - accuracy: 0.9967\n",
            "Epoch 00638: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0447 - accuracy: 0.9973 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
            "Epoch 639/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0422 - accuracy: 0.9967\n",
            "Epoch 00639: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0413 - accuracy: 0.9973 - val_loss: 0.0575 - val_accuracy: 0.9959\n",
            "Epoch 640/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0401 - accuracy: 0.9984\n",
            "Epoch 00640: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0401 - accuracy: 0.9986 - val_loss: 0.0390 - val_accuracy: 1.0000\n",
            "Epoch 641/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0379 - accuracy: 1.0000\n",
            "Epoch 00641: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0380 - accuracy: 1.0000 - val_loss: 0.0526 - val_accuracy: 0.9959\n",
            "Epoch 642/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0369 - accuracy: 1.0000\n",
            "Epoch 00642: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
            "Epoch 643/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0359 - accuracy: 1.0000\n",
            "Epoch 00643: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.0354 - val_accuracy: 1.0000\n",
            "Epoch 644/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0474 - accuracy: 0.9918\n",
            "Epoch 00644: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0532 - accuracy: 0.9905 - val_loss: 0.0839 - val_accuracy: 0.9797\n",
            "Epoch 645/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0589 - accuracy: 0.9934\n",
            "Epoch 00645: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0596 - accuracy: 0.9932 - val_loss: 0.0670 - val_accuracy: 0.9878\n",
            "Epoch 646/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0586 - accuracy: 0.9934\n",
            "Epoch 00646: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0557 - accuracy: 0.9946 - val_loss: 0.0689 - val_accuracy: 0.9797\n",
            "Epoch 647/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0566 - accuracy: 0.9951\n",
            "Epoch 00647: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0560 - accuracy: 0.9946 - val_loss: 0.0474 - val_accuracy: 0.9959\n",
            "Epoch 648/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0467 - accuracy: 0.9967\n",
            "Epoch 00648: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0455 - accuracy: 0.9973 - val_loss: 0.0473 - val_accuracy: 0.9959\n",
            "Epoch 649/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0742 - accuracy: 0.9901\n",
            "Epoch 00649: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0756 - accuracy: 0.9878 - val_loss: 0.0464 - val_accuracy: 1.0000\n",
            "Epoch 650/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0489 - accuracy: 0.9984\n",
            "Epoch 00650: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0499 - accuracy: 0.9973 - val_loss: 0.0429 - val_accuracy: 1.0000\n",
            "Epoch 651/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0447 - accuracy: 1.0000\n",
            "Epoch 00651: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0444 - accuracy: 1.0000 - val_loss: 0.0407 - val_accuracy: 1.0000\n",
            "Epoch 652/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0404 - accuracy: 1.0000\n",
            "Epoch 00652: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0411 - accuracy: 1.0000 - val_loss: 0.0403 - val_accuracy: 1.0000\n",
            "Epoch 653/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0410 - accuracy: 1.0000\n",
            "Epoch 00653: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0414 - accuracy: 1.0000 - val_loss: 0.0390 - val_accuracy: 1.0000\n",
            "Epoch 654/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0388 - accuracy: 1.0000\n",
            "Epoch 00654: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0387 - accuracy: 1.0000 - val_loss: 0.0384 - val_accuracy: 1.0000\n",
            "Epoch 655/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0385 - accuracy: 1.0000\n",
            "Epoch 00655: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0384 - accuracy: 1.0000 - val_loss: 0.0380 - val_accuracy: 1.0000\n",
            "Epoch 656/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0437 - accuracy: 0.9983\n",
            "Epoch 00656: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0423 - accuracy: 0.9986 - val_loss: 0.0453 - val_accuracy: 0.9959\n",
            "Epoch 657/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0450 - accuracy: 0.9967\n",
            "Epoch 00657: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0439 - accuracy: 0.9973 - val_loss: 0.0495 - val_accuracy: 0.9959\n",
            "Epoch 658/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0474 - accuracy: 0.9983\n",
            "Epoch 00658: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0465 - accuracy: 0.9986 - val_loss: 0.1218 - val_accuracy: 0.9675\n",
            "Epoch 659/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0645 - accuracy: 0.9901\n",
            "Epoch 00659: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0618 - accuracy: 0.9918 - val_loss: 0.0623 - val_accuracy: 0.9878\n",
            "Epoch 660/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0479 - accuracy: 0.9951\n",
            "Epoch 00660: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0477 - accuracy: 0.9959 - val_loss: 0.0413 - val_accuracy: 1.0000\n",
            "Epoch 661/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0405 - accuracy: 1.0000\n",
            "Epoch 00661: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0404 - accuracy: 1.0000 - val_loss: 0.0422 - val_accuracy: 1.0000\n",
            "Epoch 662/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0404 - accuracy: 1.0000\n",
            "Epoch 00662: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0401 - accuracy: 1.0000 - val_loss: 0.0384 - val_accuracy: 1.0000\n",
            "Epoch 663/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0466 - accuracy: 0.9951\n",
            "Epoch 00663: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0472 - accuracy: 0.9959 - val_loss: 0.0465 - val_accuracy: 1.0000\n",
            "Epoch 664/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0390 - accuracy: 1.0000\n",
            "Epoch 00664: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0392 - accuracy: 1.0000 - val_loss: 0.0386 - val_accuracy: 1.0000\n",
            "Epoch 665/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0492 - accuracy: 0.9967\n",
            "Epoch 00665: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0482 - accuracy: 0.9973 - val_loss: 0.0438 - val_accuracy: 1.0000\n",
            "Epoch 666/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0391 - accuracy: 1.0000\n",
            "Epoch 00666: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0395 - accuracy: 1.0000 - val_loss: 0.0390 - val_accuracy: 1.0000\n",
            "Epoch 667/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0378 - accuracy: 1.0000\n",
            "Epoch 00667: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0379 - accuracy: 1.0000 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
            "Epoch 668/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0380 - accuracy: 1.0000\n",
            "Epoch 00668: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0378 - accuracy: 1.0000 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
            "Epoch 669/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0363 - accuracy: 1.0000\n",
            "Epoch 00669: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0362 - accuracy: 1.0000 - val_loss: 0.0364 - val_accuracy: 1.0000\n",
            "Epoch 670/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0357 - accuracy: 1.0000\n",
            "Epoch 00670: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 0.0356 - val_accuracy: 1.0000\n",
            "Epoch 671/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0356 - accuracy: 1.0000\n",
            "Epoch 00671: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 0.0352 - val_accuracy: 1.0000\n",
            "Epoch 672/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0353 - accuracy: 1.0000\n",
            "Epoch 00672: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0354 - accuracy: 1.0000 - val_loss: 0.0344 - val_accuracy: 1.0000\n",
            "Epoch 673/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0389 - accuracy: 0.9984\n",
            "Epoch 00673: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0384 - accuracy: 0.9986 - val_loss: 0.0505 - val_accuracy: 0.9919\n",
            "Epoch 674/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0447 - accuracy: 0.9984\n",
            "Epoch 00674: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0431 - accuracy: 0.9986 - val_loss: 0.0464 - val_accuracy: 0.9959\n",
            "Epoch 675/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0430 - accuracy: 0.9967\n",
            "Epoch 00675: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0415 - accuracy: 0.9973 - val_loss: 0.0526 - val_accuracy: 0.9959\n",
            "Epoch 676/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0400 - accuracy: 0.9983\n",
            "Epoch 00676: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0415 - accuracy: 0.9973 - val_loss: 0.0521 - val_accuracy: 0.9959\n",
            "Epoch 677/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0413 - accuracy: 0.9984\n",
            "Epoch 00677: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0402 - accuracy: 0.9986 - val_loss: 0.0390 - val_accuracy: 1.0000\n",
            "Epoch 678/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0358 - accuracy: 1.0000\n",
            "Epoch 00678: loss did not improve from 0.03527\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0376 - accuracy: 0.9986 - val_loss: 0.0443 - val_accuracy: 0.9959\n",
            "Epoch 679/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0353 - accuracy: 1.0000\n",
            "Epoch 00679: loss improved from 0.03527 to 0.03507, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 0.0398 - val_accuracy: 1.0000\n",
            "Epoch 680/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0346 - accuracy: 1.0000\n",
            "Epoch 00680: loss improved from 0.03507 to 0.03474, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
            "Epoch 681/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0349 - accuracy: 1.0000\n",
            "Epoch 00681: loss did not improve from 0.03474\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 0.0423 - val_accuracy: 0.9919\n",
            "Epoch 682/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0354 - accuracy: 1.0000\n",
            "Epoch 00682: loss did not improve from 0.03474\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 0.0456 - val_accuracy: 0.9959\n",
            "Epoch 683/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0344 - accuracy: 1.0000\n",
            "Epoch 00683: loss improved from 0.03474 to 0.03419, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0342 - accuracy: 1.0000 - val_loss: 0.0335 - val_accuracy: 1.0000\n",
            "Epoch 684/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0640 - accuracy: 0.9934\n",
            "Epoch 00684: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0593 - accuracy: 0.9946 - val_loss: 0.0427 - val_accuracy: 0.9959\n",
            "Epoch 685/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0464 - accuracy: 0.9934\n",
            "Epoch 00685: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0451 - accuracy: 0.9946 - val_loss: 0.0382 - val_accuracy: 1.0000\n",
            "Epoch 686/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0397 - accuracy: 0.9984\n",
            "Epoch 00686: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0393 - accuracy: 0.9986 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
            "Epoch 687/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0356 - accuracy: 1.0000\n",
            "Epoch 00687: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
            "Epoch 688/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0420 - accuracy: 0.9984\n",
            "Epoch 00688: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0421 - accuracy: 0.9973 - val_loss: 0.0363 - val_accuracy: 1.0000\n",
            "Epoch 689/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0463 - accuracy: 0.9918\n",
            "Epoch 00689: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0483 - accuracy: 0.9905 - val_loss: 0.0368 - val_accuracy: 1.0000\n",
            "Epoch 690/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0571 - accuracy: 0.9934\n",
            "Epoch 00690: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0618 - accuracy: 0.9918 - val_loss: 0.0404 - val_accuracy: 1.0000\n",
            "Epoch 691/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0702 - accuracy: 0.9913\n",
            "Epoch 00691: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0696 - accuracy: 0.9891 - val_loss: 0.0454 - val_accuracy: 1.0000\n",
            "Epoch 692/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0558 - accuracy: 0.9951\n",
            "Epoch 00692: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0531 - accuracy: 0.9959 - val_loss: 0.0519 - val_accuracy: 0.9959\n",
            "Epoch 693/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0428 - accuracy: 0.9984\n",
            "Epoch 00693: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0425 - accuracy: 0.9986 - val_loss: 0.0456 - val_accuracy: 0.9959\n",
            "Epoch 694/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0381 - accuracy: 1.0000\n",
            "Epoch 00694: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0386 - accuracy: 1.0000 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
            "Epoch 695/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0416 - accuracy: 0.9983\n",
            "Epoch 00695: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0414 - accuracy: 0.9986 - val_loss: 0.0626 - val_accuracy: 0.9959\n",
            "Epoch 696/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0461 - accuracy: 0.9967\n",
            "Epoch 00696: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0446 - accuracy: 0.9973 - val_loss: 0.0431 - val_accuracy: 0.9959\n",
            "Epoch 697/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0380 - accuracy: 1.0000\n",
            "Epoch 00697: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0378 - accuracy: 1.0000 - val_loss: 0.0415 - val_accuracy: 0.9959\n",
            "Epoch 698/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0373 - accuracy: 1.0000\n",
            "Epoch 00698: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 0.0416 - val_accuracy: 0.9959\n",
            "Epoch 699/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0358 - accuracy: 1.0000\n",
            "Epoch 00699: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0357 - accuracy: 1.0000 - val_loss: 0.0404 - val_accuracy: 0.9959\n",
            "Epoch 700/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0352 - accuracy: 1.0000\n",
            "Epoch 00700: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 0.0365 - val_accuracy: 1.0000\n",
            "Epoch 701/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0348 - accuracy: 1.0000\n",
            "Epoch 00701: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0349 - accuracy: 1.0000 - val_loss: 0.0357 - val_accuracy: 1.0000\n",
            "Epoch 702/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0344 - accuracy: 1.0000\n",
            "Epoch 00702: loss did not improve from 0.03419\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0343 - accuracy: 1.0000 - val_loss: 0.0358 - val_accuracy: 1.0000\n",
            "Epoch 703/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0337 - accuracy: 1.0000\n",
            "Epoch 00703: loss improved from 0.03419 to 0.03355, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0335 - accuracy: 1.0000 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
            "Epoch 704/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0331 - accuracy: 1.0000\n",
            "Epoch 00704: loss improved from 0.03355 to 0.03309, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0331 - accuracy: 1.0000 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
            "Epoch 705/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0332 - accuracy: 1.0000\n",
            "Epoch 00705: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.0337 - val_accuracy: 1.0000\n",
            "Epoch 706/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0355 - accuracy: 0.9984\n",
            "Epoch 00706: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0358 - accuracy: 0.9986 - val_loss: 0.0357 - val_accuracy: 1.0000\n",
            "Epoch 707/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0427 - accuracy: 0.9934\n",
            "Epoch 00707: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0426 - accuracy: 0.9946 - val_loss: 0.0383 - val_accuracy: 1.0000\n",
            "Epoch 708/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0349 - accuracy: 1.0000\n",
            "Epoch 00708: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0359 - accuracy: 1.0000 - val_loss: 0.0365 - val_accuracy: 1.0000\n",
            "Epoch 709/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0369 - accuracy: 0.9984\n",
            "Epoch 00709: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0380 - accuracy: 0.9973 - val_loss: 0.0344 - val_accuracy: 1.0000\n",
            "Epoch 710/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0438 - accuracy: 0.9984\n",
            "Epoch 00710: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0458 - accuracy: 0.9973 - val_loss: 0.0453 - val_accuracy: 0.9959\n",
            "Epoch 711/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0593 - accuracy: 0.9901\n",
            "Epoch 00711: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0577 - accuracy: 0.9905 - val_loss: 0.0660 - val_accuracy: 0.9878\n",
            "Epoch 712/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0445 - accuracy: 0.9967\n",
            "Epoch 00712: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0433 - accuracy: 0.9973 - val_loss: 0.0549 - val_accuracy: 0.9919\n",
            "Epoch 713/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0659 - accuracy: 0.9918\n",
            "Epoch 00713: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0643 - accuracy: 0.9905 - val_loss: 0.0854 - val_accuracy: 0.9878\n",
            "Epoch 714/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0535 - accuracy: 0.9918\n",
            "Epoch 00714: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0509 - accuracy: 0.9932 - val_loss: 0.0378 - val_accuracy: 1.0000\n",
            "Epoch 715/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0486 - accuracy: 0.9918\n",
            "Epoch 00715: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0472 - accuracy: 0.9932 - val_loss: 0.0406 - val_accuracy: 1.0000\n",
            "Epoch 716/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0430 - accuracy: 0.9967\n",
            "Epoch 00716: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0509 - accuracy: 0.9932 - val_loss: 0.1413 - val_accuracy: 0.9553\n",
            "Epoch 717/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0699 - accuracy: 0.9852\n",
            "Epoch 00717: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0739 - accuracy: 0.9837 - val_loss: 0.0927 - val_accuracy: 0.9837\n",
            "Epoch 718/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0523 - accuracy: 0.9965\n",
            "Epoch 00718: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0531 - accuracy: 0.9959 - val_loss: 0.0466 - val_accuracy: 0.9959\n",
            "Epoch 719/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0455 - accuracy: 0.9984\n",
            "Epoch 00719: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0444 - accuracy: 0.9986 - val_loss: 0.0404 - val_accuracy: 1.0000\n",
            "Epoch 720/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0401 - accuracy: 1.0000\n",
            "Epoch 00720: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0431 - accuracy: 0.9986 - val_loss: 0.0411 - val_accuracy: 1.0000\n",
            "Epoch 721/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0443 - accuracy: 0.9984\n",
            "Epoch 00721: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0434 - accuracy: 0.9986 - val_loss: 0.0389 - val_accuracy: 1.0000\n",
            "Epoch 722/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0406 - accuracy: 1.0000\n",
            "Epoch 00722: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0404 - accuracy: 1.0000 - val_loss: 0.0400 - val_accuracy: 1.0000\n",
            "Epoch 723/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0391 - accuracy: 1.0000\n",
            "Epoch 00723: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0398 - accuracy: 1.0000 - val_loss: 0.0382 - val_accuracy: 1.0000\n",
            "Epoch 724/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0426 - accuracy: 0.9984\n",
            "Epoch 00724: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0429 - accuracy: 0.9986 - val_loss: 0.0412 - val_accuracy: 1.0000\n",
            "Epoch 725/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0453 - accuracy: 0.9984\n",
            "Epoch 00725: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0450 - accuracy: 0.9986 - val_loss: 0.0386 - val_accuracy: 1.0000\n",
            "Epoch 726/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0387 - accuracy: 1.0000\n",
            "Epoch 00726: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0386 - accuracy: 1.0000 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
            "Epoch 727/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0371 - accuracy: 1.0000\n",
            "Epoch 00727: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 0.0366 - val_accuracy: 1.0000\n",
            "Epoch 728/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0372 - accuracy: 1.0000\n",
            "Epoch 00728: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0369 - accuracy: 1.0000 - val_loss: 0.0364 - val_accuracy: 1.0000\n",
            "Epoch 729/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0363 - accuracy: 1.0000\n",
            "Epoch 00729: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0362 - accuracy: 1.0000 - val_loss: 0.0355 - val_accuracy: 1.0000\n",
            "Epoch 730/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0357 - accuracy: 1.0000\n",
            "Epoch 00730: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0355 - accuracy: 1.0000 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
            "Epoch 731/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 1.0000\n",
            "Epoch 00731: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0349 - accuracy: 1.0000 - val_loss: 0.0358 - val_accuracy: 1.0000\n",
            "Epoch 732/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0352 - accuracy: 1.0000\n",
            "Epoch 00732: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 0.0342 - val_accuracy: 1.0000\n",
            "Epoch 733/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0336 - accuracy: 1.0000\n",
            "Epoch 00733: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0335 - accuracy: 1.0000 - val_loss: 0.0335 - val_accuracy: 1.0000\n",
            "Epoch 734/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0343 - accuracy: 1.0000\n",
            "Epoch 00734: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 0.0337 - val_accuracy: 1.0000\n",
            "Epoch 735/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0446 - accuracy: 0.9965\n",
            "Epoch 00735: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0427 - accuracy: 0.9973 - val_loss: 0.0540 - val_accuracy: 0.9919\n",
            "Epoch 736/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0470 - accuracy: 0.9967\n",
            "Epoch 00736: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0459 - accuracy: 0.9973 - val_loss: 0.0671 - val_accuracy: 0.9878\n",
            "Epoch 737/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0413 - accuracy: 0.9967\n",
            "Epoch 00737: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0420 - accuracy: 0.9959 - val_loss: 0.0383 - val_accuracy: 1.0000\n",
            "Epoch 738/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0455 - accuracy: 0.9918\n",
            "Epoch 00738: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0452 - accuracy: 0.9918 - val_loss: 0.0459 - val_accuracy: 1.0000\n",
            "Epoch 739/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0608 - accuracy: 0.9934\n",
            "Epoch 00739: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0567 - accuracy: 0.9946 - val_loss: 0.0783 - val_accuracy: 0.9878\n",
            "Epoch 740/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0769 - accuracy: 0.9836\n",
            "Epoch 00740: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0759 - accuracy: 0.9837 - val_loss: 0.0475 - val_accuracy: 0.9959\n",
            "Epoch 741/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0565 - accuracy: 0.9918\n",
            "Epoch 00741: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0538 - accuracy: 0.9932 - val_loss: 0.0575 - val_accuracy: 0.9919\n",
            "Epoch 742/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0672 - accuracy: 0.9918\n",
            "Epoch 00742: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0643 - accuracy: 0.9932 - val_loss: 0.0602 - val_accuracy: 0.9919\n",
            "Epoch 743/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0683 - accuracy: 0.9918\n",
            "Epoch 00743: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0649 - accuracy: 0.9932 - val_loss: 0.0575 - val_accuracy: 0.9919\n",
            "Epoch 744/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0628 - accuracy: 0.9934\n",
            "Epoch 00744: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0592 - accuracy: 0.9946 - val_loss: 0.0460 - val_accuracy: 1.0000\n",
            "Epoch 745/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0495 - accuracy: 0.9984\n",
            "Epoch 00745: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0481 - accuracy: 0.9986 - val_loss: 0.0420 - val_accuracy: 1.0000\n",
            "Epoch 746/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0478 - accuracy: 0.9951\n",
            "Epoch 00746: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0476 - accuracy: 0.9946 - val_loss: 0.0532 - val_accuracy: 0.9919\n",
            "Epoch 747/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0466 - accuracy: 0.9984\n",
            "Epoch 00747: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0488 - accuracy: 0.9973 - val_loss: 0.0404 - val_accuracy: 1.0000\n",
            "Epoch 748/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0437 - accuracy: 0.9984\n",
            "Epoch 00748: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0438 - accuracy: 0.9986 - val_loss: 0.0530 - val_accuracy: 0.9959\n",
            "Epoch 749/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0414 - accuracy: 1.0000\n",
            "Epoch 00749: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0411 - accuracy: 1.0000 - val_loss: 0.0405 - val_accuracy: 1.0000\n",
            "Epoch 750/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0391 - accuracy: 1.0000\n",
            "Epoch 00750: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0391 - accuracy: 1.0000 - val_loss: 0.0391 - val_accuracy: 1.0000\n",
            "Epoch 751/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0381 - accuracy: 1.0000\n",
            "Epoch 00751: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0380 - accuracy: 1.0000 - val_loss: 0.0384 - val_accuracy: 1.0000\n",
            "Epoch 752/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0380 - accuracy: 1.0000\n",
            "Epoch 00752: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0381 - accuracy: 1.0000 - val_loss: 0.0376 - val_accuracy: 1.0000\n",
            "Epoch 753/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0371 - accuracy: 1.0000\n",
            "Epoch 00753: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0386 - accuracy: 0.9986 - val_loss: 0.0369 - val_accuracy: 1.0000\n",
            "Epoch 754/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0388 - accuracy: 1.0000\n",
            "Epoch 00754: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0385 - accuracy: 1.0000 - val_loss: 0.0367 - val_accuracy: 1.0000\n",
            "Epoch 755/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0387 - accuracy: 1.0000\n",
            "Epoch 00755: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 0.0380 - val_accuracy: 1.0000\n",
            "Epoch 756/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0371 - accuracy: 1.0000\n",
            "Epoch 00756: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0369 - accuracy: 1.0000 - val_loss: 0.0469 - val_accuracy: 0.9959\n",
            "Epoch 757/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0371 - accuracy: 1.0000\n",
            "Epoch 00757: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0370 - accuracy: 1.0000 - val_loss: 0.0376 - val_accuracy: 1.0000\n",
            "Epoch 758/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0358 - accuracy: 1.0000\n",
            "Epoch 00758: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0357 - accuracy: 1.0000 - val_loss: 0.0354 - val_accuracy: 1.0000\n",
            "Epoch 759/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0355 - accuracy: 1.0000\n",
            "Epoch 00759: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0354 - accuracy: 1.0000 - val_loss: 0.0346 - val_accuracy: 1.0000\n",
            "Epoch 760/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0347 - accuracy: 1.0000\n",
            "Epoch 00760: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0349 - accuracy: 1.0000 - val_loss: 0.0340 - val_accuracy: 1.0000\n",
            "Epoch 761/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0384 - accuracy: 0.9967\n",
            "Epoch 00761: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0376 - accuracy: 0.9973 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
            "Epoch 762/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0367 - accuracy: 1.0000\n",
            "Epoch 00762: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0364 - accuracy: 1.0000 - val_loss: 0.0454 - val_accuracy: 0.9959\n",
            "Epoch 763/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0366 - accuracy: 0.9984\n",
            "Epoch 00763: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0361 - accuracy: 0.9986 - val_loss: 0.0347 - val_accuracy: 1.0000\n",
            "Epoch 764/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0333 - accuracy: 1.0000\n",
            "Epoch 00764: loss did not improve from 0.03309\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.0339 - val_accuracy: 1.0000\n",
            "Epoch 765/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0327 - accuracy: 1.0000\n",
            "Epoch 00765: loss improved from 0.03309 to 0.03265, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0327 - accuracy: 1.0000 - val_loss: 0.0333 - val_accuracy: 1.0000\n",
            "Epoch 766/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0392 - accuracy: 0.9967\n",
            "Epoch 00766: loss did not improve from 0.03265\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0383 - accuracy: 0.9973 - val_loss: 0.0411 - val_accuracy: 1.0000\n",
            "Epoch 767/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0372 - accuracy: 0.9984\n",
            "Epoch 00767: loss did not improve from 0.03265\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0371 - accuracy: 0.9986 - val_loss: 0.0404 - val_accuracy: 0.9959\n",
            "Epoch 768/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0343 - accuracy: 1.0000\n",
            "Epoch 00768: loss did not improve from 0.03265\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 0.0356 - val_accuracy: 1.0000\n",
            "Epoch 769/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0336 - accuracy: 1.0000\n",
            "Epoch 00769: loss did not improve from 0.03265\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 0.0382 - val_accuracy: 1.0000\n",
            "Epoch 770/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0343 - accuracy: 1.0000\n",
            "Epoch 00770: loss did not improve from 0.03265\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.0344 - val_accuracy: 1.0000\n",
            "Epoch 771/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0333 - accuracy: 0.9984\n",
            "Epoch 00771: loss did not improve from 0.03265\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0334 - accuracy: 0.9986 - val_loss: 0.0325 - val_accuracy: 1.0000\n",
            "Epoch 772/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0317 - accuracy: 1.0000\n",
            "Epoch 00772: loss improved from 0.03265 to 0.03161, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.0343 - val_accuracy: 1.0000\n",
            "Epoch 773/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
            "Epoch 00773: loss did not improve from 0.03161\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 1.0000\n",
            "Epoch 774/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0307 - accuracy: 1.0000\n",
            "Epoch 00774: loss improved from 0.03161 to 0.03081, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 0.0308 - val_accuracy: 1.0000\n",
            "Epoch 775/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0304 - accuracy: 1.0000\n",
            "Epoch 00775: loss improved from 0.03081 to 0.03028, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0303 - accuracy: 1.0000 - val_loss: 0.0300 - val_accuracy: 1.0000\n",
            "Epoch 776/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0303 - accuracy: 1.0000\n",
            "Epoch 00776: loss improved from 0.03028 to 0.03025, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 1.0000\n",
            "Epoch 777/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0294 - accuracy: 1.0000\n",
            "Epoch 00777: loss improved from 0.03025 to 0.02927, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.0292 - val_accuracy: 1.0000\n",
            "Epoch 778/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0290 - accuracy: 1.0000\n",
            "Epoch 00778: loss improved from 0.02927 to 0.02893, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
            "Epoch 779/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0284 - accuracy: 1.0000\n",
            "Epoch 00779: loss improved from 0.02893 to 0.02842, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
            "Epoch 780/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0400 - accuracy: 0.9951\n",
            "Epoch 00780: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0390 - accuracy: 0.9959 - val_loss: 0.0365 - val_accuracy: 1.0000\n",
            "Epoch 781/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0363 - accuracy: 0.9967\n",
            "Epoch 00781: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0410 - accuracy: 0.9959 - val_loss: 0.0321 - val_accuracy: 1.0000\n",
            "Epoch 782/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0387 - accuracy: 0.9967\n",
            "Epoch 00782: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0392 - accuracy: 0.9973 - val_loss: 0.0616 - val_accuracy: 0.9919\n",
            "Epoch 783/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0469 - accuracy: 0.9918\n",
            "Epoch 00783: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0466 - accuracy: 0.9932 - val_loss: 0.0335 - val_accuracy: 1.0000\n",
            "Epoch 784/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0351 - accuracy: 1.0000\n",
            "Epoch 00784: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 0.0324 - val_accuracy: 1.0000\n",
            "Epoch 785/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0348 - accuracy: 0.9984\n",
            "Epoch 00785: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0350 - accuracy: 0.9986 - val_loss: 0.0334 - val_accuracy: 1.0000\n",
            "Epoch 786/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0335 - accuracy: 0.9984\n",
            "Epoch 00786: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0330 - accuracy: 0.9986 - val_loss: 0.0315 - val_accuracy: 1.0000\n",
            "Epoch 787/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0311 - accuracy: 1.0000\n",
            "Epoch 00787: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
            "Epoch 788/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0323 - accuracy: 1.0000\n",
            "Epoch 00788: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 0.0332 - val_accuracy: 1.0000\n",
            "Epoch 789/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
            "Epoch 00789: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0311 - accuracy: 1.0000 - val_loss: 0.0327 - val_accuracy: 1.0000\n",
            "Epoch 790/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0526 - accuracy: 0.9934\n",
            "Epoch 00790: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0520 - accuracy: 0.9946 - val_loss: 0.0897 - val_accuracy: 0.9797\n",
            "Epoch 791/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0540 - accuracy: 0.9896\n",
            "Epoch 00791: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0632 - accuracy: 0.9878 - val_loss: 0.0836 - val_accuracy: 0.9878\n",
            "Epoch 792/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0576 - accuracy: 0.9885\n",
            "Epoch 00792: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0582 - accuracy: 0.9878 - val_loss: 0.1219 - val_accuracy: 0.9675\n",
            "Epoch 793/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0505 - accuracy: 0.9967\n",
            "Epoch 00793: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0497 - accuracy: 0.9973 - val_loss: 0.0521 - val_accuracy: 0.9919\n",
            "Epoch 794/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0473 - accuracy: 0.9948\n",
            "Epoch 00794: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0467 - accuracy: 0.9959 - val_loss: 0.0461 - val_accuracy: 1.0000\n",
            "Epoch 795/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0496 - accuracy: 0.9948\n",
            "Epoch 00795: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0488 - accuracy: 0.9946 - val_loss: 0.0694 - val_accuracy: 0.9878\n",
            "Epoch 796/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0571 - accuracy: 0.9951\n",
            "Epoch 00796: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0549 - accuracy: 0.9959 - val_loss: 0.0470 - val_accuracy: 0.9959\n",
            "Epoch 797/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0435 - accuracy: 0.9984\n",
            "Epoch 00797: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0434 - accuracy: 0.9986 - val_loss: 0.0701 - val_accuracy: 0.9878\n",
            "Epoch 798/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0384 - accuracy: 1.0000\n",
            "Epoch 00798: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 0.0364 - val_accuracy: 1.0000\n",
            "Epoch 799/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0359 - accuracy: 1.0000\n",
            "Epoch 00799: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
            "Epoch 800/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0365 - accuracy: 1.0000\n",
            "Epoch 00800: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0362 - accuracy: 1.0000 - val_loss: 0.0367 - val_accuracy: 1.0000\n",
            "Epoch 801/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0355 - accuracy: 1.0000\n",
            "Epoch 00801: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0364 - accuracy: 0.9986 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
            "Epoch 802/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0434 - accuracy: 0.9951\n",
            "Epoch 00802: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0418 - accuracy: 0.9959 - val_loss: 0.0363 - val_accuracy: 1.0000\n",
            "Epoch 803/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0421 - accuracy: 0.9951\n",
            "Epoch 00803: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0412 - accuracy: 0.9959 - val_loss: 0.0417 - val_accuracy: 1.0000\n",
            "Epoch 804/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0402 - accuracy: 0.9967\n",
            "Epoch 00804: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0403 - accuracy: 0.9973 - val_loss: 0.0398 - val_accuracy: 1.0000\n",
            "Epoch 805/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0440 - accuracy: 0.9984\n",
            "Epoch 00805: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0446 - accuracy: 0.9973 - val_loss: 0.0410 - val_accuracy: 0.9959\n",
            "Epoch 806/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0422 - accuracy: 0.9967\n",
            "Epoch 00806: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0441 - accuracy: 0.9959 - val_loss: 0.0346 - val_accuracy: 1.0000\n",
            "Epoch 807/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0444 - accuracy: 0.9967\n",
            "Epoch 00807: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0624 - accuracy: 0.9959 - val_loss: 0.0757 - val_accuracy: 0.9878\n",
            "Epoch 808/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0449 - accuracy: 0.9967\n",
            "Epoch 00808: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0438 - accuracy: 0.9973 - val_loss: 0.0543 - val_accuracy: 0.9959\n",
            "Epoch 809/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0426 - accuracy: 0.9967\n",
            "Epoch 00809: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0429 - accuracy: 0.9973 - val_loss: 0.0523 - val_accuracy: 0.9959\n",
            "Epoch 810/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0369 - accuracy: 1.0000\n",
            "Epoch 00810: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0391 - accuracy: 0.9986 - val_loss: 0.0429 - val_accuracy: 0.9959\n",
            "Epoch 811/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0381 - accuracy: 1.0000\n",
            "Epoch 00811: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0379 - accuracy: 1.0000 - val_loss: 0.0922 - val_accuracy: 0.9837\n",
            "Epoch 812/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0353 - accuracy: 1.0000\n",
            "Epoch 00812: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0352 - accuracy: 1.0000 - val_loss: 0.0423 - val_accuracy: 0.9959\n",
            "Epoch 813/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0378 - accuracy: 0.9984\n",
            "Epoch 00813: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0373 - accuracy: 0.9986 - val_loss: 0.0365 - val_accuracy: 1.0000\n",
            "Epoch 814/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0346 - accuracy: 1.0000\n",
            "Epoch 00814: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0344 - accuracy: 1.0000 - val_loss: 0.0369 - val_accuracy: 1.0000\n",
            "Epoch 815/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0610 - accuracy: 0.9896\n",
            "Epoch 00815: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0638 - accuracy: 0.9878 - val_loss: 0.1123 - val_accuracy: 0.9634\n",
            "Epoch 816/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0504 - accuracy: 0.9965\n",
            "Epoch 00816: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0492 - accuracy: 0.9973 - val_loss: 0.0397 - val_accuracy: 1.0000\n",
            "Epoch 817/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0413 - accuracy: 0.9984\n",
            "Epoch 00817: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0404 - accuracy: 0.9986 - val_loss: 0.0391 - val_accuracy: 1.0000\n",
            "Epoch 818/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0377 - accuracy: 1.0000\n",
            "Epoch 00818: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 0.0641 - val_accuracy: 0.9878\n",
            "Epoch 819/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0383 - accuracy: 0.9984\n",
            "Epoch 00819: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0390 - accuracy: 0.9986 - val_loss: 0.0426 - val_accuracy: 0.9959\n",
            "Epoch 820/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0496 - accuracy: 0.9951\n",
            "Epoch 00820: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0475 - accuracy: 0.9959 - val_loss: 0.0675 - val_accuracy: 0.9919\n",
            "Epoch 821/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0884 - accuracy: 0.9836\n",
            "Epoch 00821: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0840 - accuracy: 0.9837 - val_loss: 0.1023 - val_accuracy: 0.9715\n",
            "Epoch 822/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0479 - accuracy: 0.9967\n",
            "Epoch 00822: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0522 - accuracy: 0.9932 - val_loss: 0.0846 - val_accuracy: 0.9797\n",
            "Epoch 823/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0602 - accuracy: 0.9948\n",
            "Epoch 00823: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0570 - accuracy: 0.9959 - val_loss: 0.0483 - val_accuracy: 1.0000\n",
            "Epoch 824/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0490 - accuracy: 0.9934\n",
            "Epoch 00824: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0471 - accuracy: 0.9946 - val_loss: 0.0401 - val_accuracy: 1.0000\n",
            "Epoch 825/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0438 - accuracy: 0.9984\n",
            "Epoch 00825: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0427 - accuracy: 0.9986 - val_loss: 0.0599 - val_accuracy: 0.9959\n",
            "Epoch 826/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0410 - accuracy: 0.9984\n",
            "Epoch 00826: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0405 - accuracy: 0.9986 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
            "Epoch 827/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0495 - accuracy: 0.9951\n",
            "Epoch 00827: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0560 - accuracy: 0.9918 - val_loss: 0.0870 - val_accuracy: 0.9878\n",
            "Epoch 828/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0572 - accuracy: 0.9934\n",
            "Epoch 00828: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0570 - accuracy: 0.9946 - val_loss: 0.0601 - val_accuracy: 1.0000\n",
            "Epoch 829/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0532 - accuracy: 0.9951\n",
            "Epoch 00829: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0520 - accuracy: 0.9959 - val_loss: 0.0456 - val_accuracy: 1.0000\n",
            "Epoch 830/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0559 - accuracy: 0.9931\n",
            "Epoch 00830: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0587 - accuracy: 0.9918 - val_loss: 0.0607 - val_accuracy: 0.9959\n",
            "Epoch 831/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0587 - accuracy: 0.9951\n",
            "Epoch 00831: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0588 - accuracy: 0.9946 - val_loss: 0.0626 - val_accuracy: 0.9919\n",
            "Epoch 832/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0405 - accuracy: 1.0000\n",
            "Epoch 00832: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0404 - accuracy: 1.0000 - val_loss: 0.0454 - val_accuracy: 1.0000\n",
            "Epoch 833/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0394 - accuracy: 1.0000\n",
            "Epoch 00833: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0392 - accuracy: 1.0000 - val_loss: 0.0441 - val_accuracy: 0.9959\n",
            "Epoch 834/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0399 - accuracy: 1.0000\n",
            "Epoch 00834: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0395 - accuracy: 1.0000 - val_loss: 0.0448 - val_accuracy: 1.0000\n",
            "Epoch 835/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0487 - accuracy: 0.9984\n",
            "Epoch 00835: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0467 - accuracy: 0.9986 - val_loss: 0.0526 - val_accuracy: 0.9919\n",
            "Epoch 836/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0428 - accuracy: 0.9984\n",
            "Epoch 00836: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0419 - accuracy: 0.9986 - val_loss: 0.0602 - val_accuracy: 0.9878\n",
            "Epoch 837/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0411 - accuracy: 0.9984\n",
            "Epoch 00837: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0406 - accuracy: 0.9986 - val_loss: 0.0482 - val_accuracy: 0.9919\n",
            "Epoch 838/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0389 - accuracy: 0.9984\n",
            "Epoch 00838: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0385 - accuracy: 0.9986 - val_loss: 0.0527 - val_accuracy: 0.9919\n",
            "Epoch 839/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0376 - accuracy: 1.0000\n",
            "Epoch 00839: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0399 - accuracy: 0.9986 - val_loss: 0.0517 - val_accuracy: 0.9919\n",
            "Epoch 840/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0368 - accuracy: 1.0000\n",
            "Epoch 00840: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.0513 - val_accuracy: 0.9919\n",
            "Epoch 841/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0368 - accuracy: 1.0000\n",
            "Epoch 00841: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0382 - accuracy: 1.0000 - val_loss: 0.0473 - val_accuracy: 0.9959\n",
            "Epoch 842/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0444 - accuracy: 0.9967\n",
            "Epoch 00842: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0450 - accuracy: 0.9959 - val_loss: 0.0813 - val_accuracy: 0.9837\n",
            "Epoch 843/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0402 - accuracy: 1.0000\n",
            "Epoch 00843: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0400 - accuracy: 1.0000 - val_loss: 0.0404 - val_accuracy: 1.0000\n",
            "Epoch 844/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0366 - accuracy: 1.0000\n",
            "Epoch 00844: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0366 - accuracy: 1.0000 - val_loss: 0.0432 - val_accuracy: 0.9959\n",
            "Epoch 845/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0368 - accuracy: 1.0000\n",
            "Epoch 00845: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0366 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 0.9959\n",
            "Epoch 846/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0366 - accuracy: 1.0000\n",
            "Epoch 00846: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 0.0467 - val_accuracy: 0.9959\n",
            "Epoch 847/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0371 - accuracy: 0.9984\n",
            "Epoch 00847: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0367 - accuracy: 0.9986 - val_loss: 0.0535 - val_accuracy: 0.9959\n",
            "Epoch 848/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0343 - accuracy: 1.0000\n",
            "Epoch 00848: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0349 - accuracy: 1.0000 - val_loss: 0.0489 - val_accuracy: 0.9919\n",
            "Epoch 849/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0342 - accuracy: 1.0000\n",
            "Epoch 00849: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0341 - accuracy: 1.0000 - val_loss: 0.0463 - val_accuracy: 0.9919\n",
            "Epoch 850/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 1.0000\n",
            "Epoch 00850: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 0.9959\n",
            "Epoch 851/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0327 - accuracy: 1.0000\n",
            "Epoch 00851: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0327 - accuracy: 1.0000 - val_loss: 0.0435 - val_accuracy: 0.9959\n",
            "Epoch 852/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0323 - accuracy: 1.0000\n",
            "Epoch 00852: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0339 - accuracy: 0.9986 - val_loss: 0.0363 - val_accuracy: 1.0000\n",
            "Epoch 853/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0338 - accuracy: 1.0000\n",
            "Epoch 00853: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0337 - accuracy: 1.0000 - val_loss: 0.0441 - val_accuracy: 0.9919\n",
            "Epoch 854/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0333 - accuracy: 1.0000\n",
            "Epoch 00854: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.0375 - val_accuracy: 1.0000\n",
            "Epoch 855/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0369 - accuracy: 0.9984\n",
            "Epoch 00855: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0368 - accuracy: 0.9986 - val_loss: 0.0625 - val_accuracy: 0.9878\n",
            "Epoch 856/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0334 - accuracy: 1.0000\n",
            "Epoch 00856: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0331 - accuracy: 1.0000 - val_loss: 0.0319 - val_accuracy: 1.0000\n",
            "Epoch 857/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0358 - accuracy: 0.9984\n",
            "Epoch 00857: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0351 - accuracy: 0.9986 - val_loss: 0.0539 - val_accuracy: 0.9878\n",
            "Epoch 858/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0347 - accuracy: 0.9984\n",
            "Epoch 00858: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0341 - accuracy: 0.9986 - val_loss: 0.0344 - val_accuracy: 1.0000\n",
            "Epoch 859/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0343 - accuracy: 1.0000\n",
            "Epoch 00859: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0337 - accuracy: 1.0000 - val_loss: 0.0315 - val_accuracy: 1.0000\n",
            "Epoch 860/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0416 - accuracy: 0.9951\n",
            "Epoch 00860: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0415 - accuracy: 0.9946 - val_loss: 0.0327 - val_accuracy: 1.0000\n",
            "Epoch 861/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0420 - accuracy: 0.9967\n",
            "Epoch 00861: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0403 - accuracy: 0.9973 - val_loss: 0.0432 - val_accuracy: 0.9959\n",
            "Epoch 862/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0386 - accuracy: 0.9984\n",
            "Epoch 00862: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0377 - accuracy: 0.9986 - val_loss: 0.0519 - val_accuracy: 0.9919\n",
            "Epoch 863/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0361 - accuracy: 1.0000\n",
            "Epoch 00863: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.0365 - val_accuracy: 0.9959\n",
            "Epoch 864/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0471 - accuracy: 0.9951\n",
            "Epoch 00864: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0485 - accuracy: 0.9946 - val_loss: 0.0524 - val_accuracy: 0.9959\n",
            "Epoch 865/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0477 - accuracy: 0.9951\n",
            "Epoch 00865: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0478 - accuracy: 0.9946 - val_loss: 0.0551 - val_accuracy: 0.9919\n",
            "Epoch 866/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0406 - accuracy: 0.9967\n",
            "Epoch 00866: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0406 - accuracy: 0.9973 - val_loss: 0.0420 - val_accuracy: 0.9959\n",
            "Epoch 867/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0414 - accuracy: 0.9983\n",
            "Epoch 00867: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0404 - accuracy: 0.9986 - val_loss: 0.0715 - val_accuracy: 0.9878\n",
            "Epoch 868/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0487 - accuracy: 0.9934\n",
            "Epoch 00868: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0461 - accuracy: 0.9946 - val_loss: 0.0427 - val_accuracy: 0.9959\n",
            "Epoch 869/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0349 - accuracy: 1.0000\n",
            "Epoch 00869: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
            "Epoch 870/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0343 - accuracy: 1.0000\n",
            "Epoch 00870: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0389 - accuracy: 0.9986 - val_loss: 0.0460 - val_accuracy: 0.9959\n",
            "Epoch 871/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0386 - accuracy: 0.9967\n",
            "Epoch 00871: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0405 - accuracy: 0.9959 - val_loss: 0.0376 - val_accuracy: 1.0000\n",
            "Epoch 872/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0359 - accuracy: 1.0000\n",
            "Epoch 00872: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0354 - accuracy: 1.0000 - val_loss: 0.0423 - val_accuracy: 0.9959\n",
            "Epoch 873/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0336 - accuracy: 1.0000\n",
            "Epoch 00873: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 0.9919\n",
            "Epoch 874/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 1.0000\n",
            "Epoch 00874: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.0373 - val_accuracy: 0.9959\n",
            "Epoch 875/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0331 - accuracy: 1.0000\n",
            "Epoch 00875: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0327 - accuracy: 1.0000 - val_loss: 0.0428 - val_accuracy: 0.9959\n",
            "Epoch 876/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0325 - accuracy: 1.0000\n",
            "Epoch 00876: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 0.0447 - val_accuracy: 0.9919\n",
            "Epoch 877/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0363 - accuracy: 0.9984\n",
            "Epoch 00877: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0361 - accuracy: 0.9986 - val_loss: 0.0401 - val_accuracy: 0.9959\n",
            "Epoch 878/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0408 - accuracy: 0.9984\n",
            "Epoch 00878: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0400 - accuracy: 0.9986 - val_loss: 0.0905 - val_accuracy: 0.9837\n",
            "Epoch 879/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0367 - accuracy: 0.9984\n",
            "Epoch 00879: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0359 - accuracy: 0.9986 - val_loss: 0.0503 - val_accuracy: 0.9919\n",
            "Epoch 880/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0359 - accuracy: 0.9984\n",
            "Epoch 00880: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0355 - accuracy: 0.9986 - val_loss: 0.0569 - val_accuracy: 0.9919\n",
            "Epoch 881/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0491 - accuracy: 0.9951\n",
            "Epoch 00881: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0466 - accuracy: 0.9959 - val_loss: 0.0495 - val_accuracy: 0.9959\n",
            "Epoch 882/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0413 - accuracy: 0.9951\n",
            "Epoch 00882: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0413 - accuracy: 0.9946 - val_loss: 0.0367 - val_accuracy: 0.9959\n",
            "Epoch 883/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0374 - accuracy: 0.9967\n",
            "Epoch 00883: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0370 - accuracy: 0.9973 - val_loss: 0.0537 - val_accuracy: 0.9919\n",
            "Epoch 884/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0533 - accuracy: 0.9951\n",
            "Epoch 00884: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0551 - accuracy: 0.9946 - val_loss: 0.0820 - val_accuracy: 0.9797\n",
            "Epoch 885/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0506 - accuracy: 0.9951\n",
            "Epoch 00885: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0501 - accuracy: 0.9946 - val_loss: 0.1026 - val_accuracy: 0.9837\n",
            "Epoch 886/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0391 - accuracy: 1.0000\n",
            "Epoch 00886: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
            "Epoch 887/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0616 - accuracy: 0.9901\n",
            "Epoch 00887: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0646 - accuracy: 0.9878 - val_loss: 0.0496 - val_accuracy: 0.9919\n",
            "Epoch 888/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0526 - accuracy: 0.9967\n",
            "Epoch 00888: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0510 - accuracy: 0.9973 - val_loss: 0.0624 - val_accuracy: 0.9878\n",
            "Epoch 889/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0399 - accuracy: 0.9984\n",
            "Epoch 00889: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0429 - accuracy: 0.9959 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
            "Epoch 890/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0370 - accuracy: 0.9984\n",
            "Epoch 00890: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0372 - accuracy: 0.9986 - val_loss: 0.0448 - val_accuracy: 0.9959\n",
            "Epoch 891/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0425 - accuracy: 0.9984\n",
            "Epoch 00891: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0416 - accuracy: 0.9986 - val_loss: 0.0479 - val_accuracy: 0.9919\n",
            "Epoch 892/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0364 - accuracy: 1.0000\n",
            "Epoch 00892: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.0425 - val_accuracy: 0.9959\n",
            "Epoch 893/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0355 - accuracy: 1.0000\n",
            "Epoch 00893: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0352 - accuracy: 1.0000 - val_loss: 0.0447 - val_accuracy: 0.9959\n",
            "Epoch 894/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0350 - accuracy: 1.0000\n",
            "Epoch 00894: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 0.0379 - val_accuracy: 0.9959\n",
            "Epoch 895/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0339 - accuracy: 1.0000\n",
            "Epoch 00895: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0339 - accuracy: 1.0000 - val_loss: 0.0385 - val_accuracy: 0.9959\n",
            "Epoch 896/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0335 - accuracy: 1.0000\n",
            "Epoch 00896: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.0388 - val_accuracy: 0.9959\n",
            "Epoch 897/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0328 - accuracy: 1.0000\n",
            "Epoch 00897: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.0392 - val_accuracy: 0.9959\n",
            "Epoch 898/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0323 - accuracy: 1.0000\n",
            "Epoch 00898: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0324 - accuracy: 1.0000 - val_loss: 0.0378 - val_accuracy: 0.9959\n",
            "Epoch 899/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0330 - accuracy: 1.0000\n",
            "Epoch 00899: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0330 - accuracy: 1.0000 - val_loss: 0.0339 - val_accuracy: 1.0000\n",
            "Epoch 900/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0327 - accuracy: 1.0000\n",
            "Epoch 00900: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0324 - accuracy: 1.0000 - val_loss: 0.0323 - val_accuracy: 1.0000\n",
            "Epoch 901/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
            "Epoch 00901: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0314 - accuracy: 1.0000 - val_loss: 0.0324 - val_accuracy: 1.0000\n",
            "Epoch 902/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0373 - accuracy: 0.9984\n",
            "Epoch 00902: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0370 - accuracy: 0.9986 - val_loss: 0.0339 - val_accuracy: 1.0000\n",
            "Epoch 903/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0337 - accuracy: 1.0000\n",
            "Epoch 00903: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.0315 - val_accuracy: 1.0000\n",
            "Epoch 904/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0320 - accuracy: 1.0000\n",
            "Epoch 00904: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 0.0320 - val_accuracy: 1.0000\n",
            "Epoch 905/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0306 - accuracy: 1.0000\n",
            "Epoch 00905: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0306 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
            "Epoch 906/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0302 - accuracy: 1.0000\n",
            "Epoch 00906: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.0315 - val_accuracy: 1.0000\n",
            "Epoch 907/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
            "Epoch 00907: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 0.0366 - val_accuracy: 0.9959\n",
            "Epoch 908/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0306 - accuracy: 1.0000\n",
            "Epoch 00908: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0305 - accuracy: 1.0000 - val_loss: 0.0339 - val_accuracy: 0.9959\n",
            "Epoch 909/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0311 - accuracy: 1.0000\n",
            "Epoch 00909: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0311 - accuracy: 1.0000 - val_loss: 0.0512 - val_accuracy: 0.9919\n",
            "Epoch 910/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0309 - accuracy: 1.0000\n",
            "Epoch 00910: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 0.0407 - val_accuracy: 0.9959\n",
            "Epoch 911/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0297 - accuracy: 1.0000\n",
            "Epoch 00911: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0301 - accuracy: 1.0000 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
            "Epoch 912/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0286 - accuracy: 1.0000\n",
            "Epoch 00912: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.0301 - val_accuracy: 1.0000\n",
            "Epoch 913/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0312 - accuracy: 0.9984\n",
            "Epoch 00913: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0308 - accuracy: 0.9986 - val_loss: 0.0549 - val_accuracy: 0.9837\n",
            "Epoch 914/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0326 - accuracy: 0.9984\n",
            "Epoch 00914: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0321 - accuracy: 0.9986 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
            "Epoch 915/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
            "Epoch 00915: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.0305 - val_accuracy: 1.0000\n",
            "Epoch 916/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0311 - accuracy: 0.9984\n",
            "Epoch 00916: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0349 - accuracy: 0.9973 - val_loss: 0.0582 - val_accuracy: 0.9878\n",
            "Epoch 917/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0358 - accuracy: 0.9951\n",
            "Epoch 00917: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0384 - accuracy: 0.9946 - val_loss: 0.0433 - val_accuracy: 0.9959\n",
            "Epoch 918/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0365 - accuracy: 0.9984\n",
            "Epoch 00918: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0371 - accuracy: 0.9973 - val_loss: 0.0514 - val_accuracy: 0.9919\n",
            "Epoch 919/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0371 - accuracy: 0.9967\n",
            "Epoch 00919: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0386 - accuracy: 0.9959 - val_loss: 0.0342 - val_accuracy: 1.0000\n",
            "Epoch 920/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0466 - accuracy: 0.9913\n",
            "Epoch 00920: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0475 - accuracy: 0.9918 - val_loss: 0.1170 - val_accuracy: 0.9756\n",
            "Epoch 921/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0786 - accuracy: 0.9819\n",
            "Epoch 00921: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0779 - accuracy: 0.9810 - val_loss: 0.0879 - val_accuracy: 0.9878\n",
            "Epoch 922/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0742 - accuracy: 0.9918\n",
            "Epoch 00922: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0701 - accuracy: 0.9932 - val_loss: 0.0542 - val_accuracy: 0.9959\n",
            "Epoch 923/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0469 - accuracy: 0.9967\n",
            "Epoch 00923: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0461 - accuracy: 0.9973 - val_loss: 0.0545 - val_accuracy: 0.9919\n",
            "Epoch 924/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0601 - accuracy: 0.9918\n",
            "Epoch 00924: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0600 - accuracy: 0.9918 - val_loss: 0.0430 - val_accuracy: 1.0000\n",
            "Epoch 925/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0448 - accuracy: 0.9967\n",
            "Epoch 00925: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0455 - accuracy: 0.9973 - val_loss: 0.0679 - val_accuracy: 0.9919\n",
            "Epoch 926/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0416 - accuracy: 0.9967\n",
            "Epoch 00926: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0488 - accuracy: 0.9959 - val_loss: 0.0452 - val_accuracy: 0.9959\n",
            "Epoch 927/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1467 - accuracy: 0.9753\n",
            "Epoch 00927: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.1399 - accuracy: 0.9755 - val_loss: 0.1311 - val_accuracy: 0.9675\n",
            "Epoch 928/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0667 - accuracy: 0.9901\n",
            "Epoch 00928: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0695 - accuracy: 0.9891 - val_loss: 0.0614 - val_accuracy: 0.9878\n",
            "Epoch 929/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0557 - accuracy: 0.9967\n",
            "Epoch 00929: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0536 - accuracy: 0.9973 - val_loss: 0.0399 - val_accuracy: 1.0000\n",
            "Epoch 930/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0447 - accuracy: 0.9984\n",
            "Epoch 00930: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0440 - accuracy: 0.9986 - val_loss: 0.0484 - val_accuracy: 1.0000\n",
            "Epoch 931/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0442 - accuracy: 0.9967\n",
            "Epoch 00931: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0443 - accuracy: 0.9973 - val_loss: 0.0471 - val_accuracy: 0.9959\n",
            "Epoch 932/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0444 - accuracy: 0.9967\n",
            "Epoch 00932: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0443 - accuracy: 0.9973 - val_loss: 0.0409 - val_accuracy: 1.0000\n",
            "Epoch 933/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0396 - accuracy: 1.0000\n",
            "Epoch 00933: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0393 - accuracy: 1.0000 - val_loss: 0.0594 - val_accuracy: 0.9959\n",
            "Epoch 934/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0381 - accuracy: 1.0000\n",
            "Epoch 00934: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0384 - accuracy: 1.0000 - val_loss: 0.0659 - val_accuracy: 0.9919\n",
            "Epoch 935/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0376 - accuracy: 1.0000\n",
            "Epoch 00935: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 0.0549 - val_accuracy: 0.9959\n",
            "Epoch 936/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0384 - accuracy: 0.9984\n",
            "Epoch 00936: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0389 - accuracy: 0.9986 - val_loss: 0.0430 - val_accuracy: 0.9959\n",
            "Epoch 937/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0385 - accuracy: 1.0000\n",
            "Epoch 00937: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 0.9959\n",
            "Epoch 938/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0364 - accuracy: 1.0000\n",
            "Epoch 00938: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0364 - accuracy: 1.0000 - val_loss: 0.0492 - val_accuracy: 0.9959\n",
            "Epoch 939/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0361 - accuracy: 1.0000\n",
            "Epoch 00939: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.0493 - val_accuracy: 0.9959\n",
            "Epoch 940/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0357 - accuracy: 1.0000\n",
            "Epoch 00940: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 0.0423 - val_accuracy: 0.9959\n",
            "Epoch 941/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0352 - accuracy: 1.0000\n",
            "Epoch 00941: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 0.0408 - val_accuracy: 0.9959\n",
            "Epoch 942/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0349 - accuracy: 1.0000\n",
            "Epoch 00942: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 0.0460 - val_accuracy: 0.9959\n",
            "Epoch 943/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0345 - accuracy: 1.0000\n",
            "Epoch 00943: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0344 - accuracy: 1.0000 - val_loss: 0.0489 - val_accuracy: 0.9959\n",
            "Epoch 944/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0343 - accuracy: 1.0000\n",
            "Epoch 00944: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0342 - accuracy: 1.0000 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
            "Epoch 945/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0338 - accuracy: 1.0000\n",
            "Epoch 00945: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0337 - accuracy: 1.0000 - val_loss: 0.0363 - val_accuracy: 1.0000\n",
            "Epoch 946/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0335 - accuracy: 1.0000\n",
            "Epoch 00946: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0349 - accuracy: 0.9986 - val_loss: 0.0367 - val_accuracy: 1.0000\n",
            "Epoch 947/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0338 - accuracy: 1.0000\n",
            "Epoch 00947: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 0.0344 - val_accuracy: 1.0000\n",
            "Epoch 948/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0325 - accuracy: 1.0000\n",
            "Epoch 00948: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.0330 - val_accuracy: 1.0000\n",
            "Epoch 949/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0328 - accuracy: 1.0000\n",
            "Epoch 00949: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0327 - accuracy: 1.0000 - val_loss: 0.0323 - val_accuracy: 1.0000\n",
            "Epoch 950/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0323 - accuracy: 1.0000\n",
            "Epoch 00950: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0322 - accuracy: 1.0000 - val_loss: 0.0321 - val_accuracy: 1.0000\n",
            "Epoch 951/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0318 - accuracy: 1.0000\n",
            "Epoch 00951: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 0.0318 - val_accuracy: 1.0000\n",
            "Epoch 952/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
            "Epoch 00952: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0317 - accuracy: 1.0000 - val_loss: 0.0316 - val_accuracy: 1.0000\n",
            "Epoch 953/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0323 - accuracy: 0.9984\n",
            "Epoch 00953: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0324 - accuracy: 0.9986 - val_loss: 0.0309 - val_accuracy: 1.0000\n",
            "Epoch 954/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0322 - accuracy: 1.0000\n",
            "Epoch 00954: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.0303 - val_accuracy: 1.0000\n",
            "Epoch 955/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0305 - accuracy: 1.0000\n",
            "Epoch 00955: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0304 - accuracy: 1.0000 - val_loss: 0.0307 - val_accuracy: 1.0000\n",
            "Epoch 956/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0504 - accuracy: 0.9934\n",
            "Epoch 00956: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0506 - accuracy: 0.9918 - val_loss: 0.0837 - val_accuracy: 0.9837\n",
            "Epoch 957/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0339 - accuracy: 1.0000\n",
            "Epoch 00957: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0335 - accuracy: 1.0000 - val_loss: 0.0362 - val_accuracy: 1.0000\n",
            "Epoch 958/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0345 - accuracy: 1.0000\n",
            "Epoch 00958: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 0.0663 - val_accuracy: 0.9797\n",
            "Epoch 959/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0367 - accuracy: 0.9984\n",
            "Epoch 00959: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0360 - accuracy: 0.9986 - val_loss: 0.0314 - val_accuracy: 1.0000\n",
            "Epoch 960/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
            "Epoch 00960: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0313 - accuracy: 1.0000 - val_loss: 0.0315 - val_accuracy: 1.0000\n",
            "Epoch 961/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0305 - accuracy: 1.0000\n",
            "Epoch 00961: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0304 - accuracy: 1.0000 - val_loss: 0.0309 - val_accuracy: 1.0000\n",
            "Epoch 962/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0322 - accuracy: 0.9984\n",
            "Epoch 00962: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0321 - accuracy: 0.9986 - val_loss: 0.0471 - val_accuracy: 0.9919\n",
            "Epoch 963/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0416 - accuracy: 0.9948\n",
            "Epoch 00963: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0404 - accuracy: 0.9959 - val_loss: 0.0317 - val_accuracy: 1.0000\n",
            "Epoch 964/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0368 - accuracy: 0.9984\n",
            "Epoch 00964: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0358 - accuracy: 0.9986 - val_loss: 0.0403 - val_accuracy: 0.9959\n",
            "Epoch 965/1000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9946\n",
            "Epoch 00965: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0399 - accuracy: 0.9946 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
            "Epoch 966/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0355 - accuracy: 0.9984\n",
            "Epoch 00966: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0347 - accuracy: 0.9986 - val_loss: 0.0329 - val_accuracy: 1.0000\n",
            "Epoch 967/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0366 - accuracy: 0.9984\n",
            "Epoch 00967: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0356 - accuracy: 0.9986 - val_loss: 0.0609 - val_accuracy: 0.9878\n",
            "Epoch 968/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0345 - accuracy: 1.0000\n",
            "Epoch 00968: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
            "Epoch 969/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0310 - accuracy: 1.0000\n",
            "Epoch 00969: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 0.0317 - val_accuracy: 1.0000\n",
            "Epoch 970/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0304 - accuracy: 1.0000\n",
            "Epoch 00970: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0305 - accuracy: 1.0000 - val_loss: 0.0341 - val_accuracy: 1.0000\n",
            "Epoch 971/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0344 - accuracy: 0.9984\n",
            "Epoch 00971: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0337 - accuracy: 0.9986 - val_loss: 0.0355 - val_accuracy: 1.0000\n",
            "Epoch 972/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0330 - accuracy: 0.9984\n",
            "Epoch 00972: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0323 - accuracy: 0.9986 - val_loss: 0.0332 - val_accuracy: 1.0000\n",
            "Epoch 973/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0297 - accuracy: 1.0000\n",
            "Epoch 00973: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 0.0317 - val_accuracy: 1.0000\n",
            "Epoch 974/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0293 - accuracy: 1.0000\n",
            "Epoch 00974: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.0320 - val_accuracy: 1.0000\n",
            "Epoch 975/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0293 - accuracy: 1.0000\n",
            "Epoch 00975: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 0.0306 - val_accuracy: 1.0000\n",
            "Epoch 976/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0294 - accuracy: 1.0000\n",
            "Epoch 00976: loss did not improve from 0.02842\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 0.0327 - val_accuracy: 1.0000\n",
            "Epoch 977/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0283 - accuracy: 1.0000\n",
            "Epoch 00977: loss improved from 0.02842 to 0.02818, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.0282 - accuracy: 1.0000 - val_loss: 0.0313 - val_accuracy: 1.0000\n",
            "Epoch 978/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0281 - accuracy: 1.0000\n",
            "Epoch 00978: loss improved from 0.02818 to 0.02798, saving model to Emotion_detection.h5\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 0.0305 - val_accuracy: 1.0000\n",
            "Epoch 979/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0291 - accuracy: 1.0000\n",
            "Epoch 00979: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.0335 - val_accuracy: 0.9959\n",
            "Epoch 980/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0293 - accuracy: 0.9983\n",
            "Epoch 00980: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0316 - accuracy: 0.9973 - val_loss: 0.0650 - val_accuracy: 0.9797\n",
            "Epoch 981/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0441 - accuracy: 0.9934\n",
            "Epoch 00981: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0415 - accuracy: 0.9946 - val_loss: 0.0728 - val_accuracy: 0.9837\n",
            "Epoch 982/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0369 - accuracy: 0.9967\n",
            "Epoch 00982: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0367 - accuracy: 0.9973 - val_loss: 0.0515 - val_accuracy: 0.9919\n",
            "Epoch 983/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0631 - accuracy: 0.9918\n",
            "Epoch 00983: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0590 - accuracy: 0.9918 - val_loss: 0.0366 - val_accuracy: 1.0000\n",
            "Epoch 984/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0441 - accuracy: 0.9967\n",
            "Epoch 00984: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0534 - accuracy: 0.9932 - val_loss: 0.0579 - val_accuracy: 0.9878\n",
            "Epoch 985/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0684 - accuracy: 0.9844\n",
            "Epoch 00985: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0733 - accuracy: 0.9837 - val_loss: 0.0645 - val_accuracy: 0.9837\n",
            "Epoch 986/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0729 - accuracy: 0.9868\n",
            "Epoch 00986: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0838 - accuracy: 0.9837 - val_loss: 0.1018 - val_accuracy: 0.9797\n",
            "Epoch 987/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0929 - accuracy: 0.9819\n",
            "Epoch 00987: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0917 - accuracy: 0.9837 - val_loss: 0.1246 - val_accuracy: 0.9675\n",
            "Epoch 988/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0668 - accuracy: 0.9836\n",
            "Epoch 00988: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0643 - accuracy: 0.9850 - val_loss: 0.0961 - val_accuracy: 0.9878\n",
            "Epoch 989/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0560 - accuracy: 0.9918\n",
            "Epoch 00989: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0530 - accuracy: 0.9932 - val_loss: 0.0565 - val_accuracy: 0.9959\n",
            "Epoch 990/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0437 - accuracy: 0.9984\n",
            "Epoch 00990: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0431 - accuracy: 0.9986 - val_loss: 0.1163 - val_accuracy: 0.9715\n",
            "Epoch 991/1000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.0436 - accuracy: 0.9965\n",
            "Epoch 00991: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0423 - accuracy: 0.9973 - val_loss: 0.0397 - val_accuracy: 1.0000\n",
            "Epoch 992/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0443 - accuracy: 0.9984\n",
            "Epoch 00992: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0461 - accuracy: 0.9973 - val_loss: 0.0640 - val_accuracy: 0.9919\n",
            "Epoch 993/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0432 - accuracy: 0.9984\n",
            "Epoch 00993: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0434 - accuracy: 0.9986 - val_loss: 0.0803 - val_accuracy: 0.9797\n",
            "Epoch 994/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0568 - accuracy: 0.9951\n",
            "Epoch 00994: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0544 - accuracy: 0.9959 - val_loss: 0.0536 - val_accuracy: 0.9959\n",
            "Epoch 995/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0598 - accuracy: 0.9918\n",
            "Epoch 00995: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0567 - accuracy: 0.9932 - val_loss: 0.1254 - val_accuracy: 0.9715\n",
            "Epoch 996/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0924 - accuracy: 0.9868\n",
            "Epoch 00996: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0925 - accuracy: 0.9864 - val_loss: 0.0496 - val_accuracy: 0.9959\n",
            "Epoch 997/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0596 - accuracy: 0.9951\n",
            "Epoch 00997: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0564 - accuracy: 0.9959 - val_loss: 0.0414 - val_accuracy: 1.0000\n",
            "Epoch 998/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0459 - accuracy: 0.9967\n",
            "Epoch 00998: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0448 - accuracy: 0.9973 - val_loss: 0.0459 - val_accuracy: 1.0000\n",
            "Epoch 999/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0526 - accuracy: 0.9934\n",
            "Epoch 00999: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0507 - accuracy: 0.9946 - val_loss: 0.0438 - val_accuracy: 0.9959\n",
            "Epoch 1000/1000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.0408 - accuracy: 1.0000\n",
            "Epoch 01000: loss did not improve from 0.02798\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0404 - accuracy: 1.0000 - val_loss: 0.0410 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONp4QSBfQQG0",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGBB-gJfTl4h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76566959-93e4-49d7-d9a0-405c8d934aa9"
      },
      "source": [
        "score = Model.evaluate(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23/23 [==============================] - 0s 3ms/step - loss: 2.2464 - accuracy: 0.1810\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5zEPcF3I9Hz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60bb2895-135a-4906-c227-c6b721b5704d"
      },
      "source": [
        "score = Model.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 0s 3ms/step - loss: 2.2456 - accuracy: 0.2398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQW8rjpGJHUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Pred=Model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQMFS78EJgyf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "3ff05511-0c06-45fb-8d95-40fe5d83f64b"
      },
      "source": [
        "Pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5190955 , 0.52021766, 0.51526755, ..., 0.52388465, 0.5105537 ,\n",
              "        0.4940175 ],\n",
              "       [0.5353318 , 0.52766293, 0.5242309 , ..., 0.53446066, 0.51910937,\n",
              "        0.4903904 ],\n",
              "       [0.5165908 , 0.51750976, 0.5145964 , ..., 0.5183896 , 0.50919914,\n",
              "        0.49335212],\n",
              "       ...,\n",
              "       [0.5339558 , 0.528772  , 0.51163197, ..., 0.5356416 , 0.51848096,\n",
              "        0.4881375 ],\n",
              "       [0.5286349 , 0.5241898 , 0.5188591 , ..., 0.533335  , 0.51790094,\n",
              "        0.49032485],\n",
              "       [0.52610844, 0.5277436 , 0.52794755, ..., 0.53014946, 0.5128746 ,\n",
              "        0.4922611 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5UbFqk3Jjin",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "461546c2-959a-469e-8e32-03fa80e7f7ae"
      },
      "source": [
        "Y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       ...,\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lumQfISRJmF8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbb9004f-b9ab-4d62-a1ae-833a744098b2"
      },
      "source": [
        "len(Pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "246"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3WmRMWhNxVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_image(ind,images_f,images_f_2,Model):\n",
        "  cv2_imshow(images_f[ind])\n",
        "  image_test=images_f_2[ind]\n",
        "  print(\"Label actual:  \" + Exp[labels[ind]]  )\n",
        "  pred_1=Model.predict(np.array([image_test]))\n",
        "  #print(pred_1)\n",
        "  pred_class=Exp[int(np.argmax(pred_1))]\n",
        "  print(\"Predicted Label: \"+ pred_class)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-k9IW1dOgyz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "377aaa3f-feaf-4750-d379-7c27787e95fe"
      },
      "source": [
        "test_image(980,images_f,images_f_2,Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-04ec3e133d10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m980\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages_f_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'images_f' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcleDEVuNpvX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "16dde536-ad20-4b0c-f770-cb2163e08e15"
      },
      "source": [
        "test_image(36,images_f,images_f_2,Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7FA244549CC0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  anger\n",
            "Predicted Label: anger\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcwbyrEgO3YK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "15c038f4-0520-4936-b6d7-79d45414d14e"
      },
      "source": [
        "test_image(122,images_f,images_f_2,Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7FA244637978>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  anger\n",
            "Predicted Label: anger\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgLf7NGWP7bm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "a93f705f-f415-4624-ba1c-d0fdc290e24f"
      },
      "source": [
        "test_image(232,images_f,images_f_2,Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7FA244516DA0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  disgust\n",
            "Predicted Label: disgust\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6bdhwOhP-li",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "3561af00-8b9a-4daf-cc4b-78f8a4e5a82b"
      },
      "source": [
        "test_image(647,images_f,images_f_2,Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAIAAADYYG7QAAAOUUlEQVR4nF1Z2W/U1hf2cr3bY8+W2chGQkIgoaFAKwESL6i/SjxUVf/MPvWxVFVViUpV6QuJgBDKkoVkJuPxLB577Ovt9/DB7RQ/TTx37j33nO985zsn/NraWrfbDcNQEARCSFEUaZqKoihJEs/zsizzPJ8kiSAIoigahvHFF1/s7u5euXLFcZxKpdJut1utVpIklFJBEARB4DiuKApKKaV0NBqdnJwcHBw8efJkf3+/3++naZplWVEU2Bzr8zwXBKHZbDYaDXJxcRFFEXbBUkEQFEURRREv8zwnhHAcZxjGo0ePHjx40Gg0HMdRFKVUKtm2rSgKzOV5nhAiiiJ+RSnN87xard64cUOW5SAIBoMBx3FsW0qpKIqCIPA8n6bpaDSq1WpkNpvlec7MhOGwIM9zWCOKIqX0/v373333XalUMgxDVVXLsjiOg2+4/z78p0fTtEajYRiGKIpHR0dHR0e9Xg8GMd9kWcZxHCEkiqIwDAm8go1wPM/z+dzD83yWZWtra//73/9KpZKmaaqqKorCjo/jGMfjpDRN8ZMkSXA9wzCyLLt27VoQBIeHh/v7+7PZLMsyHIRbpWmqqqrv+4RZA4NEUcQiQAcnNRqNH374odVqiaIIVOE8QkiapljMNkEUsizL8xyeIISoqtpqtS5fvtzpdIIgeP36NXCCC0uSRCnFG/KZtymlPM8DQFmWWZa1ubl59+7djY0NVVUBQ0QWC/CZYRkXgCkIB9wsiuKlS5c8z6OU7uzseJ7neR78hAQCgimlAvM2cxKciaUrKyv37t3b3d01TZMBEBbLsqwoCtAGJzFow+tsZ9xBUZTNzU3DMNbW1kqlkqqqMD2OY0qpJEm6rodhSHAzZhBuCXtrtdq9e/euX7/eaDSQd8x0HMbsm7/VvB3IuKIoQCilUgmefvDgwc8//8xxnKZpRVEABpTSOI4/D9m8WV9++eXVq1dLpZIsy0g9SZKQdIjRfFrNf5iPoCAIAAruqapqp9P55ptvoih6/PgxaClJktls9nH9vHvmkbS0tHTnzh3LsnC8KIqKoiiKIkmSJEmwD8Yx+5jbxE8PggU/MZjzPG+a5v3795eXl9M0DcNwMpkEQRBFUZ7nZP6WzDhJkra2tqrVqqIosiwz/8MOdhILNwMNgzwOliSJ/ckYBJnY6XRu37797t27OI4ZESZJ8i8k8QH3Ngyj3W4DN8w9gOE8oc3HYj467GLIVpYEoigit5Mk4ThuY2Oj2WyOx+M0TUEfWZYRZg12UVW1KArbtiuVCq6ITeFPSZJUVUVmscMYtJmfkOqs7OBbfGD3yfO81Wp1Op3Dw0N8hfeEoRgfQNwbGxuO4xBCjo6OfN9P01QQBBSNarXaaDRUVVVVFRgnhMCLHMfFccyuO51O4zieTqee58EU27Y7nQ7yLssyWZaXl5clSQqCgLn587SnlK6srGxsbBBCer2e67osmhcXF+fn5wj/9vZ2s9m0bdswDMuyZFlO03Q2m43H4yiKoija29t7/vw5z/OqqpqmaVmWruuCIAwGg+XlZXhIEISFhQVZloGhj5j5LL8Mw7h+/bqiKOfn54qirK+vV6vVcrksy7Lv+2dnZ71e7+jo6NmzZ0EQlMvlhYUFEHqappPJZDKZuK778uXL09NTx3HW1tYajQbgGEXRYDDo9/svX76sVCpAZKlUQj6mafqxzsxbIwhCq9VqNpso4PV6vdFoWJYFgWHbNsdxUB3D4XA0GmHHNE1xxdlsNhwOe70eIWR7e1tRFMMwCCFBEGRZhnLGcZzrukEQgBK5T0WTpcV/skwURdRzQRAQiNls1uv1Pnz4MJvNAG1KaaVSqdfrURQBW4qi6LqeZVmpVPI8L0kSWZYPDw+73a4sy9Vq1bIsSqnjOKurq5VKxTAMwzAYa3z2/AfUiqKYpgneAyrfvHnz999/u647mUwgBERRtCzr5s2bgAISE36ilE4mk+Fw+OHDh+fPnydJcnFxIQhCkiStVqter7948WJ3d3dpaYkJCqaHGI7JfM7jMFEUNU0DqP/66680TdvtdrvdFkURW6dp+ubNm+XlZUAHO8LzhJDNzc3BYPD1119LkuR5Hoq0pmm+74dh6Lou+BZZxqgciVkUxX8MIoQ4jiPLsmVZmqZRSh89egQPq6oqCIKqqkiQOI6BSlYWwCWrq6uiKNbr9TAM4zhGcjGtAuRVKhXbtvFDWIPSC8Yh8ySr63q5XIa8L5fLq6urODuOY0jVIAhAj+VyuV6v4/18QSiXy6Iojsdjz/P6/X4cx4BjvV5vNptXrlxZWFhI03Q4HEIMGYYBOoBZSZIQRrKiKAKAlmWBYJBQ0Oeu615cXBRFYZrm0tLS4uJiHMfHx8dZlsVxjJ/D86Zp3r59e3t723Vd3/dns5mqqs1ms1wugzkJIfV63ff9yWQiSZJpmrquT6dTwJEA7ZVKRRAEx3FUVQWA8jxH+mia1mw2r127BtABvLPZLAxDVAZUCTRPWZaFYYiGybZtwBZUDsXDkhwhTtPUsqxKpQJe/Vi2TdNUFEXTNEmS4HlZliVJStMUP87zHGQD7YL3iB1iH4ZhFEVZlqH2ocSyQEAggOcIIZAMMBErdV1vt9u6rhdFQdBFAAQQAKip0JSGYTDJwX1qlXAGuhbYBHPhKqgALMb1WGVAiQXA8Sf6SRCNYRiTyYSgp0EGIhysVZgvwjgmyzL4ttvtep4XRRFYFDmYJAk6OKgt7AwLsCGERxzHrAcqiqJarQqCMJ1O4V2iaRqwIgiCruvwPCvaaKLhGDTIk8mk3+8nSbKysgKYgyZkWYYpRVFYltXtdj98+OA4jmVZqqqy1iJJEhb0JElM01xcXJxOp6x5JwsLCwAp1AXWxZ8edExwFaW03++/e/fO87xGoyHLcq/XY87XdT1JEkgOSulwONzb25NleXFxsdlsgtjQr81mM4QiTVNExrZt0zRt2x4Oh0RRlGazmaapJEmgUVS+MAxlWUZ9kSQpjmPXdd++favr+uLi4mAwePLkycnJieM44/G4VqsZhpHn+T///PPq1atWq7W6utpsNt+/f7+/vz8cDhuNBqiE5/kwDIE5+On69evffvvt27dv9/f3Dw4OCHJkbW2tXC5Pp1O4B+Th+z6lFMphPB4fHx/XarW1tTVBEIIggHbr9/toGIIgePfu3YsXLxB3x3EuXbrkOA6qbJIkkCtQTsB+mqaGYVy6dGk0GsVxvLOz8+zZM2LbtuM4oId2u93r9ZIkQQYBg0EQxHHc7XZ1XXccJwxDVVXr9fr6+rrv+0gu3/ejKCKE3L17V1GUTqdTrVZN0+R5Po7jo6OjwWAAZ4PkQCVw0nA4lGV5d3f34OCAUkoGg8GNGzcsyzIMA30TgxEMSpLEdV1ZlkulkmmahJAsy2zb3tnZcV0XqvL09JTjONu2FxYWVlZWoHFRf0AZBwcHg8EATAi1j7bTNE3oyTiOf/nllyzLyN7e3tbWVqfTsSxLEIQ4jkejEWzHb1B3pE8PDMqyDHRaqVSm0+loNDJNE0jCYASqCC0DyvPx8XG5XAZYWbeEpD49Pf3xxx/39vaWlpZ4URRVVb1///6tW7cMwzg7OyOE1Go1VDEAotvtnp+fd7vdIAhkWQarMvtA6wg6yj4IllLa6/Ww4eLiomVZd+7cgbnotH7//feffvopSZKTkxPTNDc3N/9tPR8/fvzbb79B8Xz//fcoQzzPa5q2sbFhGMb6+vqrV69+/fXX09NTSmm1Wo2iSNf1SqViWRaj+CAIgiBIkkTX9fPz8+Pj4/X19YcPH1YqFUmStre3u90upRTE+PTp0/39fdSNRqOBFoMsLCycn58zyiqK4vT09MqVK6jbaZrqur61tYXcvnz58h9//JGm6ebmpuu6t27d+vPPP1ut1uHhoWVZq6urr1+/vnfv3suXL+v1eq1We/jw4VdffQV01mq1KIoYAY7H49evX7darVqtVq1WoUUlSSLr6+umaY7H48lkggSGD6IoSpJEkiSYkmXZdDp1HOfmzZvn5+ee50GW7O7u3rx58+rVq47jaJoGjVGr1RRF2d7e7nQ66OCQXLPZDIwvy/LTp0/H43G73QbFf6wbGGig0/B9/+3bt8Ph8OLiot/v27Ydx7Eoiq7rYjUKE4SK53lFUUynUzAFmj3f903TPDs7gzBHrgG/RVGMx+MwDJG/kiQ9e/as3W5XKhVMvVj9JuB+juMcx9na2nr+/Pl4PH7//v3i4iKoJQiC4XBomqYgCBD/hmFIkjSbzXBpVVVRkrMsg6q3LKtUKmHgh1IYRRG4CitPT0/DMASwmOb5t1FkotEwjJ2dnRcvXpydnaGAIHbQXGwcE0URKjFOQrsDUYGxmmmakHKWZaEOxnEMgyBFTk5OIEUAnfnRG5kfPfE8bxjG9vZ2GIaj0YjldhAEEA+aplmWBTxBrM13UdCBaZoGQQB5jv4/z/MgCFC/IBpHoxEbSCLNsU+e54S17gh2nucgNCgNuAQGwSuWZUEwoR9FYrLpJ4gY4g7CA+0lukrwraqq0+kUxyH/5234OCJhMo81bCjF0+kUfbTv+6Cler2OMgRUAbAcxyFYbCaMGQMhJAxDVEPM7cCK+D8E4sV9Gv5BrxE222IP+98Ak38ohyiNIBgm6BBWUAi2jqIoCALksCiKcRyjR4PoM00TfmL9O5uvfQQ17sdGz2wdEkQQBNikaVqe5+PxGJ0eqh6QjvUABxN0kD6j0cjzPJZfuq6jZ8Xwf34cyB4y/xYAp5S6rosCBJ9hpE0IGQ6Hmqahk8KmaH3AqKzZK5fLRVH4vg8tAGsIIWCyLMt830eqszEcovaxt2e+ATw9zwPN4Iz54WaapoPBAJ0TvkLpRq+ICELZBUEA9c30IZsFCoIAhmOZxM2NFf8PEW7RJbg/0OMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7FA2444A3B38>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  happy\n",
            "Predicted Label: happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k4ELPe5QBhc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "39869a0a-c2d4-429b-a79c-0885add6fec3"
      },
      "source": [
        "test_image(869,images_f,images_f_2,Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7FA2445880B8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  surprise\n",
            "Predicted Label: surprise\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpz-wzUCQGX-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "88eb1355-f2da-4cb0-b190-ff3bb048b68a"
      },
      "source": [
        "test_image(502,images_f,images_f_2,Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7FA244516128>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  happy\n",
            "Predicted Label: happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA8uKPvGQJfC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "acd6da22-5b8f-4efc-f0f6-765c14dce6ed"
      },
      "source": [
        "test_image(800,images_f,images_f_2,Model)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7FA21C5BB8D0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label actual:  surprise\n",
            "Predicted Label: contempt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tyx5ao4dQgMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ5pvdm4QNtF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "11410960-42a4-41c6-b72b-f37d6c4fca3d"
      },
      "source": [
        "plt.plot(History.history['loss'])\n",
        "plt.plot(History.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.subplots_adjust(top=1.00, bottom=0.0, left=0.0, right=0.95, hspace=0.25,\n",
        "                        wspace=0.35)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UycVjuzdQlW8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "a861728c-3362-45b5-f529-25d3fd2addc9"
      },
      "source": [
        "plt.plot(History.history['accuracy'])\n",
        "plt.plot(History.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.subplots_adjust(top=1.00, bottom=0.0, left=0.0, right=0.95, hspace=0.25,\n",
        "                        wspace=0.35)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pAhlICdRUgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix \n",
        "\n",
        "from sklearn.metrics import classification_report "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je6eHsWERabS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i=0\n",
        "Y_test_l=[]\n",
        "Pred_l=[]\n",
        "while(i<len(Pred)):\n",
        "  Y_test_l.append(int(np.argmax(Y_test[i])))\n",
        "  Pred_l.append(int(np.argmax(Pred[i])))\n",
        "  i+=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGRVFqYEQ0fx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "report=classification_report(Y_test_l, Pred_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXLt0q86SiI2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "b4a95e23-20dc-4ee9-ec43-4acf34988aa0"
      },
      "source": [
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        18\n",
            "           1       1.00      1.00      1.00        10\n",
            "           2       1.00      1.00      1.00        50\n",
            "           3       1.00      1.00      1.00        32\n",
            "           4       1.00      1.00      1.00        71\n",
            "           5       1.00      1.00      1.00        49\n",
            "           6       1.00      1.00      1.00        16\n",
            "\n",
            "    accuracy                           1.00       246\n",
            "   macro avg       1.00      1.00      1.00       246\n",
            "weighted avg       1.00      1.00      1.00       246\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LcBCMAESlya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "results = confusion_matrix(Y_test_l, Pred_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5x8_HqWSry3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "4b966a7a-96b1-4df2-cf4a-ecf18e5770f2"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.heatmap(results, annot=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa24404a438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD5giowI47kQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q=[]\n",
        "import numpy as np\n",
        "images_f=np.array(q)\n",
        "images_f_2=images_f/255"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CD1YlQXodgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(ind,images_f,images_f_2,Model):\n",
        "  cv2_imshow(images_f[ind])\n",
        "  image_test=images_f_2[ind]\n",
        "  pred_1=Model.predict(np.array([image_test]))\n",
        "  pred_class=Exp[int(np.argmax(pred_1))]\n",
        "  print(\"Predicted Label: \"+ pred_class)\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S4TwkdUHmvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "image= cv2.imread('/content/sample_data/111.jpeg')\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "image= cv2.resize(image,(48,48))\n",
        "q.append(image)\n",
        "images_f=np.array(q)\n",
        "images_f_2=images_f/255\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQRTROWxtnpf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "dbed4ab0-e3dd-4bb5-cf45-d536da843f59"
      },
      "source": [
        "test(0,images_f,images_f_2,model)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=48x48 at 0x7F520E06CF98>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted Label: happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqEwu3TY4Ude",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7181de0-8ae3-4b1f-8034-0d697f7495c4"
      },
      "source": [
        "print(len(q))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2UmfVgcCnXk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "59625f0e-9a82-4299-acfb-3ee9d5076471"
      },
      "source": [
        "model = tf.keras.models.load_model(\n",
        "  '/content/drive/My Drive/FYP/Emotion_detection.h5'\n",
        "  # `custom_objects` tells keras how to load a `hub.KerasLayer`\n",
        " )\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 48, 48, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 48, 48, 32)        896       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 48, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 48, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 6, 6, 256)         295168    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               295040    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 684,359\n",
            "Trainable params: 684,359\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65T8kEAOPzZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}